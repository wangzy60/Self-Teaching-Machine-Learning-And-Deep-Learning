{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4593566c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#模型设计：CNN+RNN+LinearSoftmax+CTC\" data-toc-modified-id=\"模型设计：CNN+RNN+LinearSoftmax+CTC-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>模型设计：CNN+RNN+LinearSoftmax+CTC</a></span><ul class=\"toc-item\"><li><span><a href=\"#CNN\" data-toc-modified-id=\"CNN-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>CNN</a></span><ul class=\"toc-item\"><li><span><a href=\"#Resnet18+Conv2d\" data-toc-modified-id=\"Resnet18+Conv2d-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Resnet18+Conv2d</a></span></li><li><span><a href=\"#BN（batch-normalization）\" data-toc-modified-id=\"BN（batch-normalization）-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>BN（batch normalization）</a></span></li></ul></li><li><span><a href=\"#RNN\" data-toc-modified-id=\"RNN-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>RNN</a></span><ul class=\"toc-item\"><li><span><a href=\"#Bi_1layer_GRU\" data-toc-modified-id=\"Bi_1layer_GRU-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Bi_1layer_GRU</a></span></li><li><span><a href=\"#梯度裁剪\" data-toc-modified-id=\"梯度裁剪-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>梯度裁剪</a></span></li></ul></li><li><span><a href=\"#Linear+Softmax\" data-toc-modified-id=\"Linear+Softmax-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Linear+Softmax</a></span></li><li><span><a href=\"#CTC（连续时间序列分类）\" data-toc-modified-id=\"CTC（连续时间序列分类）-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>CTC（连续时间序列分类）</a></span></li></ul></li><li><span><a href=\"#模型实现\" data-toc-modified-id=\"模型实现-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>模型实现</a></span><ul class=\"toc-item\"><li><span><a href=\"#数据集制作\" data-toc-modified-id=\"数据集制作-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>数据集制作</a></span><ul class=\"toc-item\"><li><span><a href=\"#图像预处理\" data-toc-modified-id=\"图像预处理-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>图像预处理</a></span></li><li><span><a href=\"#建立数据集类\" data-toc-modified-id=\"建立数据集类-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>建立数据集类</a></span></li><li><span><a href=\"#测试数据集类\" data-toc-modified-id=\"测试数据集类-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>测试数据集类</a></span></li></ul></li><li><span><a href=\"#网络模型搭建\" data-toc-modified-id=\"网络模型搭建-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>网络模型搭建</a></span><ul class=\"toc-item\"><li><span><a href=\"#网络搭建\" data-toc-modified-id=\"网络搭建-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>网络搭建</a></span></li><li><span><a href=\"#模型测试\" data-toc-modified-id=\"模型测试-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>模型测试</a></span></li></ul></li><li><span><a href=\"#损失函数模型搭建\" data-toc-modified-id=\"损失函数模型搭建-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>损失函数模型搭建</a></span><ul class=\"toc-item\"><li><span><a href=\"#CTC损失模型实现\" data-toc-modified-id=\"CTC损失模型实现-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>CTC损失模型实现</a></span></li></ul></li><li><span><a href=\"#DataLoader搭建\" data-toc-modified-id=\"DataLoader搭建-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>DataLoader搭建</a></span><ul class=\"toc-item\"><li><span><a href=\"#DataLoader搭建逻辑\" data-toc-modified-id=\"DataLoader搭建逻辑-2.4.1\"><span class=\"toc-item-num\">2.4.1&nbsp;&nbsp;</span>DataLoader搭建逻辑</a></span><ul class=\"toc-item\"><li><span><a href=\"#collate_fn说明\" data-toc-modified-id=\"collate_fn说明-2.4.1.1\"><span class=\"toc-item-num\">2.4.1.1&nbsp;&nbsp;</span>collate_fn说明</a></span></li></ul></li><li><span><a href=\"#DataLoader类创建\" data-toc-modified-id=\"DataLoader类创建-2.4.2\"><span class=\"toc-item-num\">2.4.2&nbsp;&nbsp;</span>DataLoader类创建</a></span></li><li><span><a href=\"#Dataloader和loss测试\" data-toc-modified-id=\"Dataloader和loss测试-2.4.3\"><span class=\"toc-item-num\">2.4.3&nbsp;&nbsp;</span>Dataloader和loss测试</a></span></li></ul></li><li><span><a href=\"#训练模块\" data-toc-modified-id=\"训练模块-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>训练模块</a></span><ul class=\"toc-item\"><li><span><a href=\"#训练逻辑\" data-toc-modified-id=\"训练逻辑-2.5.1\"><span class=\"toc-item-num\">2.5.1&nbsp;&nbsp;</span>训练逻辑</a></span></li><li><span><a href=\"#创建训练类\" data-toc-modified-id=\"创建训练类-2.5.2\"><span class=\"toc-item-num\">2.5.2&nbsp;&nbsp;</span>创建训练类</a></span></li><li><span><a href=\"#训练类测试\" data-toc-modified-id=\"训练类测试-2.5.3\"><span class=\"toc-item-num\">2.5.3&nbsp;&nbsp;</span>训练类测试</a></span><ul class=\"toc-item\"><li><span><a href=\"#直接训练结果\" data-toc-modified-id=\"直接训练结果-2.5.3.1\"><span class=\"toc-item-num\">2.5.3.1&nbsp;&nbsp;</span>直接训练结果</a></span></li><li><span><a href=\"#原因分析\" data-toc-modified-id=\"原因分析-2.5.3.2\"><span class=\"toc-item-num\">2.5.3.2&nbsp;&nbsp;</span>原因分析</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#总结\" data-toc-modified-id=\"总结-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>总结</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52759dbc",
   "metadata": {},
   "source": [
    "## 模型设计：CNN+RNN+LinearSoftmax+CTC\n",
    "数据集中需要识别的字符都基本是呈水平方向排列，因此可以将一张图片看做是n个竖条组成的一段话，其中有数字的地方是对应的字符串，没有数字的地方对应空白，这样一来，字符识别就变成了文字处理任务。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160de8ad",
   "metadata": {},
   "source": [
    "\n",
    "### CNN\n",
    "#### Resnet18+Conv2d\n",
    "**Resnet18提取RGB图片特征：  \n",
    "input（N,C,W,H）(N,3,100,200),　output(N,C,W,H)(N,512,4,7)  \n",
    "Conv2d连接Resnet18和Bi-GRU：  \n",
    "input（N,C,W,H）(N,512,4,7),　output(N,C,W,H)(N,512,1,30)** \n",
    "\n",
    "\n",
    "　　在处理文字时，循环神经网络接收（N，T，H）形状的输入，其中N表示batch_size，T表示总共的时刻，也就是需要多少个循环神经单元，H是每个字符的embedding表示结果。对应到图片字符识别，如何确定这三个值呢。假设把一张图片看做是30个竖条，那T=30，也就是需要有30个隐藏的循环神经网络去分别处理这30个竖条，N等于batch_size，这个可以根据batch的大小来确定，而剩下的问题是H应该等于多少呢，这个可以对照一下文字处理的流程。在文字处理流程中，一个字符首先通过其对应的语料库表示成one-hot形式，然后通过一个embedding层，产生一个（1，H）形状的向量来表示这个文字，而这个（1，H）的形状向量接下来就输入给循环神经网络单元，可以看到这个H其实是由embedding层确定的，相当于是我们需要决定，要把一个one-hot格式的编码，压缩成一个多少维度的向量，通常来说这个值是300-600之间。再回过头来看我们的图片，我们现在希望将图片转换为30个字符，那一张图片就相当于一段有30个字符的话，每个字符的内容的维度假设为512，那一张图片转换完的形状应该是（30,512），那这个形状如何通过CNN得到呢？CNN理论上来说是不断压缩图片长度和宽度增加图片的特征维度的过程，直观来说，对于图片的三个维度（C，H，W），在CNN过程中，C不断增加，H和W不断减少，可以看做是不断压缩图片，增加图片特征的一个过程，C可以看做是经过CNN过程提取出来的特征的数量，对于一张（3,100,200）形状的图片，我们不太能消灭其中一个维度将其转换成（30,512）的形状，但是将其转换为（512,1,30）是可以做到的，所以可以先把（3,100，200）通过各种卷积转换成（512,1,30），然后将其变形为（30,512）作为一句30个字符串的话输入给循环神经网络。  \n",
    "  \n",
    "　　resnet18因为已经固定好了每一层卷积池化的核以及步数的大小，给定（3,100,200）输入的情况下，输出固定为（512，4，7），为了让形状能顺利的输入给GRU，还需要手工添加一个卷积层，把（512,4,7）转换为（512,1,30）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efa9e4f",
   "metadata": {},
   "source": [
    "#### BN（batch normalization）\n",
    "resnet中，每个conV2d层后面都会接一个BatchNorm2d层，作用是对一个batch内的值，以channel为范围，将每个channel内的值进行标准化。下面是pytorch的官方说明：  \n",
    "\n",
    "    torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None)\n",
    "    \n",
    "    Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .\n",
    "\n",
    "　　　　　　　　　　　　　　$y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta$\n",
    "\n",
    "    The mean and standard-deviation are calculated per-dimension over the mini-batches and γ and β are learnable parameter vectors of size C (where C is the input size). By default, the elements of γ are set to 1 and the elements of β are set to 0. The standard-deviation is calculated via the biased estimator, equivalent to torch.var(input, unbiased=False).\n",
    "    \n",
    "    Because the Batch Normalization is done over the C dimension, computing statistics on (N, H, W) slices, it’s common terminology to call this Spatial Batch Normalization.\n",
    "    \n",
    "    Parameters:\n",
    "    num_features (int) – CC from an expected input of size (N, C, H, W)(N,C,H,W)\n",
    "    eps (float) – a value added to the denominator for numerical stability. Default: 1e-5\n",
    "    momentum (float) – the value used for the running_mean and running_var computation. Can be set to None for cumulative moving average (i.e. simple average). Default: 0.1\n",
    "    affine (bool) – a boolean value that when set to True, this module has learnable affine parameters. Default: True\n",
    "    track_running_stats (bool) – a boolean value that when set to True, this module tracks the running mean and variance, and when set to False, this module does not track such statistics, and initializes statistics buffers running_mean and running_var as None. When these buffers are None, this module always uses batch statistics. in both training and eval modes. Default: True\n",
    "    \n",
    "其中有几个关键点需要注意：  \n",
    "\n",
    "　　１、BN2d是以ｃｈａｎｎｅｌ为范围进行的，并不是以单个数据为范围进行的，也不是以整个ｂａｔｃｈ为范围进行的。也就是说有几个ｃｈａｎｎｅｌ就有几个平均数和几个方差。例如对于RGB图片（N，C，H，W），需要分别计算C＝０、１、２时候（N，H，W）数据的平均值和方差，然后在对应的ｃｈａｎｎｅｌ内对数据进行标准化。  \n",
    "　　２、计算公示中的$\\epsilon$是为了防止分母为零的一个微小数。  \n",
    "　　３、公示中的$\\gamma和\\beta$是否需要由参数affine控制，如果需要的话，其初始值分别为1和0，这两个参数在训练过程中会不断变化，具体变化和原理要再研究。  \n",
    "　　４、参数track_running_stats默认为True，具体作用有要再研究。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827458ca",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da0754c",
   "metadata": {},
   "source": [
    "#### Bi_1layer_GRU\n",
    "**Bi-GRU:双向循环神经网络，针对每个位置进行字符识别  \n",
    "input（N,T,Hin)(N,30,512)　output（N,T,Hout）(N,30,64*2)**  \n",
    "\n",
    "　　双向GRU相当于两层参数独立的GRU，第一层GRU接收正常时刻的输入数据，在本例中，一个数据总共有30个时刻，按照第0个GRU、第1个GRU、第29个GRU的顺序，依次输入（N，T0，Hin_t0），（N，T1，Hin_t1）......(N，T29，Hin_t29)作为输入数据，依次输出（N，T0，Hfout_t0），（N，T1，Hfout_t1）......(N，T29，Hfout_t29)，其中Hfout表示前向顺序输入的输出结果。第二层GRU按照倒序时间输入数据，按照第29个GRU、第28个GRU、第0个GRU的顺序，依次输入（N，T29，Hin_t29），（N，T28，Hin_t28）......(N，T0，Hin_t0)作为输入数据，依次输出（N，T29，Hbout_t29），（N，T28，Hbout_t28）......(N，T0，Hbout_t0),其中Hbout表示反向顺序输入的输出结果。其中第一层GRU和第二层GRU在同一个位置上的输入数据是相同的，只是其接收的隐藏状态不一样，第一层接收的是当前位置前几个GRU的隐藏状态，第二层接收的是当前位置后几个GRU的隐藏状态。  \n",
    "  \n",
    "　　输出完成之后，将得到两组输出，每组输出的形状都是（N，30，64），其中64是我们自己设定的隐藏状态的维度，这两组输出是结合之后传给下一层呢，还是各自传给下一层？理论上来说，方式不定，例如可以将两组输出结果直接相加或者相乘或者取平均数等，这样输出的维度还是（N，30，64），也可以像pytorch自带的输出结果格式一样，把这两组结果进行水平方向的拼接，按照（N，30，64*2）的维度输出，这种拼接形式具体来说是按照对应的时间来拼接（N，T0，Hfout_t0）（N，T0，Hbout_t0），．．．，（N，T0，Hfout_t29）（N，T0，Hbout_t29）的顺序进行拼接的。具体见下面的图示和实验"
   ]
  },
  {
   "attachments": {
    "%E5%8F%8C%E5%B1%82GRU%E5%8E%9F%E7%90%86.jpg": {
     "image/jpeg": "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"
    }
   },
   "cell_type": "markdown",
   "id": "21923cdb",
   "metadata": {},
   "source": [
    "![%E5%8F%8C%E5%B1%82GRU%E5%8E%9F%E7%90%86.jpg](attachment:%E5%8F%8C%E5%B1%82GRU%E5%8E%9F%E7%90%86.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7012a6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------One direction(forward)------------------------\n",
      "input:\n",
      " tensor([[[-0.4868],\n",
      "         [-0.6038]]]) \n",
      "\n",
      "output:\n",
      " (tensor([[[ 0.0849, -0.0720],\n",
      "         [ 0.1567, -0.1298]]], grad_fn=<TransposeBackward1>), tensor([[[ 0.1567, -0.1298]]], grad_fn=<StackBackward0>))\n",
      "-----------------------One direction(backward)------------------------\n",
      "input:\n",
      " tensor([[[-0.6038],\n",
      "         [-0.4868]]]) \n",
      "\n",
      "output:\n",
      " (tensor([[[ 0.1122, -0.1333],\n",
      "         [ 0.1399, -0.0742]]], grad_fn=<TransposeBackward1>), tensor([[[ 0.1399, -0.0742]]], grad_fn=<StackBackward0>))\n",
      "----------------------------Two direction-----------------------------\n",
      "input:\n",
      " tensor([[[-0.4868],\n",
      "         [-0.6038]]]) \n",
      "\n",
      "output:\n",
      " (tensor([[[ 0.0849, -0.0720,  0.1399, -0.0742],\n",
      "         [ 0.1567, -0.1298,  0.1122, -0.1333]]], grad_fn=<TransposeBackward1>), tensor([[[ 0.1567, -0.1298]],\n",
      "\n",
      "        [[ 0.1399, -0.0742]]], grad_fn=<StackBackward0>))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.random.manual_seed(5)\n",
    "print('One direction(forward)'.center(70,'-'))\n",
    "test_input = torch.randn(1,2,1)\n",
    "print('input:\\n',test_input,'\\n')\n",
    "test_model = nn.GRU(input_size=1,hidden_size=2,batch_first=True,bidirectional=False)\n",
    "# print('model params:\\n',[x for x in test_model.named_parameters()],'\\n')\n",
    "test_output = test_model(test_input)\n",
    "print('output:\\n',test_output)\n",
    "\n",
    "torch.random.manual_seed(5)\n",
    "print('One direction(backward)'.center(70,'-'))\n",
    "test_input = torch.randn(1,2,1)\n",
    "test_input = test_input.flip(1)\n",
    "print('input:\\n',test_input,'\\n')\n",
    "test_model = nn.GRU(input_size=1,hidden_size=2,batch_first=True,bidirectional=False)\n",
    "# print('model params:\\n',[x for x in test_model.named_parameters()],'\\n')\n",
    "test_output = test_model(test_input)\n",
    "print('output:\\n',test_output)\n",
    "\n",
    "print('Two direction'.center(70,'-'))\n",
    "torch.random.manual_seed(5)\n",
    "test_input = torch.randn(1,2,1)\n",
    "print('input:\\n',test_input,'\\n')\n",
    "test_model = nn.GRU(input_size=1,hidden_size=2,batch_first=True,bidirectional=True)\n",
    "test_model.weight_ih_l0_reverse.data = test_model.weight_ih_l0.data\n",
    "test_model.weight_hh_l0_reverse.data = test_model.weight_hh_l0.data\n",
    "test_model.bias_ih_l0_reverse.data = test_model.bias_ih_l0.data\n",
    "test_model.bias_hh_l0_reverse.data = test_model.bias_hh_l0.data\n",
    "# print('model params:\\n',[x for x in test_model.named_parameters()],'\\n')\n",
    "test_output = test_model(test_input)\n",
    "print('output:\\n',test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b3036d",
   "metadata": {},
   "source": [
    "#### 梯度裁剪\n",
    "　　梯度裁剪的目的是为了避免梯度爆炸，循环神经网络中当隐藏神经元比较多的时候容易产生梯度爆炸，裁剪的操作比较简单，以模型为单位，当模型内的所有参数的导数的L２范数超过一个我们设定的阈值的时候，以“阈值／参数导数的L２范数”为系数，和每个参数的导数相乘，以结果作为每个参数的新导数。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5fc40aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grads:\n",
      " tensor([[-0.2017,  1.0677,  0.3246],\n",
      "        [-0.0684, -0.9959,  1.1563]]) \n",
      "\n",
      "threshold:\n",
      " 1 \n",
      "\n",
      "grads_L2:\n",
      " tensor(1.9025) \n",
      "\n",
      "new_grads:\n",
      " tensor([[-0.1060,  0.5612,  0.1706],\n",
      "        [-0.0359, -0.5235,  0.6078]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "grads = torch.randn(2,3)\n",
    "print('grads:\\n',grads,'\\n')\n",
    "threshold = 1\n",
    "print('threshold:\\n',threshold,'\\n')\n",
    "print('grads_L2:\\n',torch.sqrt(torch.sum(grads*grads)),'\\n')\n",
    "new_grads = (threshold/torch.sqrt(torch.sum(grads*grads)))*grads\n",
    "print('new_grads:\\n',new_grads,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ed3672",
   "metadata": {},
   "source": [
    "### Linear+Softmax\n",
    "**Linear：将GRU生成的seq转换成满足输出类别数量的特征  \n",
    "input(N，30，128)　output(N,30,11)**  \n",
    "\n",
    "　　这里的线性层，主要是把128维的特征，转化为最终的分类计算结果，最后有11个分类，分别是数字0-9以及空白。  \n",
    "\n",
    "**Softmax：对计算结果进行归一化  \n",
    "input(N，30，11)　output(N,30,1)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e89efd7",
   "metadata": {},
   "source": [
    "### CTC（连续时间序列分类）\n",
    "　　CTC是为了解决连续时间序列分类问题中的预测输出和真实标签难以对齐计算损失，而提出的一种解决算法。直观来说，语言有快慢之分，文字有大小之分，如何让快慢或者大小不同的同一个字都能准确的识别是CTC想解决的问题。具体来说为了解决这个问题，CTC算法有如下一些比较特殊的设计。  \n",
    "　　1、设计的目标类别要比实际目标类别多一个空白类，也就是说某一块区域的预测结果有可能是字符，也可能是空白。例如，目标类别是1、2、3，那设计目标类别应该是1、2、3、-，其中‘-’表示空白  \n",
    "　　２、输出原则为：连续相同字符都输出为一个字符，然后将空格字符取消，输出最后结果。如输出为111-22---33，合并相同字符之后应该为1-2-3，最后输出应该为123，这个原则同样适用于有字符串中有连续相同字符的情况，如111-111-222-333，合并相同字符之后应该为1-1-2-3，输出结果为1123。  \n",
    "　　３、损失的具体计算原则使用了动态规划，比较复杂，可以参考，具体可以再研究https://distill.pub/2017/ctc/  \n",
    "  \n",
    "　　下面是pytorch中具体的使用说明：　　\n",
    "\n",
    "    torch.nn.CTCLoss(blank=0, reduction='mean', zero_infinity=False)\n",
    "    \n",
    "    Calculates loss between a continuous (unsegmented) time series and a target sequence. CTCLoss sums over the probability of possible alignments of input to target, producing a loss value which is differentiable with respect to each input node. The alignment of input to target is assumed to be “many-to-one”, which limits the length of the target sequence such that it must be \\leq≤ the input length.\n",
    "\n",
    "    Parameters:\n",
    "        blank (int, optional) – blank label. Default 0.\n",
    "\n",
    "        reduction (str, optional) – Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the output losses will be divided by the target lengths and then the mean over the batch is taken. Default: 'mean'\n",
    "\n",
    "        zero_infinity (bool, optional) – Whether to zero infinite losses and the associated gradients. Default: False Infinite losses mainly occur when the inputs are too short to be aligned to the targets.\n",
    "        \n",
    "    Shape:\n",
    "        Log_probs: Tensor of size (T,N,C) or (T,C), where T=input length, N =batch size, and C =number of classes (including blank). The logarithmized probabilities of the outputs (e.g. obtained with torch.nn.functional.log_softmax()).\n",
    "\n",
    "        Targets: Tensor of size (N,S) or (sum(target_lengths)), whereN=batch size and S=max target length, if shape is (N,S). It represent the target sequences. Each element in the target sequence is a class index. And the target index cannot be blank (default=0). In the (N,S) form, targets are padded to the length of the longest sequence, and stacked. In the (sum(target_lengths)) form, the targets are assumed to be un-padded and concatenated within 1 dimension.\n",
    "\n",
    "        Input_lengths: Tuple or tensor of size (N) or (), where N = batch size. It represent the lengths of the inputs (must each be ≤T). And the lengths are specified for each sequence to achieve masking under the assumption that sequences are padded to equal lengths.\n",
    "\n",
    "        Target_lengths: Tuple or tensor of size (N) or (), where N=batch size. It represent lengths of the targets. Lengths are specified for each sequence to achieve masking under the assumption that sequences are padded to equal lengths. If target shape is (N,S), target_lengths are effectively the stop index s_n for each target sequence, such that target_n = targets[n,0:s_n] for each target in a batch. Lengths must each be ≤S If the targets are given as a 1d tensor that is the concatenation of individual targets, the target_lengths must add up to the total length of the tensor.\n",
    "\n",
    "        Output: scalar. If reduction is 'none', then (N) if input is batched or () if input is unbatched, where N =batch size.\n",
    "        \n",
    "需要注意点如下：  \n",
    "        \n",
    "　　1、blank空白标签一定要依据空白符在预测总字符集中的位置来设定，否则就会出错；\n",
    "\n",
    "　　2、targets建议将其shape设为(sum(target_lengths))，然后再由target_lengths进行输入序列长度指定就好了，这是因为如果设定为(N, S)，则因为S的标签长度如果是可变的，那么我们组装出来的二维张量的第一维度的长度仅为min(S)将损失一部分标签值（多维数组每行的长度必须一致），这就导致模型无法预测较长长度的标签；\n",
    "\n",
    "　　3、输出序列长度T尽量在模型设计时就要考虑到模型需要预测的最长序列，如需要预测的最长序列其长度为I，则理论上T应大于等于2I+1，这是因为CTCLoss假设在最坏情况下每个真实标签前后都至少有一个空白标签进行隔开以区分重复项；\n",
    "\n",
    "　　4、输出的log_probs除了进行log_softmax()处理再送入CTCLoss外，还必须要调整其维度顺序，确保其shape为(T, N, C)！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ffceff",
   "metadata": {},
   "source": [
    "## 模型实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea7a480",
   "metadata": {},
   "source": [
    "### 数据集制作\n",
    "#### 图像预处理\n",
    "具体处理措施：  \n",
    "\n",
    "·所有图片resize到长200、宽100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae9a14c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreatTransforms:\n",
    "    def __init__(self,input_size):\n",
    "        from torchvision import transforms as transforms\n",
    "        self.img_transforms = transforms.Compose([transforms.Resize(size=input_size),\n",
    "                                                  transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6e5c7a",
   "metadata": {},
   "source": [
    "#### 建立数据集类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d713eada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,img_path,label_content,transforms=None):\n",
    "        self.img_path = img_path\n",
    "        self.label_content = label_content\n",
    "        if transforms is not None:\n",
    "            self.transforms = transforms\n",
    "        else:\n",
    "            self.transforms = None\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        img_path = self.img_path[index]\n",
    "        img_cv2 = cv2.imread(img_path)  #return （H，W，C），C list by B，G，R\n",
    "        img_cv2 = img_cv2[:,:,::-1].copy()  #return （H，W，C），C list by R，G，B\n",
    "        img_tensor = torch.from_numpy(img_cv2).permute(2,0,1).float()  #return （C，H，W），C list by R，G，B\n",
    "        if self.transforms is not None:\n",
    "            img_tensor = self.transforms(img_tensor)\n",
    "        assert img_tensor.shape[0] == 3\n",
    "        \n",
    "        img_name = os.path.split(img_path)[1]\n",
    "        img_label = self.label_content[img_name]['label']\n",
    "        #transform the label 0~9 to index 1~10，index 0 is for blank\n",
    "        img_label_index = [x+1 for x in img_label]\n",
    "        assert isinstance(img_label_index,list) == True\n",
    "        \n",
    "        return img_tensor,torch.LongTensor(img_label_index)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183b1d35",
   "metadata": {},
   "source": [
    "#### 测试数据集类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03fef02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img shape: torch.Size([3, 240, 480])\n",
      "img label: tensor([ 2, 10])\n"
     ]
    }
   ],
   "source": [
    "def test_dataset():\n",
    "    \n",
    "    input_size = (240,480)\n",
    "    \n",
    "    import json\n",
    "    import glob\n",
    "    \n",
    "    test_img_path = glob.glob('../input/train/*.png')\n",
    "    test_img_path.sort()\n",
    "    test_label_content = json.load(open('../input/train.json'))\n",
    "    \n",
    "    img_transforms = CreatTransforms(input_size).img_transforms\n",
    "    \n",
    "    test_data = MyDataset(test_img_path,test_label_content,img_transforms)\n",
    "    \n",
    "    return test_data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_data = test_dataset()\n",
    "    print('img shape:',test_data[0][0].shape)\n",
    "    print('img label:',test_data[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a3d18f",
   "metadata": {},
   "source": [
    "### 网络模型搭建"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d0235e",
   "metadata": {},
   "source": [
    "#### 网络搭建\n",
    "按照一张图片竖向切分成15条让rnn识别来设计。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eacfd9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #self.cnn: input(N,3,240,480),output(N,512,8,15)\n",
    "        trained_resnet18 = models.resnet18(weights='DEFAULT')\n",
    "        self.cnn = nn.Sequential(trained_resnet18.conv1,\n",
    "                                 trained_resnet18.bn1,\n",
    "                                 trained_resnet18.relu,\n",
    "                                 trained_resnet18.maxpool,\n",
    "                                 trained_resnet18.layer1,\n",
    "                                 trained_resnet18.layer2,\n",
    "                                 trained_resnet18.layer3,\n",
    "                                 trained_resnet18.layer4)\n",
    "        #self.feat_to_seq: input(N,512,8,15),output(N,512,1,15)\n",
    "        self.feat_to_seq = nn.Conv2d(512,512,kernel_size=(8,1),stride=1)\n",
    "        #self.rnn: input(15,N,512),output(15,N,64*2)\n",
    "        self.rnn = nn.GRU(input_size=512,hidden_size=64,bidirectional=True)\n",
    "        #self.linear: input(15,N,64*2),output(15,N,11)\n",
    "        self.linear = nn.Linear(128,11)\n",
    "        #self.softmax: input(15,N,11),output(15,N,11) \n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        X = self.cnn(X)\n",
    "        X = self.feat_to_seq(X)\n",
    "        X = X.squeeze(dim=2)\n",
    "        X = X.permute((2,0,1))\n",
    "        X,_ = self.rnn(X)\n",
    "        X = self.linear(X)\n",
    "        y_hat = self.softmax(X)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f83f712",
   "metadata": {},
   "source": [
    "#### 模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9acc3ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([1, 3, 240, 480])\n",
      "y_hat shape: torch.Size([15, 1, 11])\n",
      "y_label: tensor([ 2, 10])\n",
      "y_hat row0 result:\n",
      " tensor([[-0.1821, -0.2814, -0.2751,  0.0969,  0.0972,  0.0611,  0.0943,  0.0847,\n",
      "         -0.2799,  0.0761, -0.3214]])\n"
     ]
    }
   ],
   "source": [
    "def model_test(index):\n",
    "    test_data,test_label = test_dataset()[index]\n",
    "    test_data = test_data.view(1,*test_data.shape)\n",
    "    test_model = MyModel()\n",
    "    y_hat = test_model(test_data)\n",
    "    return test_data,test_label,y_hat\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    test_data,test_label,y_hat = model_test(0)\n",
    "    print('X shape:',test_data.shape)\n",
    "    print('y_hat shape:',y_hat.shape)\n",
    "    print('y_label:',test_label)\n",
    "    print('y_hat row0 result:\\n',y_hat[0,:,:].data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d695a04a",
   "metadata": {},
   "source": [
    "### 损失函数模型搭建"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec41853",
   "metadata": {},
   "source": [
    "#### CTC损失模型实现\n",
    "将0作为空白结果的索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87c561fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MyLoss():\n",
    "    def __init__(self):\n",
    "        self.ctc = nn.CTCLoss(blank=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121630a7",
   "metadata": {},
   "source": [
    "### DataLoader搭建"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad94a36",
   "metadata": {},
   "source": [
    "#### DataLoader搭建逻辑\n",
    "　　因为CTC损失模型对输入数据形状的特殊要求，这里的DataLoader需要自定义collate_fn函数，以使得通过DataLoader输出的batch_data的格式能满足CTC损失函数的参数输入要求。  \n",
    "  \n",
    "　　主要需要改写的功能是让一个batch内的label数据水平拼接且不增加维度，而不是以默认的stack的形式进行拼接。原因是，如果使用默认的collate_fn函数，一个batch内的label数据会以stack的形式拼接，但是每个img含有的数字个数是不同的，而这会导致比较长的label的信息将会被部分砍掉，造成信息损失。举例说明，有3个label，分别是（2、3、4），（1），（3，4，5，6），这三个label最短的长度为1，当这三个数据按stack的形式组成1个batch传入CTC时，因为label的长度没有对齐，会按照长度最短的label的长度来对每个label进行裁剪，超出最短长度的部位将被剪掉，最后输入CTC的分别是（2），（1），（3）。  \n",
    "  \n",
    "　　另外一个需要改写的功能是，除了feat和label之外，需要额外输出一个参数：每个label的长度。因为label被水平合并成了一个row，无法判断哪一个索引是属于哪一个数据的，所以要把每个label的长度值做成一个tensor传给CTC模型以进行计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74fb317",
   "metadata": {},
   "source": [
    "##### collate_fn说明\n",
    "DataLoader类初始化的时候有一个‘collate_fn＝’的关键字参数，用来确定DataLoader的具体返回格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "662ab6ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-1.1154137 , -0.53994045, -1.09151636],\n",
       "        [-0.54182111,  0.19145612,  0.25883391],\n",
       "        [ 0.47944172,  0.6605742 , -1.3369123 ],\n",
       "        [ 0.20228337,  0.8416316 , -0.68252285]]),\n",
       " array([[1, 2],\n",
       "        [3, 4],\n",
       "        [4, 5],\n",
       "        [2, 3]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "_a= np.random.randn(4,3)\n",
    "_b= np.array([[1,2],[3,4],[4,5],[2,3]])\n",
    "_a,_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5221c9",
   "metadata": {},
   "source": [
    "　　首先来看一组对比，使用一个空的collate_fn函数和使用一个默认的collate_fn函数分别返回一组batch data。  \n",
    "　　可以看到，如果使用空的collate_fn时，DataLoader只是把每一个data按照batch_index组合成一个list就输出了，数据还是ndarry数据。而当使用默认的collate_fn的时候，不同数据的ndarry数据按照stack的形式变成了tensor数据。因此可以判断，**接受Dataset实例的__getitem__()方法返回的数据做为输入数据并根据函数内容返回结果作为DataLoader的输出，collate_fn这个参数对应的函数的作用。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "396504e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用我们写的内容为空的collate_fn返回的batch data：\n",
      " [(array([-1.1154137 , -0.53994045, -1.09151636]), array([1, 2])), (array([-0.54182111,  0.19145612,  0.25883391]), array([3, 4])), (array([ 0.47944172,  0.6605742 , -1.3369123 ]), array([4, 5])), (array([ 0.20228337,  0.8416316 , -0.68252285]), array([2, 3]))] \n",
      "\n",
      "使用DataLoader类默认的collate_fn返回的batch data：\n",
      " [tensor([[-1.1154, -0.5399, -1.0915],\n",
      "        [-0.5418,  0.1915,  0.2588],\n",
      "        [ 0.4794,  0.6606, -1.3369],\n",
      "        [ 0.2023,  0.8416, -0.6825]], dtype=torch.float64), tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [4, 5],\n",
      "        [2, 3]], dtype=torch.int32)]\n"
     ]
    }
   ],
   "source": [
    "class _MyDataset(Dataset):\n",
    "    def __init__(self,data,label):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "    def __getitem__(self,index):\n",
    "        return self.data[index],self.label[index]\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "def _my_collate1(batch_data):\n",
    "    return batch_data\n",
    "\n",
    "my_data = _MyDataset(_a,_b)\n",
    "\n",
    "for i in DataLoader(my_data,batch_size=4,collate_fn=_my_collate1):\n",
    "    print('使用我们写的内容为空的collate_fn返回的batch data：\\n',i,'\\n')\n",
    "    break\n",
    "\n",
    "for i in DataLoader(my_data,batch_size=4):\n",
    "    print('使用DataLoader类默认的collate_fn返回的batch data：\\n',i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5adbd3f",
   "metadata": {},
   "source": [
    "　　下面尝试自己编写一个函数用做参数传递给collate_fn，使得Dataloader返回的label数据不再以stack的方式进行堆叠，而是以水平方向拼接的方式来组合，最后输出的是一维的数据，另外，额外再多输出一个label_len数据。  \n",
    "　　还应该看到的是Dataset类传递给DataLoader的数据是以list(外部)+tuple(内部)+原始格式的形式存在的，这将作为collate_fn函数的输入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b865c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "自定义collate_fn输出结果：\n",
      " (tensor([[-1.1154, -0.5399, -1.0915],\n",
      "        [-0.5418,  0.1915,  0.2588],\n",
      "        [ 0.4794,  0.6606, -1.3369],\n",
      "        [ 0.2023,  0.8416, -0.6825]], dtype=torch.float64), tensor([1, 3, 4, 2, 2, 4, 5, 3], dtype=torch.int32), tensor([2, 2, 2, 2])) \n",
      "\n",
      "默认的collate_fn输出结果：\n",
      " [tensor([[-1.1154, -0.5399, -1.0915],\n",
      "        [-0.5418,  0.1915,  0.2588],\n",
      "        [ 0.4794,  0.6606, -1.3369],\n",
      "        [ 0.2023,  0.8416, -0.6825]], dtype=torch.float64), tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [4, 5],\n",
      "        [2, 3]], dtype=torch.int32)]\n"
     ]
    }
   ],
   "source": [
    "def _my_collate2(batch_data):\n",
    "    '''\n",
    "    batch_data: [(feat1,label1),(feat2,label2),……]\n",
    "    '''\n",
    "    feat_list =[]\n",
    "    label_list = []\n",
    "    label_len_list = []\n",
    "    for sample in batch_data:\n",
    "        feat_list.append(torch.from_numpy(sample[0]))\n",
    "        label_list.append(torch.from_numpy(sample[1]))\n",
    "        label_len_list.append(len(sample[1]))\n",
    "    feat = torch.stack(feat_list,0)\n",
    "    label = torch.stack(label_list,1).view(-1)\n",
    "    label_len = torch.LongTensor(label_len_list)\n",
    "    return feat,label,label_len\n",
    "\n",
    "my_data = _MyDataset(_a,_b)\n",
    "\n",
    "for i in DataLoader(my_data,batch_size=4,collate_fn=_my_collate2):\n",
    "    print('自定义collate_fn输出结果：\\n',i,'\\n')\n",
    "    break\n",
    "for i in DataLoader(my_data,batch_size=4):\n",
    "    print('默认的collate_fn输出结果：\\n',i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43ec568",
   "metadata": {},
   "source": [
    "#### DataLoader类创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22ff0dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataLoader():\n",
    "    def __init__(self,my_dataset,batch_size=20,use_my_collate=True,shuffle=False,):\n",
    "        from torch.utils.data import DataLoader\n",
    "        if use_my_collate:\n",
    "            self.data_loader = DataLoader(my_dataset,batch_size=batch_size,shuffle=shuffle,collate_fn=self.my_collate)\n",
    "        else:\n",
    "            self.data_loader = DataLoader(my_dataset,batch_size=batch_size,shuffle=shuffle)\n",
    "        \n",
    "    def my_collate(self,batch_data):\n",
    "        '''\n",
    "        batch_data shape: [(feat_tensor1,label_tensor1),(feat_tensor2,label_tensor2),……]\n",
    "        '''\n",
    "        feat_list =[]\n",
    "        label_list = []\n",
    "        label_len_list = []\n",
    "        for sample in batch_data:\n",
    "            feat_list.append(sample[0])\n",
    "            label_list.extend(list(sample[1].numpy()))\n",
    "            label_len_list.append(len(sample[1]))\n",
    "            \n",
    "        feat = torch.stack(feat_list,0)\n",
    "        label = torch.LongTensor(label_list)\n",
    "        label_len = torch.LongTensor(label_len_list)\n",
    "        \n",
    "        return feat,label,label_len\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee91b1c",
   "metadata": {},
   "source": [
    "#### Dataloader和loss测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28d6ed96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(-4.3469, grad_fn=<MeanBackward0>) \n",
      "\n",
      "y: torch.Size([15, 20, 11]) \n",
      "\n",
      "label:\n",
      " tensor([ 2, 10,  3,  4,  3,  6, 10,  4,  4,  2,  4,  4,  3,  9,  8,  5,  5,  2,\n",
      "         3,  9,  2,  7,  3,  4,  7,  4,  5,  3,  6,  9,  2,  7,  3,  4,  8, 10,\n",
      "         6,  4,  3,  3,  3,  7,  3]) \n",
      "\n",
      "y_len:\n",
      " tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15]) \n",
      "\n",
      "label_len:\n",
      " tensor([2, 2, 2, 2, 2, 2, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_myloss():\n",
    "    test_data = test_dataset()\n",
    "    test_dataloader = MyDataLoader(test_data,batch_size=20).data_loader\n",
    "    \n",
    "    for epoch1 in test_dataloader:\n",
    "        epoch1_feat,epoch1_label,epoch1_label_len = epoch1\n",
    "        break\n",
    "        \n",
    "    test_model = MyModel()\n",
    "    \n",
    "    epoch1_hat = test_model(epoch1_feat)\n",
    "    epoch1_hat_len = torch.tensor([len(epoch1_hat)]*epoch1_hat.shape[1])\n",
    "\n",
    "    #epoch1_hat(T,N,C),\n",
    "    #epoch1_label,所有数据的label水平拼接成一个row\n",
    "    #epoch1_hat_len,每个数据的len值为T，将T的值复制N次水平拼接成一个row即为输入\n",
    "    #epoch1_label_len,所有数据的label的长度值拼接成一个row\n",
    "    my_loss = MyLoss().ctc\n",
    "    loss = my_loss(epoch1_hat,epoch1_label,epoch1_hat_len,epoch1_label_len)\n",
    "    \n",
    "    return  loss,epoch1_hat,epoch1_label,epoch1_hat_len,epoch1_label_len\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    loss,epoch1_hat,epoch1_label,epoch1_hat_len,epoch1_label_len = test_myloss()\n",
    "    print('loss:',loss,'\\n')\n",
    "    print('y:',epoch1_hat.shape,'\\n')\n",
    "    print('label:\\n',epoch1_label,'\\n')\n",
    "    print('y_len:\\n',epoch1_hat_len,'\\n')\n",
    "    print('label_len:\\n',epoch1_label_len,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e80f4b",
   "metadata": {},
   "source": [
    "### 训练模块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5520beb0",
   "metadata": {},
   "source": [
    "#### 训练逻辑\n",
    "主要功能：  \n",
    "·　进度显示  \n",
    "·　固定随机种子  \n",
    "·　学习率自动调整  \n",
    "·　梯度裁剪  \n",
    "·　模型存储  \n",
    "·　模型读取"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc19e2d3",
   "metadata": {},
   "source": [
    "#### 创建训练类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3015b0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTrain():\n",
    "    def __init__(self,max_epoch=1,batch_size=20,lr=0.001,random_seed=1,grad_threshold=10):\n",
    "        self.max_epoch = max_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.random_seed = random_seed\n",
    "        self.grad_threshold = grad_threshold\n",
    "    \n",
    "    def train(self):\n",
    "        import json\n",
    "        import glob\n",
    "        \n",
    "        max_epoch,batch_size,grad_threshold = self.max_epoch,self.batch_size,self.grad_threshold\n",
    "        \n",
    "        #fix random seed\n",
    "        if self.random_seed is not None:\n",
    "            self.fix_seed()\n",
    "            \n",
    "        #creat dataset instance\n",
    "        input_size = (240,480)\n",
    "        img_path = glob.glob('../input/train/*.png')\n",
    "        label_content = json.load(open('../input/train.json'))\n",
    "        img_path.sort()\n",
    "        img_transforms = CreatTransforms(input_size).img_transforms\n",
    "        my_dataset = MyDataset(img_path,label_content,img_transforms)\n",
    "        \n",
    "        #creat model and loss instance\n",
    "        crnn_model = MyModel()\n",
    "        ctc_loss = MyLoss().ctc\n",
    "        if torch.cuda.is_available():\n",
    "            crnn_model.cuda()\n",
    "            ctc_loss.cuda()\n",
    "        \n",
    "        #creat optim instance\n",
    "        adam_optimizer = torch.optim.Adam(crnn_model.parameters(),lr=self.lr)\n",
    "        \n",
    "        batch_device = next(iter(crnn_model.parameters())).device\n",
    "        print('train device:',batch_device)\n",
    "        \n",
    "        #train each epoch\n",
    "        for epoch_index in range(max_epoch):\n",
    "            \n",
    "            loss_list=[]\n",
    "            acc_list=[]\n",
    "            \n",
    "            #creat dataloader instance\n",
    "            from tqdm import tqdm\n",
    "            my_dataloader = MyDataLoader(my_dataset,batch_size=batch_size,shuffle=True).data_loader\n",
    "            my_dataloader = tqdm(my_dataloader,ncols=120)\n",
    "            \n",
    "            #train each batch data\n",
    "            for batch_index,batch_data in enumerate(my_dataloader):\n",
    "                \n",
    "                batch_feat = batch_data[0]\n",
    "                batch_label = batch_data[1]\n",
    "                batch_label_len = batch_data[2]\n",
    "                if torch.cuda.is_available():\n",
    "                    batch_feat = batch_feat.cuda()\n",
    "                    batch_label = batch_label.cuda()\n",
    "                    batch_label_len = batch_label_len.cuda()\n",
    "                    \n",
    "                adam_optimizer.zero_grad()\n",
    "                \n",
    "                batch_y_hat = crnn_model(batch_feat)\n",
    "                batch_y_hat_len = torch.LongTensor([len(batch_y_hat)]*batch_size)\n",
    "                if torch.cuda.is_available():\n",
    "                    batch_y_hat_len = batch_y_hat_len.cuda()\n",
    "                    \n",
    "                batch_loss = ctc_loss(batch_y_hat,batch_label,batch_y_hat_len,batch_label_len)\n",
    "                loss_list.append(batch_loss)\n",
    "                batch_loss.backward()\n",
    "                \n",
    "                #进行梯度裁剪，避免梯度爆炸\n",
    "                nn.utils.clip_grad_norm_(crnn_model.parameters(),grad_threshold)\n",
    "                                \n",
    "                #更新模型参数\n",
    "                adam_optimizer.step()\n",
    "                \n",
    "                #调整学习率\n",
    "                adam_optimizer.param_groups[0]['lr'] = self.lr*(0.8**(epoch_index//10))\n",
    "                \n",
    "                #更新tqdm进度条的描述内容\n",
    "                batch_acc = self.get_accurancy(batch_y_hat,batch_label,batch_label_len)\n",
    "                acc_list.append(batch_acc)\n",
    "                batch_lr = adam_optimizer.param_groups[0]['lr']\n",
    "                batch_loss_mean = sum(loss_list)/len(loss_list)\n",
    "                batch_acc_mean = sum(acc_list)/len(acc_list)\n",
    "                my_dataloader.set_description(f'epoch{epoch_index}| batch{batch_index}| loss: {round(batch_loss.item(),3)}| lr: {batch_lr}| loss_mean: {round(batch_loss_mean.item(),3)}| batch_acc: {round(batch_acc_mean,3)}')\n",
    "\n",
    "    \n",
    "    def fix_seed(self):\n",
    "        import random\n",
    "        import numpy as np\n",
    "        import torch\n",
    "        \n",
    "        random.seed(self.random_seed)\n",
    "        np.random.seed(self.random_seed)\n",
    "        torch.random.manual_seed(self.random_seed)\n",
    "        torch.cuda.manual_seed_all(self.random_seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        \n",
    "    def get_accurancy(self,y_hat,label,label_len):\n",
    "        #y_hat(T,N,C)\n",
    "        y_hat = y_hat.permute(1,0,2).cpu()\n",
    "#         print('y_hat:',y_hat[0])\n",
    "        label =label.cpu()\n",
    "        label_len = label_len.cpu()\n",
    "        #pred(N,T)\n",
    "        pred = torch.argmax(y_hat,dim=2)\n",
    "#         print('pred:',pred[0])\n",
    "        batch_size = pred.shape[0]\n",
    "        acc = 0\n",
    "        #decode\n",
    "        for i in range(batch_size):\n",
    "            raw_pred_list = list(pred[i].numpy())\n",
    "#             print('raw_pred_list:',raw_pred_list)\n",
    "            pred_data = []\n",
    "            for j in range(len(raw_pred_list)):\n",
    "                if j == 0 and raw_pred_list[0] != 0:\n",
    "                    pred_data.append(raw_pred_list[0]-1)\n",
    "                if j != 0 and raw_pred_list[j] != raw_pred_list[j-1] and raw_pred_list[j] != 0:\n",
    "                    pred_data.append(raw_pred_list[j]-1)\n",
    "#             print('pred_data:',pred_data)\n",
    "            \n",
    "            label_start_index = int(torch.sum(label_len[:i]).item())\n",
    "            label_end_index = int(label_len[i].item())+label_start_index\n",
    "            label_data = list(label[label_start_index:label_end_index].int().numpy())\n",
    "            label_data = [x-1 for x in label_data]\n",
    "#             print(f'pred_data:{pred_data},label_data:{label_data}\\n')\n",
    "            \n",
    "            if pred_data == label_data:\n",
    "                acc+=1\n",
    "        acc/=batch_size\n",
    "        return acc\n",
    "        \n",
    "    def model_save(self):\n",
    "        pass\n",
    "    def model_load(self):\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec28803",
   "metadata": {},
   "source": [
    "#### 训练类测试"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa0cb56",
   "metadata": {},
   "source": [
    "##### 直接训练结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ab94283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch0| batch1499| loss: -6.488| lr: 0.001| loss_mean: -6.924| batch_acc: 0.0: 100%|█| 1500/1500 [11:43<00:00,  2.13it/s\n",
      "epoch1| batch1499| loss: -7.004| lr: 0.001| loss_mean: -6.926| batch_acc: 0.0: 100%|█| 1500/1500 [17:36<00:00,  1.42it/s\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    test_train = MyTrain(max_epoch=2)\n",
    "    test_train.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f6b5e3",
   "metadata": {},
   "source": [
    "##### 原因分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b43aaa8",
   "metadata": {},
   "source": [
    "　　从上面的训练情况来看，训练失败了。主要是两个方面有直观的问题，可以看到，使用模型训练2轮之后，第一个问题是acc为0，这个不太正常，查看训练结果之后发现经过第一个ｂａｔｃｈ之后，预测的标签结果全是０，说明模型没有学习动，欠拟合。第二个问题是ｌｏｓｓ一直为负数，经过查找资料，发现如果预测的结果经过ｓｏｆｔｍａｘ归一化之后再传给ＣＴＣｌｏｓｓ就会出现损失为负数的情况，如果预测的结果不经过ｓｏｆｔｍａｘ就传给ＣＴＣｌｏｓｓ的话，损失结果就为正值，这个问题好解决。  \n",
    "  \n",
    "　　回到第一个问题，学不动欠拟合的问题。欠拟合学不动可能有多种可能原因造成，从目前了解的来看，有两种原因，一种是参数太多导致模型过于复杂，导致某些层无法起作用，另外一种是，某些特定的超参数的设置出了问题。  \n",
    "  \n",
    "　　参数过于复杂导致的欠拟合问题。上面的模型在ｆｅａｔ到ｓｅｑ转换的过程中使用了一个ｃｏｎｖ２ｄ层，ｉｎｆｅａｔ和ｏｕｔｆｅａｔ都是５１２，虽然看起来好像没有改变对象的ｃｈａｎｎｅｌ数量，但是对于卷积神经网络来说，这意味着这层有５１２个卷积核的参数需要学习，而这种卷积层想要达到的目的其实只是把特征的Ｈｅｉｇｈｔ维度降维１。这个操作涉及的参数很多，目的又很明确，就是在高度上做平均，很容易导致反向传播学不出来能达到在ｈｅｉｇｈｔ方向上达到ｍｅａｎ效果的卷积核，从而导致训练失败，而这也是这个模型无法进行训练的最重要的原因。  \n",
    "  \n",
    "　　另外，对于超参数的设置出了问题在本项目中，主要体现在T的设置和图片传入cnn的尺寸问题。  \n",
    "　　首先说超参数T的设置问题：对于ＣＴＣ算法来说，我们需要把一张图片竖向切成一定的条数来进行损失的计算，这个条数也就是我们的时间序列Ｔ，CTC api中提示说这个T一般要设置为数据最长label的两倍加1，也就是说$T≥2*Max(len(label))+1$，对我们这个数据集来说，最长的label的长度是6，所以理论上来说T最少要设置为13，CTCloss才能正常运算，但是，有一个问题是，这个T的上限是多少，T取多少的时候能达到最好的效果呢？这个问题要解决，比较粗暴的方法可以用参数搜索，将13以上的参数都搜索一遍，看看参数的变化，另外一种方式，可以从ＣＴＣ的原理上进行一下判断，CTC的原理是将照片切成T个竖条，然后分别识别这T个竖条中的内容，看是某一个数字还是空白，而要想让CTC能正常识别，每个竖条的尺寸不能太小，不然可能什么也看不出来，极端点，如果切完之后每一个竖条的宽度为1，那让人去看，人也看不出来，所以可以先做个实验，看把一张图片切成多少条的情况下，还能比较好的识别出每条的内容，因此来初步确定超参数T，然后在这个范围内再进行参数搜索准确的确定T。  \n",
    "　　其次，关于输入给ｃｎｎ的图片大小问题，这中间涉及到两个个参数之间的权衡和转换，首先是输入给ＣＮＮ的照片的尺寸，假如形状为（Ｎ，Ｃ，Ｈ，Ｗ），第二是输入给ＲＮＮ的特征的尺寸，假如形状为（Ｔ，Ｎ，ＩＮ），其中Ｔ就是上面说的把一张照片切成Ｔ条。可以看到这两个形状的ｓｉｚｅ是不一样的，ＲＮＮ的输入要少一维，从流程上来说，ＣＮＮ要把Ｈ压缩成１，然后去掉Ｈ维度然后传给ＲＮＮ，这里面的问题是，我们使用的ＣＮＮ是训练好的ｒｅｓｎｅｔ１８，参数和卷积层都确定好了，我们不能随意增减卷积层，不然就需要重新训练卷积参数，但是如果使用带预训练的ｒｅｓｎｅｔ的问题是，我们的Ｗ经过ｒｅｓｎｅｔ之后要刚好等于ＩＮ，同时对经过ｒｅｓｎｅｔ的Ｈ使用ｍｅａｎ得到Ｈ为１的特征，才能直接传给ＲＮＮ，根据Ｔ的最小值为１３，而ｒｅｓｎｅｔ计算过程会缩小图片尺寸约到３０分之一，导致了我们的图片输入宽度最小要为４００,这将放大很多图片，也让整个计算量比较大，但是不管怎么说，这个方式通过ｍｅａｎ和提前计算，似的ＣＮＮ和ＲＮＮ在衔接的时候无需使用额外的卷积层进行转换，节省了参数，理论上来说让模型变得更加简单了，应该能训练动了。\n",
    "  \n",
    "　　下面将这个ｆｅａｔ到ｓｅｑ的卷积操作换成一个ｍｅａｎ操作，减少模型的参数，看看是否能让模型训练动。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fb17989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,img_path,label_content,transforms=None):\n",
    "        self.img_path = img_path\n",
    "        self.label_content = label_content\n",
    "        if transforms is not None:\n",
    "            self.transforms = transforms\n",
    "        else:\n",
    "            self.transforms = None\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        img_path = self.img_path[index]\n",
    "        img_cv2 = cv2.imread(img_path)  #return （H，W，C），C list by B，G，R\n",
    "        img_cv2 = img_cv2[:,:,::-1].copy()  #return （H，W，C），C list by R，G，B\n",
    "        img_tensor = torch.from_numpy(img_cv2).permute(2,0,1).float()  #return （C，H，W），C list by R，G，B\n",
    "        if self.transforms is not None:\n",
    "            img_tensor = self.transforms(img_tensor)\n",
    "        assert img_tensor.shape[0] == 3\n",
    "        \n",
    "        img_name = os.path.split(img_path)[1]\n",
    "        img_label = self.label_content[img_name]['label']\n",
    "        #transform the label 0~9 to index 1~10，index 0 is for blank\n",
    "        img_label_index = [x+1 for x in img_label]\n",
    "        assert isinstance(img_label_index,list) == True\n",
    "        \n",
    "        return img_tensor,torch.LongTensor(img_label_index)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_path)\n",
    "\n",
    "    \n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #self.cnn: input(N,3,240,480),output(N,512,8,15)\n",
    "        trained_resnet18 = models.resnet18(weights='DEFAULT')\n",
    "        self.cnn = nn.Sequential(trained_resnet18.conv1,\n",
    "                                 trained_resnet18.bn1,\n",
    "                                 trained_resnet18.relu,\n",
    "                                 trained_resnet18.maxpool,\n",
    "                                 trained_resnet18.layer1,\n",
    "                                 trained_resnet18.layer2,\n",
    "                                 trained_resnet18.layer3,\n",
    "                                 trained_resnet18.layer4)\n",
    "        #self.rnn: input(15,N,512),output(15,N,64*2)\n",
    "        self.rnn = nn.GRU(input_size=512,hidden_size=64,bidirectional=True)\n",
    "        #self.linear: input(15,N,64*2),output(15,N,11)\n",
    "        self.linear = nn.Linear(128,11)\n",
    "        #self.softmax: input(15,N,11),output(15,N,11) \n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        X = self.cnn(X)\n",
    "        X = X.mean(dim=2)\n",
    "        X = X.permute((2,0,1))\n",
    "        X,_ = self.rnn(X)\n",
    "        X = self.linear(X)\n",
    "        y_hat = self.softmax(X)\n",
    "        return y_hat\n",
    "    \n",
    "    \n",
    "class MyLoss():\n",
    "    def __init__(self):\n",
    "        self.ctc = nn.CTCLoss(blank=0)\n",
    "        \n",
    "\n",
    "class MyDataLoader():\n",
    "    def __init__(self,my_dataset,batch_size=20,use_my_collate=True,shuffle=False,):\n",
    "        from torch.utils.data import DataLoader\n",
    "        if use_my_collate:\n",
    "            self.data_loader = DataLoader(my_dataset,batch_size=batch_size,shuffle=shuffle,collate_fn=self.my_collate)\n",
    "        else:\n",
    "            self.data_loader = DataLoader(my_dataset,batch_size=batch_size,shuffle=shuffle)\n",
    "        \n",
    "    def my_collate(self,batch_data):\n",
    "        '''\n",
    "        batch_data shape: [(feat_tensor1,label_tensor1),(feat_tensor2,label_tensor2),……]\n",
    "        '''\n",
    "        feat_list =[]\n",
    "        label_list = []\n",
    "        label_len_list = []\n",
    "        for sample in batch_data:\n",
    "            feat_list.append(sample[0])\n",
    "            label_list.extend(list(sample[1].numpy()))\n",
    "            label_len_list.append(len(sample[1]))\n",
    "            \n",
    "        feat = torch.stack(feat_list,0)\n",
    "        label = torch.LongTensor(label_list)\n",
    "        label_len = torch.LongTensor(label_len_list)\n",
    "        \n",
    "        return feat,label,label_len\n",
    "\n",
    "\n",
    "class MyTrain():\n",
    "    def __init__(self,max_epoch=1,batch_size=20,lr=0.001,random_seed=1,grad_threshold=10):\n",
    "        self.max_epoch = max_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.random_seed = random_seed\n",
    "        self.grad_threshold = grad_threshold\n",
    "    \n",
    "    def train(self):\n",
    "        import json\n",
    "        import glob\n",
    "        \n",
    "        max_epoch,batch_size,grad_threshold = self.max_epoch,self.batch_size,self.grad_threshold\n",
    "        \n",
    "        #fix random seed\n",
    "        if self.random_seed is not None:\n",
    "            self.fix_seed()\n",
    "            \n",
    "        #creat dataset instance\n",
    "        input_size = (240,480)\n",
    "        img_path = glob.glob('../input/train/*.png')\n",
    "        label_content = json.load(open('../input/train.json'))\n",
    "        img_path.sort()\n",
    "        img_transforms = CreatTransforms(input_size).img_transforms\n",
    "        my_dataset = MyDataset(img_path,label_content,img_transforms)\n",
    "        \n",
    "        #creat model and loss instance\n",
    "        crnn_model = MyModel()\n",
    "        ctc_loss = MyLoss().ctc\n",
    "        if torch.cuda.is_available():\n",
    "            crnn_model.cuda()\n",
    "            ctc_loss.cuda()\n",
    "        \n",
    "        #creat optim instance\n",
    "        adam_optimizer = torch.optim.Adam(crnn_model.parameters(),lr=self.lr)\n",
    "        \n",
    "        batch_device = next(iter(crnn_model.parameters())).device\n",
    "        print('train device:',batch_device)\n",
    "        \n",
    "        #train each epoch\n",
    "        for epoch_index in range(max_epoch):\n",
    "            \n",
    "            loss_list=[]\n",
    "            acc_list=[]\n",
    "            \n",
    "            #creat dataloader instance\n",
    "            from tqdm import tqdm\n",
    "            my_dataloader = MyDataLoader(my_dataset,batch_size=batch_size,shuffle=True).data_loader\n",
    "            my_dataloader = tqdm(my_dataloader,ncols=120)\n",
    "            \n",
    "            #train each batch data\n",
    "            for batch_index,batch_data in enumerate(my_dataloader):\n",
    "                \n",
    "                batch_feat = batch_data[0]\n",
    "                batch_label = batch_data[1]\n",
    "                batch_label_len = batch_data[2]\n",
    "                if torch.cuda.is_available():\n",
    "                    batch_feat = batch_feat.cuda()\n",
    "                    batch_label = batch_label.cuda()\n",
    "                    batch_label_len = batch_label_len.cuda()\n",
    "                    \n",
    "                adam_optimizer.zero_grad()\n",
    "                \n",
    "                batch_y_hat = crnn_model(batch_feat)\n",
    "                batch_y_hat_len = torch.LongTensor([len(batch_y_hat)]*batch_size)\n",
    "                if torch.cuda.is_available():\n",
    "                    batch_y_hat_len = batch_y_hat_len.cuda()\n",
    "                    \n",
    "                batch_loss = ctc_loss(batch_y_hat,batch_label,batch_y_hat_len,batch_label_len)\n",
    "                loss_list.append(batch_loss)\n",
    "                batch_loss.backward()\n",
    "                \n",
    "                #进行梯度裁剪，避免梯度爆炸\n",
    "                nn.utils.clip_grad_norm_(crnn_model.parameters(),grad_threshold)\n",
    "                                \n",
    "                #更新模型参数\n",
    "                adam_optimizer.step()\n",
    "                \n",
    "                #调整学习率\n",
    "                adam_optimizer.param_groups[0]['lr'] = self.lr*(0.8**(epoch_index//10))\n",
    "                \n",
    "                #更新tqdm进度条的描述内容\n",
    "                batch_acc = self.get_accurancy(batch_y_hat,batch_label,batch_label_len)\n",
    "                acc_list.append(batch_acc)\n",
    "                batch_lr = adam_optimizer.param_groups[0]['lr']\n",
    "                batch_loss_mean = sum(loss_list)/len(loss_list)\n",
    "                batch_acc_mean = sum(acc_list)/len(acc_list)\n",
    "                my_dataloader.set_description(f'epoch{epoch_index}| batch{batch_index}| loss: {round(batch_loss.item(),3)}| lr: {batch_lr}| loss_mean: {round(batch_loss_mean.item(),3)}| batch_acc: {round(batch_acc_mean,3)}')\n",
    "\n",
    "    \n",
    "    def fix_seed(self):\n",
    "        import random\n",
    "        import numpy as np\n",
    "        import torch\n",
    "        \n",
    "        random.seed(self.random_seed)\n",
    "        np.random.seed(self.random_seed)\n",
    "        torch.random.manual_seed(self.random_seed)\n",
    "        torch.cuda.manual_seed_all(self.random_seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        \n",
    "    def get_accurancy(self,y_hat,label,label_len):\n",
    "        #y_hat(T,N,C)\n",
    "        y_hat = y_hat.permute(1,0,2).cpu()\n",
    "#         print('y_hat:',y_hat[0])\n",
    "        label =label.cpu()\n",
    "        label_len = label_len.cpu()\n",
    "        #pred(N,T)\n",
    "        pred = torch.argmax(y_hat,dim=2)\n",
    "#         print('pred:',pred[0])\n",
    "        batch_size = pred.shape[0]\n",
    "        acc = 0\n",
    "        #decode\n",
    "        for i in range(batch_size):\n",
    "            raw_pred_list = list(pred[i].numpy())\n",
    "#             print('raw_pred_list:',raw_pred_list)\n",
    "            pred_data = []\n",
    "            for j in range(len(raw_pred_list)):\n",
    "                if j == 0 and raw_pred_list[0] != 0:\n",
    "                    pred_data.append(raw_pred_list[0]-1)\n",
    "                if j != 0 and raw_pred_list[j] != raw_pred_list[j-1] and raw_pred_list[j] != 0:\n",
    "                    pred_data.append(raw_pred_list[j]-1)\n",
    "#             print('pred_data:',pred_data)\n",
    "            \n",
    "            label_start_index = int(torch.sum(label_len[:i]).item())\n",
    "            label_end_index = int(label_len[i].item())+label_start_index\n",
    "            label_data = list(label[label_start_index:label_end_index].int().numpy())\n",
    "            label_data = [x-1 for x in label_data]\n",
    "#             print(f'pred_data:{pred_data},label_data:{label_data}\\n')\n",
    "            \n",
    "            if pred_data == label_data:\n",
    "                acc+=1\n",
    "        acc/=batch_size\n",
    "        return acc\n",
    "        \n",
    "    def model_save(self):\n",
    "        pass\n",
    "    def model_load(self):\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cddb6969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch0| batch1499| loss: -6.524| lr: 0.001| loss_mean: -7.024| batch_acc: 0.007: 100%|█| 1500/1500 [15:02<00:00,  1.66it\n",
      "epoch1| batch1499| loss: -7.406| lr: 0.001| loss_mean: -7.101| batch_acc: 0.03: 100%|█| 1500/1500 [15:50<00:00,  1.58it/\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    test_train = MyTrain(max_epoch=2)\n",
    "    test_train.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a98f01",
   "metadata": {},
   "source": [
    "　　可以看到，ｂａｔｃｈ＿ａｃｃ的值发生了变化，不再是０，说明模型开始学习动了，但是很低。另外还有ｌｏｓｓ，仍然是负数。经过几次实验发现，保持ｉｎｐｕｔ和Ｔ一致的情况下，使用卷积来连接ｃｎｎ和ｒｎｎ比直接ｍｅａｎ的效果更好，损失更低的同时ａｃｃ也更高，但是也不能像最开始一样，让卷积的维度太高，这样会导致模型训练不动，下面看一下当ｉｎｐｕｔ和Ｔ还是和上面一样，保持（３，２４０，４８０）和１５的时候，做完ｍｅａｎ之后再做一次卷积，看看跟上面效果有什么差别，另外，这次网络前向传播的结果不再经过ｓｏｆｔｍａｘ而是由ｌｉｎｅａｒ直接传给ＣＴＣ，ｌｏｓｓ将恢复正常。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "737f0e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreatTransforms:\n",
    "    def __init__(self,input_size):\n",
    "        from torchvision import transforms as transforms\n",
    "        self.img_transforms = transforms.Compose([transforms.Resize(size=input_size),\n",
    "                                                  transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,img_path,label_content,transforms=None):\n",
    "        self.img_path = img_path\n",
    "        self.label_content = label_content\n",
    "        if transforms is not None:\n",
    "            self.transforms = transforms\n",
    "        else:\n",
    "            self.transforms = None\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        img_path = self.img_path[index]\n",
    "        img_cv2 = cv2.imread(img_path)  #return （H，W，C），C list by B，G，R\n",
    "        img_cv2 = img_cv2[:,:,::-1].copy()  #return （H，W，C），C list by R，G，B\n",
    "        img_tensor = torch.from_numpy(img_cv2).permute(2,0,1).float()  #return （C，H，W），C list by R，G，B\n",
    "        if self.transforms is not None:\n",
    "            img_tensor = self.transforms(img_tensor)\n",
    "        assert img_tensor.shape[0] == 3\n",
    "        \n",
    "        img_name = os.path.split(img_path)[1]\n",
    "        img_label = self.label_content[img_name]['label']\n",
    "        #transform the label 0~9 to index 1~10，index 0 is for blank\n",
    "        img_label_index = [x+1 for x in img_label]\n",
    "        assert isinstance(img_label_index,list) == True\n",
    "        \n",
    "        return img_tensor,torch.LongTensor(img_label_index)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_path)\n",
    "\n",
    "    \n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #self.cnn: input(N,3,240,480),output(N,512,8,15)\n",
    "        trained_resnet18 = models.resnet18(weights='DEFAULT')\n",
    "        self.cnn = nn.Sequential(trained_resnet18.conv1,\n",
    "                                 trained_resnet18.bn1,\n",
    "                                 trained_resnet18.relu,\n",
    "                                 trained_resnet18.maxpool,\n",
    "                                 trained_resnet18.layer1,\n",
    "                                 trained_resnet18.layer2,\n",
    "                                 trained_resnet18.layer3,\n",
    "                                 trained_resnet18.layer4)\n",
    "        self.feat_to_seq = nn.Conv2d(15,15,kernel_size=(1,1),stride=1)\n",
    "        #self.rnn: input(15,N,512),output(15,N,64*2)\n",
    "        self.rnn = nn.GRU(input_size=512,hidden_size=64,bidirectional=True)\n",
    "        #self.linear: input(15,N,64*2),output(15,N,11)\n",
    "        self.linear = nn.Linear(128,11)\n",
    "        #self.softmax: input(15,N,11),output(15,N,11) \n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        X = self.cnn(X)\n",
    "        X = X.mean(dim=2,keepdim=True)\n",
    "        X = X.permute(0,3,1,2)\n",
    "        X = self.feat_to_seq(X)\n",
    "        X = X.squeeze(dim=3)\n",
    "        X = X.permute(1,0,2)\n",
    "        X,_ = self.rnn(X)\n",
    "        X = self.linear(X)\n",
    "        y_hat = self.softmax(X)\n",
    "        return y_hat\n",
    "    \n",
    "    \n",
    "class MyLoss():\n",
    "    def __init__(self):\n",
    "        self.ctc = nn.CTCLoss(blank=0)\n",
    "        \n",
    "\n",
    "class MyDataLoader():\n",
    "    def __init__(self,my_dataset,batch_size=20,use_my_collate=True,shuffle=False,):\n",
    "        from torch.utils.data import DataLoader\n",
    "        if use_my_collate:\n",
    "            self.data_loader = DataLoader(my_dataset,batch_size=batch_size,shuffle=shuffle,collate_fn=self.my_collate)\n",
    "        else:\n",
    "            self.data_loader = DataLoader(my_dataset,batch_size=batch_size,shuffle=shuffle)\n",
    "        \n",
    "    def my_collate(self,batch_data):\n",
    "        '''\n",
    "        batch_data shape: [(feat_tensor1,label_tensor1),(feat_tensor2,label_tensor2),……]\n",
    "        '''\n",
    "        feat_list =[]\n",
    "        label_list = []\n",
    "        label_len_list = []\n",
    "        for sample in batch_data:\n",
    "            feat_list.append(sample[0])\n",
    "            label_list.extend(list(sample[1].numpy()))\n",
    "            label_len_list.append(len(sample[1]))\n",
    "            \n",
    "        feat = torch.stack(feat_list,0)\n",
    "        label = torch.LongTensor(label_list)\n",
    "        label_len = torch.LongTensor(label_len_list)\n",
    "        \n",
    "        return feat,label,label_len\n",
    "\n",
    "\n",
    "class MyTrain():\n",
    "    def __init__(self,max_epoch=1,batch_size=20,lr=0.001,random_seed=1,grad_threshold=10):\n",
    "        self.max_epoch = max_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.random_seed = random_seed\n",
    "        self.grad_threshold = grad_threshold\n",
    "    \n",
    "    def train(self):\n",
    "        import json\n",
    "        import glob\n",
    "        \n",
    "        max_epoch,batch_size,grad_threshold = self.max_epoch,self.batch_size,self.grad_threshold\n",
    "        \n",
    "        #fix random seed\n",
    "        if self.random_seed is not None:\n",
    "            self.fix_seed()\n",
    "            \n",
    "        #creat dataset instance\n",
    "        input_size = (240,480)\n",
    "        img_path = glob.glob('../input/train/*.png')\n",
    "        label_content = json.load(open('../input/train.json'))\n",
    "        img_path.sort()\n",
    "        img_transforms = CreatTransforms(input_size).img_transforms\n",
    "        my_dataset = MyDataset(img_path,label_content,img_transforms)\n",
    "        \n",
    "        #creat model and loss instance\n",
    "        crnn_model = MyModel()\n",
    "        ctc_loss = MyLoss().ctc\n",
    "        if torch.cuda.is_available():\n",
    "            crnn_model.cuda()\n",
    "            ctc_loss.cuda()\n",
    "        \n",
    "        #creat optim instance\n",
    "        adam_optimizer = torch.optim.Adam(crnn_model.parameters(),lr=self.lr)\n",
    "        \n",
    "        batch_device = next(iter(crnn_model.parameters())).device\n",
    "        print('train device:',batch_device)\n",
    "        \n",
    "        #train each epoch\n",
    "        for epoch_index in range(max_epoch):\n",
    "            \n",
    "            loss_list=[]\n",
    "            acc_list=[]\n",
    "            \n",
    "            #creat dataloader instance\n",
    "            from tqdm import tqdm\n",
    "            my_dataloader = MyDataLoader(my_dataset,batch_size=batch_size,shuffle=True).data_loader\n",
    "            my_dataloader = tqdm(my_dataloader,ncols=120)\n",
    "            \n",
    "            #train each batch data\n",
    "            for batch_index,batch_data in enumerate(my_dataloader):\n",
    "                \n",
    "                batch_feat = batch_data[0]\n",
    "                batch_label = batch_data[1]\n",
    "                batch_label_len = batch_data[2]\n",
    "                if torch.cuda.is_available():\n",
    "                    batch_feat = batch_feat.cuda()\n",
    "                    batch_label = batch_label.cuda()\n",
    "                    batch_label_len = batch_label_len.cuda()\n",
    "                    \n",
    "                adam_optimizer.zero_grad()\n",
    "                \n",
    "                batch_y_hat = crnn_model(batch_feat)\n",
    "                batch_y_hat_len = torch.LongTensor([len(batch_y_hat)]*batch_size)\n",
    "                if torch.cuda.is_available():\n",
    "                    batch_y_hat_len = batch_y_hat_len.cuda()\n",
    "                    \n",
    "                batch_loss = ctc_loss(batch_y_hat,batch_label,batch_y_hat_len,batch_label_len)\n",
    "                loss_list.append(batch_loss)\n",
    "                batch_loss.backward()\n",
    "                \n",
    "                #进行梯度裁剪，避免梯度爆炸\n",
    "                nn.utils.clip_grad_norm_(crnn_model.parameters(),grad_threshold)\n",
    "                                \n",
    "                #更新模型参数\n",
    "                adam_optimizer.step()\n",
    "                \n",
    "                #调整学习率\n",
    "                adam_optimizer.param_groups[0]['lr'] = self.lr*(0.8**(epoch_index//10))\n",
    "                \n",
    "                #更新tqdm进度条的描述内容\n",
    "                batch_acc = self.get_accurancy(batch_y_hat,batch_label,batch_label_len)\n",
    "                acc_list.append(batch_acc)\n",
    "                batch_lr = adam_optimizer.param_groups[0]['lr']\n",
    "                batch_loss_mean = sum(loss_list)/len(loss_list)\n",
    "                batch_acc_mean = sum(acc_list)/len(acc_list)\n",
    "                my_dataloader.set_description(f'epoch{epoch_index}| batch{batch_index}| loss: {round(batch_loss.item(),3)}| lr: {batch_lr}| loss_mean: {round(batch_loss_mean.item(),3)}| batch_acc: {round(batch_acc_mean,3)}')\n",
    "\n",
    "    \n",
    "    def fix_seed(self):\n",
    "        import random\n",
    "        import numpy as np\n",
    "        import torch\n",
    "        \n",
    "        random.seed(self.random_seed)\n",
    "        np.random.seed(self.random_seed)\n",
    "        torch.random.manual_seed(self.random_seed)\n",
    "        torch.cuda.manual_seed_all(self.random_seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        \n",
    "    def get_accurancy(self,y_hat,label,label_len):\n",
    "        #y_hat(T,N,C)\n",
    "        y_hat = y_hat.permute(1,0,2).cpu()\n",
    "#         print('y_hat:',y_hat[0])\n",
    "        label =label.cpu()\n",
    "        label_len = label_len.cpu()\n",
    "        #pred(N,T)\n",
    "        pred = torch.argmax(y_hat,dim=2)\n",
    "#         print('pred:',pred[0])\n",
    "        batch_size = pred.shape[0]\n",
    "        acc = 0\n",
    "        #decode\n",
    "        for i in range(batch_size):\n",
    "            raw_pred_list = list(pred[i].numpy())\n",
    "#             print('raw_pred_list:',raw_pred_list)\n",
    "            pred_data = []\n",
    "            for j in range(len(raw_pred_list)):\n",
    "                if j == 0 and raw_pred_list[0] != 0:\n",
    "                    pred_data.append(raw_pred_list[0]-1)\n",
    "                if j != 0 and raw_pred_list[j] != raw_pred_list[j-1] and raw_pred_list[j] != 0:\n",
    "                    pred_data.append(raw_pred_list[j]-1)\n",
    "#             print('pred_data:',pred_data)\n",
    "            \n",
    "            label_start_index = int(torch.sum(label_len[:i]).item())\n",
    "            label_end_index = int(label_len[i].item())+label_start_index\n",
    "            label_data = list(label[label_start_index:label_end_index].int().numpy())\n",
    "            label_data = [x-1 for x in label_data]\n",
    "#             print(f'pred_data:{pred_data},label_data:{label_data}\\n')\n",
    "            \n",
    "            if pred_data == label_data:\n",
    "                acc+=1\n",
    "        acc/=batch_size\n",
    "        return acc\n",
    "        \n",
    "    def model_save(self):\n",
    "        pass\n",
    "    def model_load(self):\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7efb6c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch0| batch1499| loss: -7.438| lr: 0.001| loss_mean: -7.026| batch_acc: 0.017: 100%|█| 1500/1500 [10:21<00:00,  2.41it\n",
      "epoch1| batch1499| loss: -7.187| lr: 0.001| loss_mean: -7.107| batch_acc: 0.044: 100%|█| 1500/1500 [10:19<00:00,  2.42it\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    test_train = MyTrain(max_epoch=2)\n",
    "    test_train.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a1f695",
   "metadata": {},
   "source": [
    "　　下面继续优化，把去掉前向传播中的softmax让loss变成正数，同时，缩小图片input的尺寸，改成200宽100高，保持T=15。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15347f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreatTransforms:\n",
    "    def __init__(self,input_size):\n",
    "        from torchvision import transforms as transforms\n",
    "        self.img_transforms = transforms.Compose([transforms.Resize(size=input_size),\n",
    "                                                  transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,img_path,label_content,transforms=None):\n",
    "        self.img_path = img_path\n",
    "        self.label_content = label_content\n",
    "        if transforms is not None:\n",
    "            self.transforms = transforms\n",
    "        else:\n",
    "            self.transforms = None\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        img_path = self.img_path[index]\n",
    "        img_cv2 = cv2.imread(img_path)  #return （H，W，C），C list by B，G，R\n",
    "        img_cv2 = img_cv2[:,:,::-1].copy()  #return （H，W，C），C list by R，G，B\n",
    "        img_tensor = torch.from_numpy(img_cv2).permute(2,0,1).float()  #return （C，H，W），C list by R，G，B\n",
    "        if self.transforms is not None:\n",
    "            img_tensor = self.transforms(img_tensor)\n",
    "        assert img_tensor.shape[0] == 3\n",
    "        \n",
    "        img_name = os.path.split(img_path)[1]\n",
    "        img_label = self.label_content[img_name]['label']\n",
    "        #transform the label 0~9 to index 1~10，index 0 is for blank\n",
    "        img_label_index = [x+1 for x in img_label]\n",
    "        assert isinstance(img_label_index,list) == True\n",
    "        \n",
    "        return img_tensor,torch.LongTensor(img_label_index)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_path)\n",
    "\n",
    "    \n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #self.cnn: input(N,3,240,480),output(N,512,8,15)\n",
    "        trained_resnet18 = models.resnet18(weights='DEFAULT')\n",
    "        self.cnn = nn.Sequential(trained_resnet18.conv1,\n",
    "                                 trained_resnet18.bn1,\n",
    "                                 trained_resnet18.relu,\n",
    "                                 trained_resnet18.maxpool,\n",
    "                                 trained_resnet18.layer1,\n",
    "                                 trained_resnet18.layer2,\n",
    "                                 trained_resnet18.layer3,\n",
    "                                 trained_resnet18.layer4)\n",
    "        self.feat_to_seq = nn.Conv2d(7,15,kernel_size=(1,1),stride=1)\n",
    "        #self.rnn: input(15,N,512),output(15,N,64*2)\n",
    "        self.rnn = nn.GRU(input_size=512,hidden_size=64,bidirectional=True)\n",
    "        #self.linear: input(15,N,64*2),output(15,N,11)\n",
    "        self.linear = nn.Linear(128,11)\n",
    "        #self.softmax: input(15,N,11),output(15,N,11) \n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        X = self.cnn(X)\n",
    "        X = X.mean(dim=2,keepdim=True)\n",
    "        X = X.permute(0,3,1,2)\n",
    "        X = self.feat_to_seq(X)\n",
    "        X = X.squeeze(dim=3)\n",
    "        X = X.permute(1,0,2)\n",
    "        X,_ = self.rnn(X)\n",
    "        y_hat = self.linear(X)\n",
    "#         y_hat = self.softmax(X)\n",
    "        return y_hat\n",
    "    \n",
    "    \n",
    "class MyLoss():\n",
    "    def __init__(self):\n",
    "        self.ctc = nn.CTCLoss(blank=0)\n",
    "        \n",
    "\n",
    "class MyDataLoader():\n",
    "    def __init__(self,my_dataset,batch_size=20,use_my_collate=True,shuffle=False,):\n",
    "        from torch.utils.data import DataLoader\n",
    "        if use_my_collate:\n",
    "            self.data_loader = DataLoader(my_dataset,batch_size=batch_size,shuffle=shuffle,collate_fn=self.my_collate)\n",
    "        else:\n",
    "            self.data_loader = DataLoader(my_dataset,batch_size=batch_size,shuffle=shuffle)\n",
    "        \n",
    "    def my_collate(self,batch_data):\n",
    "        '''\n",
    "        batch_data shape: [(feat_tensor1,label_tensor1),(feat_tensor2,label_tensor2),……]\n",
    "        '''\n",
    "        feat_list =[]\n",
    "        label_list = []\n",
    "        label_len_list = []\n",
    "        for sample in batch_data:\n",
    "            feat_list.append(sample[0])\n",
    "            label_list.extend(list(sample[1].numpy()))\n",
    "            label_len_list.append(len(sample[1]))\n",
    "            \n",
    "        feat = torch.stack(feat_list,0)\n",
    "        label = torch.LongTensor(label_list)\n",
    "        label_len = torch.LongTensor(label_len_list)\n",
    "        \n",
    "        return feat,label,label_len\n",
    "\n",
    "\n",
    "class MyTrain():\n",
    "    def __init__(self,max_epoch=1,batch_size=20,lr=0.001,random_seed=1,grad_threshold=10):\n",
    "        self.max_epoch = max_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.random_seed = random_seed\n",
    "        self.grad_threshold = grad_threshold\n",
    "    \n",
    "    def train(self):\n",
    "        import json\n",
    "        import glob\n",
    "        \n",
    "        max_epoch,batch_size,grad_threshold = self.max_epoch,self.batch_size,self.grad_threshold\n",
    "        \n",
    "        #fix random seed\n",
    "        if self.random_seed is not None:\n",
    "            self.fix_seed()\n",
    "            \n",
    "        #creat dataset instance\n",
    "        input_size = (100,200)\n",
    "        img_path = glob.glob('../input/train/*.png')\n",
    "        label_content = json.load(open('../input/train.json'))\n",
    "        img_path.sort()\n",
    "        img_transforms = CreatTransforms(input_size).img_transforms\n",
    "        my_dataset = MyDataset(img_path,label_content,img_transforms)\n",
    "        \n",
    "        #creat model and loss instance\n",
    "        crnn_model = MyModel()\n",
    "        ctc_loss = MyLoss().ctc\n",
    "        if torch.cuda.is_available():\n",
    "            crnn_model.cuda()\n",
    "            ctc_loss.cuda()\n",
    "        \n",
    "        #creat optim instance\n",
    "        adam_optimizer = torch.optim.Adam(crnn_model.parameters(),lr=self.lr)\n",
    "        \n",
    "        batch_device = next(iter(crnn_model.parameters())).device\n",
    "        print('train device:',batch_device)\n",
    "        \n",
    "        #train each epoch\n",
    "        for epoch_index in range(max_epoch):\n",
    "            \n",
    "            loss_list=[]\n",
    "            acc_list=[]\n",
    "            \n",
    "            #creat dataloader instance\n",
    "            from tqdm import tqdm\n",
    "            my_dataloader = MyDataLoader(my_dataset,batch_size=batch_size,shuffle=True).data_loader\n",
    "            my_dataloader = tqdm(my_dataloader,ncols=120)\n",
    "            \n",
    "            #train each batch data\n",
    "            for batch_index,batch_data in enumerate(my_dataloader):\n",
    "                \n",
    "                batch_feat = batch_data[0]\n",
    "                batch_label = batch_data[1]\n",
    "                batch_label_len = batch_data[2]\n",
    "                if torch.cuda.is_available():\n",
    "                    batch_feat = batch_feat.cuda()\n",
    "                    batch_label = batch_label.cuda()\n",
    "                    batch_label_len = batch_label_len.cuda()\n",
    "                    \n",
    "                adam_optimizer.zero_grad()\n",
    "                \n",
    "                batch_y_hat = crnn_model(batch_feat)\n",
    "                batch_y_hat_len = torch.LongTensor([len(batch_y_hat)]*batch_size)\n",
    "                if torch.cuda.is_available():\n",
    "                    batch_y_hat_len = batch_y_hat_len.cuda()\n",
    "                    \n",
    "                batch_loss = ctc_loss(batch_y_hat,batch_label,batch_y_hat_len,batch_label_len)\n",
    "                loss_list.append(batch_loss)\n",
    "                batch_loss.backward()\n",
    "                \n",
    "                #进行梯度裁剪，避免梯度爆炸\n",
    "                nn.utils.clip_grad_norm_(crnn_model.parameters(),grad_threshold)\n",
    "                                \n",
    "                #更新模型参数\n",
    "                adam_optimizer.step()\n",
    "                \n",
    "                #调整学习率\n",
    "                adam_optimizer.param_groups[0]['lr'] = self.lr*(0.8**(epoch_index//10))\n",
    "                \n",
    "                #更新tqdm进度条的描述内容\n",
    "                batch_acc = self.get_accurancy(batch_y_hat,batch_label,batch_label_len)\n",
    "                acc_list.append(batch_acc)\n",
    "                batch_lr = adam_optimizer.param_groups[0]['lr']\n",
    "                batch_loss_mean = sum(loss_list)/len(loss_list)\n",
    "                batch_acc_mean = sum(acc_list)/len(acc_list)\n",
    "                my_dataloader.set_description(f'epoch{epoch_index}| batch{batch_index}| loss: {round(batch_loss.item(),3)}| lr: {batch_lr}| loss_mean: {round(batch_loss_mean.item(),3)}| batch_acc: {round(batch_acc_mean,3)}')\n",
    "\n",
    "    \n",
    "    def fix_seed(self):\n",
    "        import random\n",
    "        import numpy as np\n",
    "        import torch\n",
    "        \n",
    "        random.seed(self.random_seed)\n",
    "        np.random.seed(self.random_seed)\n",
    "        torch.random.manual_seed(self.random_seed)\n",
    "        torch.cuda.manual_seed_all(self.random_seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        \n",
    "    def get_accurancy(self,y_hat,label,label_len):\n",
    "        #y_hat(T,N,C)\n",
    "        y_hat = y_hat.permute(1,0,2).cpu()\n",
    "#         print('y_hat:',y_hat[0])\n",
    "        label =label.cpu()\n",
    "        label_len = label_len.cpu()\n",
    "        #pred(N,T)\n",
    "        pred = torch.argmax(y_hat,dim=2)\n",
    "#         print('pred:',pred[0])\n",
    "        batch_size = pred.shape[0]\n",
    "        acc = 0\n",
    "        #decode\n",
    "        for i in range(batch_size):\n",
    "            raw_pred_list = list(pred[i].numpy())\n",
    "#             print('raw_pred_list:',raw_pred_list)\n",
    "            pred_data = []\n",
    "            for j in range(len(raw_pred_list)):\n",
    "                if j == 0 and raw_pred_list[0] != 0:\n",
    "                    pred_data.append(raw_pred_list[0]-1)\n",
    "                if j != 0 and raw_pred_list[j] != raw_pred_list[j-1] and raw_pred_list[j] != 0:\n",
    "                    pred_data.append(raw_pred_list[j]-1)\n",
    "#             print('pred_data:',pred_data)\n",
    "            \n",
    "            label_start_index = int(torch.sum(label_len[:i]).item())\n",
    "            label_end_index = int(label_len[i].item())+label_start_index\n",
    "            label_data = list(label[label_start_index:label_end_index].int().numpy())\n",
    "            label_data = [x-1 for x in label_data]\n",
    "#             print(f'pred_data:{pred_data},label_data:{label_data}\\n')\n",
    "            \n",
    "            if pred_data == label_data:\n",
    "                acc+=1\n",
    "        acc/=batch_size\n",
    "        return acc\n",
    "        \n",
    "    def model_save(self):\n",
    "        pass\n",
    "    def model_load(self):\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2031f6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch0| batch1499| loss: 1.028| lr: 0.001| loss_mean: 1.87| batch_acc: 0.258: 100%|█| 1500/1500 [03:10<00:00,  7.87it/s]\n",
      "epoch1| batch1499| loss: 0.68| lr: 0.001| loss_mean: 0.923| batch_acc: 0.591: 100%|█| 1500/1500 [03:10<00:00,  7.85it/s]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    test_train = MyTrain(max_epoch=2)\n",
    "    test_train.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2a5b4e",
   "metadata": {},
   "source": [
    "　　在使用卷积之后，ａｃｃ的结果有所提升，接下来验证一下学习率的变化是否有利于ａｃｃ的提升，涉及到较多轮的运算，将ｅｐｏｃｈ设置为１０，一种情况是学习率不变，一直为０.００１，另一种对比情况是，在５个ｅｐｏｃｈ内让学习率逐渐衰减，超过５之后学习率回到０.００１，重复这个过程，固定随机种子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "913ccff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreatTransforms:\n",
    "    def __init__(self,input_size):\n",
    "        from torchvision import transforms as transforms\n",
    "        self.img_transforms = transforms.Compose([transforms.Resize(size=input_size),\n",
    "                                                  transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,img_path,label_content,transforms=None):\n",
    "        self.img_path = img_path\n",
    "        self.label_content = label_content\n",
    "        if transforms is not None:\n",
    "            self.transforms = transforms\n",
    "        else:\n",
    "            self.transforms = None\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        img_path = self.img_path[index]\n",
    "        img_cv2 = cv2.imread(img_path)  #return （H，W，C），C list by B，G，R\n",
    "        img_cv2 = img_cv2[:,:,::-1].copy()  #return （H，W，C），C list by R，G，B\n",
    "        img_tensor = torch.from_numpy(img_cv2).permute(2,0,1).float()  #return （C，H，W），C list by R，G，B\n",
    "        if self.transforms is not None:\n",
    "            img_tensor = self.transforms(img_tensor)\n",
    "        assert img_tensor.shape[0] == 3\n",
    "        \n",
    "        img_name = os.path.split(img_path)[1]\n",
    "        img_label = self.label_content[img_name]['label']\n",
    "        #transform the label 0~9 to index 1~10，index 0 is for blank\n",
    "        img_label_index = [x+1 for x in img_label]\n",
    "        assert isinstance(img_label_index,list) == True\n",
    "        \n",
    "        return img_tensor,torch.LongTensor(img_label_index)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_path)\n",
    "\n",
    "    \n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #self.cnn: input(N,3,240,480),output(N,512,8,15)\n",
    "        trained_resnet18 = models.resnet18(weights='DEFAULT')\n",
    "        self.cnn = nn.Sequential(trained_resnet18.conv1,\n",
    "                                 trained_resnet18.bn1,\n",
    "                                 trained_resnet18.relu,\n",
    "                                 trained_resnet18.maxpool,\n",
    "                                 trained_resnet18.layer1,\n",
    "                                 trained_resnet18.layer2,\n",
    "                                 trained_resnet18.layer3,\n",
    "                                 trained_resnet18.layer4)\n",
    "        self.feat_to_seq = nn.Conv2d(7,15,kernel_size=(1,1),stride=1)\n",
    "        #self.rnn: input(15,N,512),output(15,N,64*2)\n",
    "        self.rnn = nn.GRU(input_size=512,hidden_size=64,bidirectional=True)\n",
    "        #self.linear: input(15,N,64*2),output(15,N,11)\n",
    "        self.linear = nn.Linear(128,11)\n",
    "        #self.softmax: input(15,N,11),output(15,N,11) \n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        X = self.cnn(X)\n",
    "        X = X.mean(dim=2,keepdim=True)\n",
    "        X = X.permute(0,3,1,2)\n",
    "        X = self.feat_to_seq(X)\n",
    "        X = X.squeeze(dim=3)\n",
    "        X = X.permute(1,0,2)\n",
    "        X,_ = self.rnn(X)\n",
    "        y_hat = self.linear(X)\n",
    "#         y_hat = self.softmax(X)\n",
    "        return y_hat\n",
    "    \n",
    "    \n",
    "class MyLoss():\n",
    "    def __init__(self):\n",
    "        self.ctc = nn.CTCLoss(blank=0)\n",
    "        \n",
    "\n",
    "class MyDataLoader():\n",
    "    def __init__(self,my_dataset,batch_size=20,use_my_collate=True,shuffle=False,):\n",
    "        from torch.utils.data import DataLoader\n",
    "        if use_my_collate:\n",
    "            self.data_loader = DataLoader(my_dataset,batch_size=batch_size,shuffle=shuffle,collate_fn=self.my_collate)\n",
    "        else:\n",
    "            self.data_loader = DataLoader(my_dataset,batch_size=batch_size,shuffle=shuffle)\n",
    "        \n",
    "    def my_collate(self,batch_data):\n",
    "        '''\n",
    "        batch_data shape: [(feat_tensor1,label_tensor1),(feat_tensor2,label_tensor2),……]\n",
    "        '''\n",
    "        feat_list =[]\n",
    "        label_list = []\n",
    "        label_len_list = []\n",
    "        for sample in batch_data:\n",
    "            feat_list.append(sample[0])\n",
    "            label_list.extend(list(sample[1].numpy()))\n",
    "            label_len_list.append(len(sample[1]))\n",
    "            \n",
    "        feat = torch.stack(feat_list,0)\n",
    "        label = torch.LongTensor(label_list)\n",
    "        label_len = torch.LongTensor(label_len_list)\n",
    "        \n",
    "        return feat,label,label_len\n",
    "\n",
    "\n",
    "class MyTrain():\n",
    "    def __init__(self,max_epoch=1,batch_size=20,lr=0.001,random_seed=1,grad_threshold=10):\n",
    "        self.max_epoch = max_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.random_seed = random_seed\n",
    "        self.grad_threshold = grad_threshold\n",
    "    \n",
    "    def train(self):\n",
    "        import json\n",
    "        import glob\n",
    "        \n",
    "        max_epoch,batch_size,grad_threshold = self.max_epoch,self.batch_size,self.grad_threshold\n",
    "        \n",
    "        #fix random seed\n",
    "        if self.random_seed is not None:\n",
    "            self.fix_seed()\n",
    "            \n",
    "        #creat dataset instance\n",
    "        input_size = (100,200)\n",
    "        img_path = glob.glob('../input/train/*.png')\n",
    "        label_content = json.load(open('../input/train.json'))\n",
    "        img_path.sort()\n",
    "        img_transforms = CreatTransforms(input_size).img_transforms\n",
    "        my_dataset = MyDataset(img_path,label_content,img_transforms)\n",
    "        \n",
    "        #creat model and loss instance\n",
    "        crnn_model = MyModel()\n",
    "        ctc_loss = MyLoss().ctc\n",
    "        if torch.cuda.is_available():\n",
    "            crnn_model.cuda()\n",
    "            ctc_loss.cuda()\n",
    "        \n",
    "        #creat optim instance\n",
    "        adam_optimizer = torch.optim.Adam(crnn_model.parameters(),lr=self.lr)\n",
    "        \n",
    "        batch_device = next(iter(crnn_model.parameters())).device\n",
    "        print('train device:',batch_device)\n",
    "        \n",
    "        #train each epoch\n",
    "        for epoch_index in range(max_epoch):\n",
    "            \n",
    "            loss_list=[]\n",
    "            acc_list=[]\n",
    "            \n",
    "            #creat dataloader instance\n",
    "            from tqdm import tqdm\n",
    "            my_dataloader = MyDataLoader(my_dataset,batch_size=batch_size,shuffle=True).data_loader\n",
    "            my_dataloader = tqdm(my_dataloader,ncols=120)\n",
    "            \n",
    "            #train each batch data\n",
    "            for batch_index,batch_data in enumerate(my_dataloader):\n",
    "                \n",
    "                batch_feat = batch_data[0]\n",
    "                batch_label = batch_data[1]\n",
    "                batch_label_len = batch_data[2]\n",
    "                if torch.cuda.is_available():\n",
    "                    batch_feat = batch_feat.cuda()\n",
    "                    batch_label = batch_label.cuda()\n",
    "                    batch_label_len = batch_label_len.cuda()\n",
    "                    \n",
    "                adam_optimizer.zero_grad()\n",
    "                \n",
    "                batch_y_hat = crnn_model(batch_feat)\n",
    "                batch_y_hat_len = torch.LongTensor([len(batch_y_hat)]*batch_size)\n",
    "                if torch.cuda.is_available():\n",
    "                    batch_y_hat_len = batch_y_hat_len.cuda()\n",
    "                    \n",
    "                batch_loss = ctc_loss(batch_y_hat,batch_label,batch_y_hat_len,batch_label_len)\n",
    "                loss_list.append(batch_loss)\n",
    "                batch_loss.backward()\n",
    "                \n",
    "                #进行梯度裁剪，避免梯度爆炸\n",
    "                nn.utils.clip_grad_norm_(crnn_model.parameters(),grad_threshold)\n",
    "                                \n",
    "                #更新模型参数\n",
    "                adam_optimizer.step()\n",
    "                \n",
    "                #调整学习率\n",
    "                adam_optimizer.param_groups[0]['lr'] = self.lr*(0.8**(epoch_index%5))\n",
    "                \n",
    "                #更新tqdm进度条的描述内容\n",
    "                batch_acc = self.get_accurancy(batch_y_hat,batch_label,batch_label_len)\n",
    "                acc_list.append(batch_acc)\n",
    "                batch_lr = adam_optimizer.param_groups[0]['lr']\n",
    "                batch_loss_mean = sum(loss_list)/len(loss_list)\n",
    "                batch_acc_mean = sum(acc_list)/len(acc_list)\n",
    "                my_dataloader.set_description(f'epoch{epoch_index}| batch{batch_index}| loss: {round(batch_loss.item(),3)}| lr: {batch_lr}| loss_mean: {round(batch_loss_mean.item(),3)}| batch_acc: {round(batch_acc_mean,3)}')\n",
    "\n",
    "    \n",
    "    def fix_seed(self):\n",
    "        import random\n",
    "        import numpy as np\n",
    "        import torch\n",
    "        \n",
    "        random.seed(self.random_seed)\n",
    "        np.random.seed(self.random_seed)\n",
    "        torch.random.manual_seed(self.random_seed)\n",
    "        torch.cuda.manual_seed_all(self.random_seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        \n",
    "    def get_accurancy(self,y_hat,label,label_len):\n",
    "        #y_hat(T,N,C)\n",
    "        y_hat = y_hat.permute(1,0,2).cpu()\n",
    "#         print('y_hat:',y_hat[0])\n",
    "        label =label.cpu()\n",
    "        label_len = label_len.cpu()\n",
    "        #pred(N,T)\n",
    "        pred = torch.argmax(y_hat,dim=2)\n",
    "#         print('pred:',pred[0])\n",
    "        batch_size = pred.shape[0]\n",
    "        acc = 0\n",
    "        #decode\n",
    "        for i in range(batch_size):\n",
    "            raw_pred_list = list(pred[i].numpy())\n",
    "#             print('raw_pred_list:',raw_pred_list)\n",
    "            pred_data = []\n",
    "            for j in range(len(raw_pred_list)):\n",
    "                if j == 0 and raw_pred_list[0] != 0:\n",
    "                    pred_data.append(raw_pred_list[0]-1)\n",
    "                if j != 0 and raw_pred_list[j] != raw_pred_list[j-1] and raw_pred_list[j] != 0:\n",
    "                    pred_data.append(raw_pred_list[j]-1)\n",
    "#             print('pred_data:',pred_data)\n",
    "            \n",
    "            label_start_index = int(torch.sum(label_len[:i]).item())\n",
    "            label_end_index = int(label_len[i].item())+label_start_index\n",
    "            label_data = list(label[label_start_index:label_end_index].int().numpy())\n",
    "            label_data = [x-1 for x in label_data]\n",
    "#             print(f'pred_data:{pred_data},label_data:{label_data}\\n')\n",
    "            \n",
    "            if pred_data == label_data:\n",
    "                acc+=1\n",
    "        acc/=batch_size\n",
    "        return acc\n",
    "        \n",
    "    def model_save(self):\n",
    "        pass\n",
    "    def model_load(self):\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5f5769d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch0| batch1499| loss: 1.531| lr: 0.001| loss_mean: 2.031| batch_acc: 0.175: 100%|█| 1500/1500 [07:26<00:00,  3.36it/s\n",
      "epoch1| batch1499| loss: 0.414| lr: 0.0008| loss_mean: 1.034| batch_acc: 0.551: 100%|█| 1500/1500 [03:08<00:00,  7.95it/\n",
      "epoch2| batch1499| loss: 0.388| lr: 0.0006400000000000002| loss_mean: 0.737| batch_acc: 0.662: 100%|█| 1500/1500 [03:09<\n",
      "epoch3| batch1499| loss: 0.412| lr: 0.0005120000000000001| loss_mean: 0.564| batch_acc: 0.722: 100%|█| 1500/1500 [03:09<\n",
      "epoch4| batch1499| loss: 0.51| lr: 0.0004096000000000001| loss_mean: 0.428| batch_acc: 0.772: 100%|█| 1500/1500 [03:09<0\n",
      "epoch5| batch1499| loss: 0.418| lr: 0.001| loss_mean: 0.542| batch_acc: 0.723: 100%|█| 1500/1500 [03:10<00:00,  7.89it/s\n",
      "epoch6| batch1499| loss: 0.601| lr: 0.0008| loss_mean: 0.4| batch_acc: 0.783: 100%|█| 1500/1500 [03:07<00:00,  7.98it/s]\n",
      "epoch7| batch1499| loss: 0.522| lr: 0.0006400000000000002| loss_mean: 0.277| batch_acc: 0.84: 100%|█| 1500/1500 [03:03<0\n",
      "epoch8| batch1499| loss: 0.32| lr: 0.0005120000000000001| loss_mean: 0.185| batch_acc: 0.887: 100%|█| 1500/1500 [03:03<0\n",
      "epoch9| batch1499| loss: 0.058| lr: 0.0004096000000000001| loss_mean: 0.122| batch_acc: 0.924: 100%|█| 1500/1500 [03:04<\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    test_train = MyTrain(max_epoch=10,random_seed=6)\n",
    "    test_train.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aabf0928",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreatTransforms:\n",
    "    def __init__(self,input_size):\n",
    "        from torchvision import transforms as transforms\n",
    "        self.img_transforms = transforms.Compose([transforms.Resize(size=input_size),\n",
    "                                                  transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,img_path,label_content,transforms=None):\n",
    "        self.img_path = img_path\n",
    "        self.label_content = label_content\n",
    "        if transforms is not None:\n",
    "            self.transforms = transforms\n",
    "        else:\n",
    "            self.transforms = None\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        img_path = self.img_path[index]\n",
    "        img_cv2 = cv2.imread(img_path)  #return （H，W，C），C list by B，G，R\n",
    "        img_cv2 = img_cv2[:,:,::-1].copy()  #return （H，W，C），C list by R，G，B\n",
    "        img_tensor = torch.from_numpy(img_cv2).permute(2,0,1).float()  #return （C，H，W），C list by R，G，B\n",
    "        if self.transforms is not None:\n",
    "            img_tensor = self.transforms(img_tensor)\n",
    "        assert img_tensor.shape[0] == 3\n",
    "        \n",
    "        img_name = os.path.split(img_path)[1]\n",
    "        img_label = self.label_content[img_name]['label']\n",
    "        #transform the label 0~9 to index 1~10，index 0 is for blank\n",
    "        img_label_index = [x+1 for x in img_label]\n",
    "        assert isinstance(img_label_index,list) == True\n",
    "        \n",
    "        return img_tensor,torch.LongTensor(img_label_index)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_path)\n",
    "\n",
    "    \n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #self.cnn: input(N,3,240,480),output(N,512,8,15)\n",
    "        trained_resnet18 = models.resnet18(weights='DEFAULT')\n",
    "        self.cnn = nn.Sequential(trained_resnet18.conv1,\n",
    "                                 trained_resnet18.bn1,\n",
    "                                 trained_resnet18.relu,\n",
    "                                 trained_resnet18.maxpool,\n",
    "                                 trained_resnet18.layer1,\n",
    "                                 trained_resnet18.layer2,\n",
    "                                 trained_resnet18.layer3,\n",
    "                                 trained_resnet18.layer4)\n",
    "        self.feat_to_seq = nn.Conv2d(7,15,kernel_size=(1,1),stride=1)\n",
    "        #self.rnn: input(15,N,512),output(15,N,64*2)\n",
    "        self.rnn = nn.GRU(input_size=512,hidden_size=64,bidirectional=True)\n",
    "        #self.linear: input(15,N,64*2),output(15,N,11)\n",
    "        self.linear = nn.Linear(128,11)\n",
    "        #self.softmax: input(15,N,11),output(15,N,11) \n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        X = self.cnn(X)\n",
    "        X = X.mean(dim=2,keepdim=True)\n",
    "        X = X.permute(0,3,1,2)\n",
    "        X = self.feat_to_seq(X)\n",
    "        X = X.squeeze(dim=3)\n",
    "        X = X.permute(1,0,2)\n",
    "        X,_ = self.rnn(X)\n",
    "        y_hat = self.linear(X)\n",
    "#         y_hat = self.softmax(X)\n",
    "        return y_hat\n",
    "    \n",
    "    \n",
    "class MyLoss():\n",
    "    def __init__(self):\n",
    "        self.ctc = nn.CTCLoss(blank=0)\n",
    "        \n",
    "\n",
    "class MyDataLoader():\n",
    "    def __init__(self,my_dataset,batch_size=20,use_my_collate=True,shuffle=False,):\n",
    "        from torch.utils.data import DataLoader\n",
    "        if use_my_collate:\n",
    "            self.data_loader = DataLoader(my_dataset,batch_size=batch_size,shuffle=shuffle,collate_fn=self.my_collate)\n",
    "        else:\n",
    "            self.data_loader = DataLoader(my_dataset,batch_size=batch_size,shuffle=shuffle)\n",
    "        \n",
    "    def my_collate(self,batch_data):\n",
    "        '''\n",
    "        batch_data shape: [(feat_tensor1,label_tensor1),(feat_tensor2,label_tensor2),……]\n",
    "        '''\n",
    "        feat_list =[]\n",
    "        label_list = []\n",
    "        label_len_list = []\n",
    "        for sample in batch_data:\n",
    "            feat_list.append(sample[0])\n",
    "            label_list.extend(list(sample[1].numpy()))\n",
    "            label_len_list.append(len(sample[1]))\n",
    "            \n",
    "        feat = torch.stack(feat_list,0)\n",
    "        label = torch.LongTensor(label_list)\n",
    "        label_len = torch.LongTensor(label_len_list)\n",
    "        \n",
    "        return feat,label,label_len\n",
    "\n",
    "\n",
    "class MyTrain():\n",
    "    def __init__(self,max_epoch=1,batch_size=20,lr=0.001,random_seed=1,grad_threshold=10):\n",
    "        self.max_epoch = max_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.random_seed = random_seed\n",
    "        self.grad_threshold = grad_threshold\n",
    "    \n",
    "    def train(self):\n",
    "        import json\n",
    "        import glob\n",
    "        \n",
    "        max_epoch,batch_size,grad_threshold = self.max_epoch,self.batch_size,self.grad_threshold\n",
    "        \n",
    "        #fix random seed\n",
    "        if self.random_seed is not None:\n",
    "            self.fix_seed()\n",
    "            \n",
    "        #creat dataset instance\n",
    "        input_size = (100,200)\n",
    "        img_path = glob.glob('../input/train/*.png')\n",
    "        label_content = json.load(open('../input/train.json'))\n",
    "        img_path.sort()\n",
    "        img_transforms = CreatTransforms(input_size).img_transforms\n",
    "        my_dataset = MyDataset(img_path,label_content,img_transforms)\n",
    "        \n",
    "        #creat model and loss instance\n",
    "        crnn_model = MyModel()\n",
    "        ctc_loss = MyLoss().ctc\n",
    "        if torch.cuda.is_available():\n",
    "            crnn_model.cuda()\n",
    "            ctc_loss.cuda()\n",
    "        \n",
    "        #creat optim instance\n",
    "        adam_optimizer = torch.optim.Adam(crnn_model.parameters(),lr=self.lr)\n",
    "        \n",
    "        batch_device = next(iter(crnn_model.parameters())).device\n",
    "        print('train device:',batch_device)\n",
    "        \n",
    "        #train each epoch\n",
    "        for epoch_index in range(max_epoch):\n",
    "            \n",
    "            loss_list=[]\n",
    "            acc_list=[]\n",
    "            \n",
    "            #creat dataloader instance\n",
    "            from tqdm import tqdm\n",
    "            my_dataloader = MyDataLoader(my_dataset,batch_size=batch_size,shuffle=True).data_loader\n",
    "            my_dataloader = tqdm(my_dataloader,ncols=120)\n",
    "            \n",
    "            #train each batch data\n",
    "            for batch_index,batch_data in enumerate(my_dataloader):\n",
    "                \n",
    "                batch_feat = batch_data[0]\n",
    "                batch_label = batch_data[1]\n",
    "                batch_label_len = batch_data[2]\n",
    "                if torch.cuda.is_available():\n",
    "                    batch_feat = batch_feat.cuda()\n",
    "                    batch_label = batch_label.cuda()\n",
    "                    batch_label_len = batch_label_len.cuda()\n",
    "                    \n",
    "                adam_optimizer.zero_grad()\n",
    "                \n",
    "                batch_y_hat = crnn_model(batch_feat)\n",
    "                batch_y_hat_len = torch.LongTensor([len(batch_y_hat)]*batch_size)\n",
    "                if torch.cuda.is_available():\n",
    "                    batch_y_hat_len = batch_y_hat_len.cuda()\n",
    "                    \n",
    "                batch_loss = ctc_loss(batch_y_hat,batch_label,batch_y_hat_len,batch_label_len)\n",
    "                loss_list.append(batch_loss)\n",
    "                batch_loss.backward()\n",
    "                \n",
    "                #进行梯度裁剪，避免梯度爆炸\n",
    "                nn.utils.clip_grad_norm_(crnn_model.parameters(),grad_threshold)\n",
    "                                \n",
    "                #更新模型参数\n",
    "                adam_optimizer.step()\n",
    "                \n",
    "                #调整学习率\n",
    "#                 adam_optimizer.param_groups[0]['lr'] = self.lr*(0.8**(epoch_index%5))\n",
    "                \n",
    "                #更新tqdm进度条的描述内容\n",
    "                batch_acc = self.get_accurancy(batch_y_hat,batch_label,batch_label_len)\n",
    "                acc_list.append(batch_acc)\n",
    "                batch_lr = adam_optimizer.param_groups[0]['lr']\n",
    "                batch_loss_mean = sum(loss_list)/len(loss_list)\n",
    "                batch_acc_mean = sum(acc_list)/len(acc_list)\n",
    "                my_dataloader.set_description(f'epoch{epoch_index}| batch{batch_index}| loss: {round(batch_loss.item(),3)}| lr: {batch_lr}| loss_mean: {round(batch_loss_mean.item(),3)}| batch_acc: {round(batch_acc_mean,3)}')\n",
    "\n",
    "    \n",
    "    def fix_seed(self):\n",
    "        import random\n",
    "        import numpy as np\n",
    "        import torch\n",
    "        \n",
    "        random.seed(self.random_seed)\n",
    "        np.random.seed(self.random_seed)\n",
    "        torch.random.manual_seed(self.random_seed)\n",
    "        torch.cuda.manual_seed_all(self.random_seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        \n",
    "    def get_accurancy(self,y_hat,label,label_len):\n",
    "        #y_hat(T,N,C)\n",
    "        y_hat = y_hat.permute(1,0,2).cpu()\n",
    "#         print('y_hat:',y_hat[0])\n",
    "        label =label.cpu()\n",
    "        label_len = label_len.cpu()\n",
    "        #pred(N,T)\n",
    "        pred = torch.argmax(y_hat,dim=2)\n",
    "#         print('pred:',pred[0])\n",
    "        batch_size = pred.shape[0]\n",
    "        acc = 0\n",
    "        #decode\n",
    "        for i in range(batch_size):\n",
    "            raw_pred_list = list(pred[i].numpy())\n",
    "#             print('raw_pred_list:',raw_pred_list)\n",
    "            pred_data = []\n",
    "            for j in range(len(raw_pred_list)):\n",
    "                if j == 0 and raw_pred_list[0] != 0:\n",
    "                    pred_data.append(raw_pred_list[0]-1)\n",
    "                if j != 0 and raw_pred_list[j] != raw_pred_list[j-1] and raw_pred_list[j] != 0:\n",
    "                    pred_data.append(raw_pred_list[j]-1)\n",
    "#             print('pred_data:',pred_data)\n",
    "            \n",
    "            label_start_index = int(torch.sum(label_len[:i]).item())\n",
    "            label_end_index = int(label_len[i].item())+label_start_index\n",
    "            label_data = list(label[label_start_index:label_end_index].int().numpy())\n",
    "            label_data = [x-1 for x in label_data]\n",
    "#             print(f'pred_data:{pred_data},label_data:{label_data}\\n')\n",
    "            \n",
    "            if pred_data == label_data:\n",
    "                acc+=1\n",
    "        acc/=batch_size\n",
    "        return acc\n",
    "        \n",
    "    def model_save(self):\n",
    "        pass\n",
    "    def model_load(self):\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70a7de72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch0| batch1499| loss: 1.531| lr: 0.001| loss_mean: 2.031| batch_acc: 0.175: 100%|█| 1500/1500 [06:50<00:00,  3.66it/s\n",
      "epoch1| batch1499| loss: 0.464| lr: 0.001| loss_mean: 1.061| batch_acc: 0.543: 100%|█| 1500/1500 [03:05<00:00,  8.08it/s\n",
      "epoch2| batch1499| loss: 0.552| lr: 0.001| loss_mean: 0.789| batch_acc: 0.646: 100%|█| 1500/1500 [03:09<00:00,  7.91it/s\n",
      "epoch3| batch1499| loss: 0.608| lr: 0.001| loss_mean: 0.645| batch_acc: 0.695: 100%|█| 1500/1500 [03:30<00:00,  7.14it/s\n",
      "epoch4| batch1499| loss: 0.783| lr: 0.001| loss_mean: 0.534| batch_acc: 0.738: 100%|█| 1500/1500 [06:11<00:00,  4.03it/s\n",
      "epoch5| batch1499| loss: 0.524| lr: 0.001| loss_mean: 0.445| batch_acc: 0.778: 100%|█| 1500/1500 [03:30<00:00,  7.12it/s\n",
      "epoch6| batch1499| loss: 0.633| lr: 0.001| loss_mean: 0.357| batch_acc: 0.819: 100%|█| 1500/1500 [06:05<00:00,  4.10it/s\n",
      "epoch7| batch1499| loss: 0.32| lr: 0.001| loss_mean: 0.294| batch_acc: 0.844: 100%|█| 1500/1500 [03:40<00:00,  6.80it/s]\n",
      "epoch8| batch1499| loss: 0.4| lr: 0.001| loss_mean: 0.246| batch_acc: 0.867: 100%|██| 1500/1500 [03:08<00:00,  7.96it/s]\n",
      "epoch9| batch1499| loss: 0.134| lr: 0.001| loss_mean: 0.197| batch_acc: 0.89: 100%|█| 1500/1500 [03:08<00:00,  7.97it/s]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    test_train = MyTrain(max_epoch=10,random_seed=6)\n",
    "    test_train.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e8a4eb",
   "metadata": {},
   "source": [
    "　　对比上面两次实验的结果，可以看出来，在训练过程中学习率循环衰减的一组，相比学习率保持一定的一组，在ｅｐｏｃｈ９的ａｃｃ提升效率仍然较高。在多轮训练中控制学习率进行循环可以有效的提升学习效果。  \n",
    "　　\n",
    "　　前面的训练都在训练前对图片进行了Ｎｏｒｍ处理，下面看一下如果在训练前不对图片进行Ｎｏｒｍ处理训练效果会不会有区别，仍然使用学习率循环衰减，计算１０个ｅｐｏｃｈ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bbbb1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreatTransforms:\n",
    "    def __init__(self,input_size):\n",
    "        from torchvision import transforms as transforms\n",
    "        self.img_transforms = transforms.Compose([transforms.Resize(size=input_size)])\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,img_path,label_content,transforms=None):\n",
    "        self.img_path = img_path\n",
    "        self.label_content = label_content\n",
    "        if transforms is not None:\n",
    "            self.transforms = transforms\n",
    "        else:\n",
    "            self.transforms = None\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        img_path = self.img_path[index]\n",
    "        img_cv2 = cv2.imread(img_path)  #return （H，W，C），C list by B，G，R\n",
    "        img_cv2 = img_cv2[:,:,::-1].copy()  #return （H，W，C），C list by R，G，B\n",
    "        img_tensor = torch.from_numpy(img_cv2).permute(2,0,1).float()  #return （C，H，W），C list by R，G，B\n",
    "        if self.transforms is not None:\n",
    "            img_tensor = self.transforms(img_tensor)\n",
    "        assert img_tensor.shape[0] == 3\n",
    "        \n",
    "        img_name = os.path.split(img_path)[1]\n",
    "        img_label = self.label_content[img_name]['label']\n",
    "        #transform the label 0~9 to index 1~10，index 0 is for blank\n",
    "        img_label_index = [x+1 for x in img_label]\n",
    "        assert isinstance(img_label_index,list) == True\n",
    "        \n",
    "        return img_tensor,torch.LongTensor(img_label_index)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_path)\n",
    "\n",
    "    \n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #self.cnn: input(N,3,240,480),output(N,512,8,15)\n",
    "        trained_resnet18 = models.resnet18(weights='DEFAULT')\n",
    "        self.cnn = nn.Sequential(trained_resnet18.conv1,\n",
    "                                 trained_resnet18.bn1,\n",
    "                                 trained_resnet18.relu,\n",
    "                                 trained_resnet18.maxpool,\n",
    "                                 trained_resnet18.layer1,\n",
    "                                 trained_resnet18.layer2,\n",
    "                                 trained_resnet18.layer3,\n",
    "                                 trained_resnet18.layer4)\n",
    "        self.feat_to_seq = nn.Conv2d(7,15,kernel_size=(1,1),stride=1)\n",
    "        #self.rnn: input(15,N,512),output(15,N,64*2)\n",
    "        self.rnn = nn.GRU(input_size=512,hidden_size=64,bidirectional=True)\n",
    "        #self.linear: input(15,N,64*2),output(15,N,11)\n",
    "        self.linear = nn.Linear(128,11)\n",
    "        #self.softmax: input(15,N,11),output(15,N,11) \n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        X = self.cnn(X)\n",
    "        X = X.mean(dim=2,keepdim=True)\n",
    "        X = X.permute(0,3,1,2)\n",
    "        X = self.feat_to_seq(X)\n",
    "        X = X.squeeze(dim=3)\n",
    "        X = X.permute(1,0,2)\n",
    "        X,_ = self.rnn(X)\n",
    "        y_hat = self.linear(X)\n",
    "#         y_hat = self.softmax(X)\n",
    "        return y_hat\n",
    "    \n",
    "    \n",
    "class MyLoss():\n",
    "    def __init__(self):\n",
    "        self.ctc = nn.CTCLoss(blank=0)\n",
    "        \n",
    "\n",
    "class MyDataLoader():\n",
    "    def __init__(self,my_dataset,batch_size=20,use_my_collate=True,shuffle=False,):\n",
    "        from torch.utils.data import DataLoader\n",
    "        if use_my_collate:\n",
    "            self.data_loader = DataLoader(my_dataset,batch_size=batch_size,shuffle=shuffle,collate_fn=self.my_collate)\n",
    "        else:\n",
    "            self.data_loader = DataLoader(my_dataset,batch_size=batch_size,shuffle=shuffle)\n",
    "        \n",
    "    def my_collate(self,batch_data):\n",
    "        '''\n",
    "        batch_data shape: [(feat_tensor1,label_tensor1),(feat_tensor2,label_tensor2),……]\n",
    "        '''\n",
    "        feat_list =[]\n",
    "        label_list = []\n",
    "        label_len_list = []\n",
    "        for sample in batch_data:\n",
    "            feat_list.append(sample[0])\n",
    "            label_list.extend(list(sample[1].numpy()))\n",
    "            label_len_list.append(len(sample[1]))\n",
    "            \n",
    "        feat = torch.stack(feat_list,0)\n",
    "        label = torch.LongTensor(label_list)\n",
    "        label_len = torch.LongTensor(label_len_list)\n",
    "        \n",
    "        return feat,label,label_len\n",
    "\n",
    "\n",
    "class MyTrain():\n",
    "    def __init__(self,max_epoch=1,batch_size=20,lr=0.001,random_seed=1,grad_threshold=10):\n",
    "        self.max_epoch = max_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.random_seed = random_seed\n",
    "        self.grad_threshold = grad_threshold\n",
    "    \n",
    "    def train(self):\n",
    "        import json\n",
    "        import glob\n",
    "        \n",
    "        max_epoch,batch_size,grad_threshold = self.max_epoch,self.batch_size,self.grad_threshold\n",
    "        \n",
    "        #fix random seed\n",
    "        if self.random_seed is not None:\n",
    "            self.fix_seed()\n",
    "            \n",
    "        #creat dataset instance\n",
    "        input_size = (100,200)\n",
    "        img_path = glob.glob('../input/train/*.png')\n",
    "        label_content = json.load(open('../input/train.json'))\n",
    "        img_path.sort()\n",
    "        img_transforms = CreatTransforms(input_size).img_transforms\n",
    "        my_dataset = MyDataset(img_path,label_content,img_transforms)\n",
    "        \n",
    "        #creat model and loss instance\n",
    "        crnn_model = MyModel()\n",
    "        ctc_loss = MyLoss().ctc\n",
    "        if torch.cuda.is_available():\n",
    "            crnn_model.cuda()\n",
    "            ctc_loss.cuda()\n",
    "        \n",
    "        #creat optim instance\n",
    "        adam_optimizer = torch.optim.Adam(crnn_model.parameters(),lr=self.lr)\n",
    "        \n",
    "        batch_device = next(iter(crnn_model.parameters())).device\n",
    "        print('train device:',batch_device)\n",
    "        \n",
    "        #train each epoch\n",
    "        for epoch_index in range(max_epoch):\n",
    "            \n",
    "            loss_list=[]\n",
    "            acc_list=[]\n",
    "            \n",
    "            #creat dataloader instance\n",
    "            from tqdm import tqdm\n",
    "            my_dataloader = MyDataLoader(my_dataset,batch_size=batch_size,shuffle=True).data_loader\n",
    "            my_dataloader = tqdm(my_dataloader,ncols=120)\n",
    "            \n",
    "            #train each batch data\n",
    "            for batch_index,batch_data in enumerate(my_dataloader):\n",
    "                \n",
    "                batch_feat = batch_data[0]\n",
    "                batch_label = batch_data[1]\n",
    "                batch_label_len = batch_data[2]\n",
    "                if torch.cuda.is_available():\n",
    "                    batch_feat = batch_feat.cuda()\n",
    "                    batch_label = batch_label.cuda()\n",
    "                    batch_label_len = batch_label_len.cuda()\n",
    "                    \n",
    "                adam_optimizer.zero_grad()\n",
    "                \n",
    "                batch_y_hat = crnn_model(batch_feat)\n",
    "                batch_y_hat_len = torch.LongTensor([len(batch_y_hat)]*batch_size)\n",
    "                if torch.cuda.is_available():\n",
    "                    batch_y_hat_len = batch_y_hat_len.cuda()\n",
    "                    \n",
    "                batch_loss = ctc_loss(batch_y_hat,batch_label,batch_y_hat_len,batch_label_len)\n",
    "                loss_list.append(batch_loss)\n",
    "                batch_loss.backward()\n",
    "                \n",
    "                #进行梯度裁剪，避免梯度爆炸\n",
    "                nn.utils.clip_grad_norm_(crnn_model.parameters(),grad_threshold)\n",
    "                                \n",
    "                #更新模型参数\n",
    "                adam_optimizer.step()\n",
    "                \n",
    "                #调整学习率\n",
    "#                 adam_optimizer.param_groups[0]['lr'] = self.lr*(0.8**(epoch_index%5))\n",
    "                \n",
    "                #更新tqdm进度条的描述内容\n",
    "                batch_acc = self.get_accurancy(batch_y_hat,batch_label,batch_label_len)\n",
    "                acc_list.append(batch_acc)\n",
    "                batch_lr = adam_optimizer.param_groups[0]['lr']\n",
    "                batch_loss_mean = sum(loss_list)/len(loss_list)\n",
    "                batch_acc_mean = sum(acc_list)/len(acc_list)\n",
    "                my_dataloader.set_description(f'epoch{epoch_index}| batch{batch_index}| loss: {round(batch_loss.item(),3)}| lr: {batch_lr}| loss_mean: {round(batch_loss_mean.item(),3)}| batch_acc: {round(batch_acc_mean,3)}')\n",
    "\n",
    "    \n",
    "    def fix_seed(self):\n",
    "        import random\n",
    "        import numpy as np\n",
    "        import torch\n",
    "        \n",
    "        random.seed(self.random_seed)\n",
    "        np.random.seed(self.random_seed)\n",
    "        torch.random.manual_seed(self.random_seed)\n",
    "        torch.cuda.manual_seed_all(self.random_seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        \n",
    "    def get_accurancy(self,y_hat,label,label_len):\n",
    "        #y_hat(T,N,C)\n",
    "        y_hat = y_hat.permute(1,0,2).cpu()\n",
    "#         print('y_hat:',y_hat[0])\n",
    "        label =label.cpu()\n",
    "        label_len = label_len.cpu()\n",
    "        #pred(N,T)\n",
    "        pred = torch.argmax(y_hat,dim=2)\n",
    "#         print('pred:',pred[0])\n",
    "        batch_size = pred.shape[0]\n",
    "        acc = 0\n",
    "        #decode\n",
    "        for i in range(batch_size):\n",
    "            raw_pred_list = list(pred[i].numpy())\n",
    "#             print('raw_pred_list:',raw_pred_list)\n",
    "            pred_data = []\n",
    "            for j in range(len(raw_pred_list)):\n",
    "                if j == 0 and raw_pred_list[0] != 0:\n",
    "                    pred_data.append(raw_pred_list[0]-1)\n",
    "                if j != 0 and raw_pred_list[j] != raw_pred_list[j-1] and raw_pred_list[j] != 0:\n",
    "                    pred_data.append(raw_pred_list[j]-1)\n",
    "#             print('pred_data:',pred_data)\n",
    "            \n",
    "            label_start_index = int(torch.sum(label_len[:i]).item())\n",
    "            label_end_index = int(label_len[i].item())+label_start_index\n",
    "            label_data = list(label[label_start_index:label_end_index].int().numpy())\n",
    "            label_data = [x-1 for x in label_data]\n",
    "#             print(f'pred_data:{pred_data},label_data:{label_data}\\n')\n",
    "            \n",
    "            if pred_data == label_data:\n",
    "                acc+=1\n",
    "        acc/=batch_size\n",
    "        return acc\n",
    "        \n",
    "    def model_save(self):\n",
    "        pass\n",
    "    def model_load(self):\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd4f06b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch0| batch1499| loss: 1.67| lr: 0.001| loss_mean: 2.005| batch_acc: 0.192: 100%|█| 1500/1500 [03:06<00:00,  8.02it/s]\n",
      "epoch1| batch1499| loss: 0.436| lr: 0.001| loss_mean: 1.029| batch_acc: 0.559: 100%|█| 1500/1500 [03:07<00:00,  7.99it/s\n",
      "epoch2| batch1499| loss: 0.466| lr: 0.001| loss_mean: 0.777| batch_acc: 0.651: 100%|█| 1500/1500 [03:06<00:00,  8.04it/s\n",
      "epoch3| batch1499| loss: 0.463| lr: 0.001| loss_mean: 0.636| batch_acc: 0.702: 100%|█| 1500/1500 [03:03<00:00,  8.15it/s\n",
      "epoch4| batch1499| loss: 0.591| lr: 0.001| loss_mean: 0.525| batch_acc: 0.746: 100%|█| 1500/1500 [03:03<00:00,  8.18it/s\n",
      "epoch5| batch1499| loss: 0.609| lr: 0.001| loss_mean: 0.437| batch_acc: 0.781: 100%|█| 1500/1500 [03:03<00:00,  8.18it/s\n",
      "epoch6| batch1499| loss: 0.55| lr: 0.001| loss_mean: 0.36| batch_acc: 0.814: 100%|██| 1500/1500 [03:02<00:00,  8.20it/s]\n",
      "epoch7| batch1499| loss: 0.42| lr: 0.001| loss_mean: 0.298| batch_acc: 0.842: 100%|█| 1500/1500 [03:03<00:00,  8.19it/s]\n",
      "epoch8| batch1499| loss: 0.118| lr: 0.001| loss_mean: 0.236| batch_acc: 0.871: 100%|█| 1500/1500 [04:55<00:00,  5.08it/s\n",
      "epoch9| batch1499| loss: 0.073| lr: 0.001| loss_mean: 0.2| batch_acc: 0.888: 100%|██| 1500/1500 [04:25<00:00,  5.65it/s]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    test_train = MyTrain(max_epoch=10,random_seed=6)\n",
    "    test_train.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35643fba",
   "metadata": {},
   "source": [
    "　　从上面把Ｎｏｒｍ移除之后的效果来看，在这个项目中，加不加Ｎｏｒｍ对训练结果基本没有什么影响，但是可以看到，没有使用Ｎｏｒｍ时，第一轮所用的时间变少了大概３分钟，这应该是将所有图片进行Ｎｏｒｍ需要花费的时间"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18cce89",
   "metadata": {},
   "source": [
    "## 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bc9eed",
   "metadata": {},
   "source": [
    "　　前期模型训练不动，主要是因为参数过大，或者模型过于复杂，通过精简参数或者取消某些复杂的结构，让模型先跑起来是最重要的事情。  \n",
    "　　\n",
    "　　模型跑起来之后，发现ｃｎｎ到ｓｅｑ转换的过程中，使用卷积相比直接使用ｍｅａｎ效果更好一些，另外，使用卷积可以解决图片最小输入尺寸受到Ｔ限制的问题，而Ｔ的确定可以实际的对图片进行裁切后判断，但是需要满足Ｔ＞＝２ｌｅｎ（ｌａｂｅｌ）＋１。  \n",
    "　　\n",
    "　　Ｎｏｒｍ对模型训练的结果没有什么影响，但是可能对模型的启动和正常训练有些影响，需要后面进一步验证。  \n",
    "　　\n",
    "　　学习率衰减有利于提升长时间训练的精度，这里的衰减并不是一直减小学习率，而是设置一个学习率的循环区间，让学习率在训练过程中按照从大到小，再回到最大值，然后继续从大到小的循环。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d658cf13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<button onclick=\"$('.input, .prompt, .output_stderr, .output_error').toggle();\">Toggle Code</button>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display_html\n",
    "display_html(\"\"\"<button onclick=\"$('.input, .prompt, .output_stderr, .output_error').toggle();\">Toggle Code</button>\"\"\", raw=True)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "原始单元格格式",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "199px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
