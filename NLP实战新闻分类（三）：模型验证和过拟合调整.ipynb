{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beae2c1e",
   "metadata": {},
   "source": [
    "# 模型验证"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86df8dc6",
   "metadata": {},
   "source": [
    "## 划分训练集和验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7bf82541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def split_train_valid(csv_path,train_prec=0.75):\n",
    "    csv_data = pd.read_csv(csv_path,sep='\\t') \n",
    "    text_data = csv_data.text\n",
    "    label_data = csv_data.label\n",
    "    \n",
    "    mask = np.random.rand(len(text_data))<train_prec\n",
    "    train_text_data = text_data[mask].reset_index(drop=True)\n",
    "    valid_text_data = text_data[~mask].reset_index(drop=True)\n",
    "    train_label_data = label_data[mask].reset_index(drop=True)\n",
    "    valid_label_data = label_data[~mask].reset_index(drop=True)\n",
    "    \n",
    "    return train_text_data,valid_text_data,train_label_data,valid_label_data\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self,text_data,label_data):\n",
    "        self.text_data = text_data\n",
    "        self.label_data = label_data\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        #所有text内的token索引增加1，0空出来代表空格，将每个text控制在1000长\n",
    "        text_str = self.text_data[index]\n",
    "        text_list = [int(x)+1 for x in text_str.split()]\n",
    "        if len(text_list)>=1000:\n",
    "            text_list = text_list[:1000]\n",
    "        else:\n",
    "            text_list.extend([0]*(1000-len(text_list)))\n",
    "        text_array = np.array(text_list)\n",
    "        label_array = np.array(self.label_data[index])\n",
    "        return text_array,label_array\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text_data)\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(7551,100)\n",
    "        self.rnn = nn.GRU(100,50,batch_first=True)\n",
    "        self.fc = nn.Linear(50,14)\n",
    "        \n",
    "    def forward(self,X):\n",
    "#         print('X:',X.shape)\n",
    "        X = self.embedding(X)\n",
    "#         print('embedding:',X.shape)\n",
    "        _,X = self.rnn(X)\n",
    "#         print('rnn:',X.shape)\n",
    "        X = X.squeeze(dim=0)\n",
    "#         print('squeeze:',X.shape)\n",
    "        y_hat = self.fc(X)\n",
    "#         print('y_hat:',y_hat.shape)\n",
    "        return y_hat\n",
    "\n",
    "    \n",
    "class MyTrain():\n",
    "    def __init__(self,max_epoch=1,random_seed=1,lr=0.001,out_dir='./'):\n",
    "        self.max_epoch = max_epoch\n",
    "        self.random_seed = random_seed\n",
    "        self.lr = lr\n",
    "        self.out_dir = out_dir\n",
    "        self.iter = 0\n",
    "        \n",
    "    def fix_random(self):\n",
    "        import random\n",
    "        import numpy as np\n",
    "        import torch\n",
    "        random.seed(self.random_seed)\n",
    "        np.random.seed(self.random_seed)\n",
    "        torch.random.manual_seed(self.random_seed)\n",
    "        torch.cuda.random.manual_seed_all(self.random_seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        print(f'random seed:{self.random_seed}')\n",
    "        \n",
    "    def my_train(self):\n",
    "        \n",
    "        max_epoch,lr = self.max_epoch,self.lr\n",
    "        \n",
    "        if self.random_seed is not None:\n",
    "            self.fix_random()\n",
    "        \n",
    "        train_text_data,valid_text_data,train_label_data,valid_label_data = split_train_valid('./train_set.csv',train_prec=0.75)\n",
    "        train_dataset = MyDataset(train_text_data,train_label_data)\n",
    "        valid_dataset = MyDataset(valid_text_data,valid_label_data)\n",
    "        \n",
    "        my_model = MyModel()\n",
    "        my_optim = torch.optim.Adam(my_model.parameters(),lr=lr)\n",
    "        my_loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            my_model.cuda()\n",
    "            my_loss.cuda()\n",
    "        print(f'train device:{next(iter(my_model.parameters())).device}')  #显示训练设备\n",
    "        \n",
    "        best_f1_score = 0\n",
    "        epoch_index = 0\n",
    "\n",
    "        for epoch_index in range(max_epoch):\n",
    "            \n",
    "            loss_list = []\n",
    "            train_f1_score_list = []\n",
    "            valid_f1_score_list = []\n",
    "            train_dataloader = DataLoader(train_dataset,batch_size=120,shuffle=True)\n",
    "            valid_dataloader = DataLoader(valid_dataset,batch_size=40,shuffle=True)\n",
    "            my_dataloader = tqdm(train_dataloader)\n",
    "            \n",
    "            batch_index = 0\n",
    "                        \n",
    "            for train_data,valid_data in zip(my_dataloader,valid_dataloader):\n",
    "                                \n",
    "                my_model.train()  #将模型设置为训练模式\n",
    "                train_text,train_label = train_data\n",
    "                valid_text,valid_label = valid_data\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    train_text = train_text.cuda()\n",
    "                    train_label = train_label.cuda()\n",
    "                    valid_text = valid_text.cuda()\n",
    "                    valid_label = valid_label.cuda()\n",
    "                \n",
    "                train_y_hat = my_model(train_text)\n",
    "                batch_train_loss = my_loss(train_y_hat,train_label)\n",
    "                \n",
    "                my_optim.zero_grad()\n",
    "                batch_train_loss.backward()\n",
    "                my_optim.step()\n",
    "                my_optim.param_groups[0]['lr'] = lr*(0.8**(epoch_index%10))\n",
    "        \n",
    "                my_model.eval()  #将模型设置为验证模式\n",
    "                with torch.no_grad():\n",
    "                    valid_y_hat = my_model(valid_text)\n",
    "                    batch_valid_f1_score = self.f1_score(valid_y_hat.data,valid_label.data)\n",
    "                    valid_f1_score_list.append(batch_valid_f1_score)\n",
    "                    mean_valid_f1 = round(sum(valid_f1_score_list)/len(valid_f1_score_list),3)\n",
    "\n",
    "                    #显示batch结果\n",
    "                    batch_lr = round(my_optim.param_groups[0]['lr'],5)\n",
    "                    batch_loss = round(batch_train_loss.item(),4)\n",
    "                    loss_list.append(batch_loss)\n",
    "                    mean_loss = round((sum(loss_list)/len(loss_list)),3)\n",
    "\n",
    "                    batch_train_f1_score = self.f1_score(train_y_hat.data,train_label.data)\n",
    "                    train_f1_score_list.append(batch_train_f1_score)\n",
    "                    mean_train_f1 = round(sum(train_f1_score_list)/len(train_f1_score_list),3)\n",
    "\n",
    "                    my_dataloader.set_description(f'epoch:{epoch_index},batch:{batch_index},lr:{batch_lr},loss:{batch_loss},mean_loss:{mean_loss},train_f1:{mean_train_f1},valid_f1:{mean_valid_f1}')\n",
    "\n",
    "                    #存储模型\n",
    "                    if batch_train_f1_score>best_f1_score:\n",
    "                        torch.save(my_model.state_dict(),os.path.join(self.out_dir,'embedding_gru_best'))\n",
    "                        best_f1_score = batch_train_f1_score\n",
    "\n",
    "                    batch_index+=1\n",
    "            \n",
    "                \n",
    "    def f1_score(self,y_hat,label,eps=1e-8):\n",
    "        #y_hat(N,C),label(1)\n",
    "        y_hat = y_hat.cpu()\n",
    "        label = label.cpu()\n",
    "        preds_list = list(torch.argmax(y_hat,dim=1).numpy())\n",
    "        label_list = list(label.numpy())\n",
    "#         print(f'preds:{preds_list},label:{label_list}')\n",
    "        class_index_list = []\n",
    "        for class_index in label_list:\n",
    "            if class_index not in class_index_list:\n",
    "                class_index_list.append(class_index)\n",
    "\n",
    "        f1_score_list = []\n",
    "        for index in class_index_list:\n",
    "            if index not in preds_list:\n",
    "                sub_f1_score = 0\n",
    "            else:\n",
    "                tp = 0\n",
    "                fp = 0\n",
    "                fn = 0\n",
    "                for i in range(len(preds_list)):\n",
    "                    if preds_list[i] == index and label_list[i] == index:\n",
    "                        tp+=1\n",
    "                    if preds_list[i] == index and label_list[i] != index: \n",
    "                        fp+=1\n",
    "                    if preds_list[i] != index and label_list[i] == index:\n",
    "                        fn+=1\n",
    "                prec_val = tp/(tp+fp) \n",
    "                recall_val = tp/(tp+fn)\n",
    "                sub_f1_score = 2*(prec_val*recall_val)/(prec_val+recall_val+eps)\n",
    "            f1_score_list.append(sub_f1_score)\n",
    "\n",
    "        batch_f1_score = sum(f1_score_list)/len(f1_score_list)\n",
    "\n",
    "        return batch_f1_score\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b48728e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random seed:1\n",
      "train device:cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:0,batch:1248,lr:0.001,loss:0.7694,mean_loss:1.442,train_f1:0.264,valid_f1:0.304: 100%|▉| 1249/1251 [01:39<00:00, \n",
      "epoch:1,batch:1248,lr:0.0008,loss:0.4283,mean_loss:0.509,train_f1:0.691,valid_f1:0.73: 100%|▉| 1249/1251 [01:40<00:00, \n",
      "epoch:2,batch:1248,lr:0.00064,loss:0.2753,mean_loss:0.319,train_f1:0.839,valid_f1:0.847: 100%|▉| 1249/1251 [01:39<00:00\n",
      "epoch:3,batch:1248,lr:0.00051,loss:0.2345,mean_loss:0.256,train_f1:0.88,valid_f1:0.876: 100%|▉| 1249/1251 [01:40<00:00,\n",
      "epoch:4,batch:1248,lr:0.00041,loss:0.2978,mean_loss:0.223,train_f1:0.9,valid_f1:0.89: 100%|▉| 1249/1251 [01:40<00:00, 1\n",
      "epoch:5,batch:1248,lr:0.00033,loss:0.1748,mean_loss:0.201,train_f1:0.911,valid_f1:0.895: 100%|▉| 1249/1251 [01:39<00:00\n",
      "epoch:6,batch:1248,lr:0.00026,loss:0.183,mean_loss:0.185,train_f1:0.921,valid_f1:0.897: 100%|▉| 1249/1251 [01:40<00:00,\n",
      "epoch:7,batch:1248,lr:0.00021,loss:0.2157,mean_loss:0.174,train_f1:0.926,valid_f1:0.9: 100%|▉| 1249/1251 [01:40<00:00, \n",
      "epoch:8,batch:1248,lr:0.00017,loss:0.198,mean_loss:0.165,train_f1:0.931,valid_f1:0.901: 100%|▉| 1249/1251 [01:38<00:00,\n",
      "epoch:9,batch:1248,lr:0.00013,loss:0.1335,mean_loss:0.158,train_f1:0.934,valid_f1:0.902: 100%|▉| 1249/1251 [01:38<00:00\n",
      "epoch:10,batch:1248,lr:0.001,loss:0.1941,mean_loss:0.181,train_f1:0.921,valid_f1:0.901: 100%|▉| 1249/1251 [01:39<00:00,\n",
      "epoch:11,batch:1248,lr:0.0008,loss:0.1496,mean_loss:0.155,train_f1:0.934,valid_f1:0.906: 100%|▉| 1249/1251 [01:39<00:00\n",
      "epoch:12,batch:1248,lr:0.00064,loss:0.0939,mean_loss:0.136,train_f1:0.944,valid_f1:0.908: 100%|▉| 1249/1251 [01:42<00:0\n",
      "epoch:13,batch:1248,lr:0.00051,loss:0.0945,mean_loss:0.121,train_f1:0.951,valid_f1:0.909: 100%|▉| 1249/1251 [01:45<00:0\n",
      "epoch:14,batch:1248,lr:0.00041,loss:0.1497,mean_loss:0.109,train_f1:0.957,valid_f1:0.909: 100%|▉| 1249/1251 [01:40<00:0\n",
      "epoch:15,batch:1248,lr:0.00033,loss:0.1224,mean_loss:0.098,train_f1:0.962,valid_f1:0.907: 100%|▉| 1249/1251 [01:40<00:0\n",
      "epoch:16,batch:1248,lr:0.00026,loss:0.1611,mean_loss:0.09,train_f1:0.965,valid_f1:0.909: 100%|▉| 1249/1251 [01:40<00:00\n",
      "epoch:17,batch:1248,lr:0.00021,loss:0.1131,mean_loss:0.084,train_f1:0.968,valid_f1:0.907: 100%|▉| 1249/1251 [01:40<00:0\n",
      "epoch:18,batch:1248,lr:0.00017,loss:0.0421,mean_loss:0.079,train_f1:0.971,valid_f1:0.907: 100%|▉| 1249/1251 [01:41<00:0\n",
      "epoch:19,batch:1248,lr:0.00013,loss:0.0265,mean_loss:0.074,train_f1:0.973,valid_f1:0.908: 100%|▉| 1249/1251 [01:40<00:0\n",
      "epoch:20,batch:1248,lr:0.001,loss:0.0772,mean_loss:0.112,train_f1:0.953,valid_f1:0.905: 100%|▉| 1249/1251 [01:40<00:00,\n",
      "epoch:21,batch:1248,lr:0.0008,loss:0.0875,mean_loss:0.092,train_f1:0.962,valid_f1:0.909: 100%|▉| 1249/1251 [01:41<00:00\n",
      "epoch:22,batch:1248,lr:0.00064,loss:0.1238,mean_loss:0.079,train_f1:0.97,valid_f1:0.909: 100%|▉| 1249/1251 [01:40<00:00\n",
      "epoch:23,batch:1248,lr:0.00051,loss:0.0508,mean_loss:0.067,train_f1:0.975,valid_f1:0.908: 100%|▉| 1249/1251 [01:40<00:0\n",
      "epoch:24,batch:236,lr:0.00041,loss:0.067,mean_loss:0.056,train_f1:0.978,valid_f1:0.909:  19%|▏| 237/1251 [00:19<01:21, \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2288/2592556474.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mMyTrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_seed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmy_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2288/391788532.py\u001b[0m in \u001b[0;36mmy_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    126\u001b[0m                     \u001b[0mvalid_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalid_label\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m                 \u001b[0mtrain_y_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmy_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m                 \u001b[0mbatch_train_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_y_hat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2288/391788532.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;31m#         print('embedding:',X.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;31m#         print('rnn:',X.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    948\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m             result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[0;32m    951\u001b[0m                              self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0;32m    952\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    MyTrain(max_epoch=100,random_seed=1).my_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580b830f",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c09f5ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model_path,test_data_path):\n",
    "    p_model = MyModel()\n",
    "    best_stat_dict = torch.load(model_path)\n",
    "    p_model.load_state_dict(best_stat_dict)\n",
    "    if torch.cuda.is_available():\n",
    "        p_model.cuda()\n",
    "    test_data = pd.read_csv(test_data_path,sep='\\t')\n",
    "    test_text = test_data.text\n",
    "    test_label = np.ones_like(test_text) #虚拟一个方便生成dataset和DataLoader\n",
    "    test_dataset = MyDataset(test_text,test_label)\n",
    "    test_dataloader = DataLoader(test_dataset,batch_size=1)\n",
    "    test_dataloader = tqdm(test_dataloader)\n",
    "    pred_list = []\n",
    "    for test_index,batch_data in enumerate(test_dataloader):\n",
    "        batch_text,batch_label = batch_data\n",
    "        if torch.cuda.is_available():\n",
    "            batch_text = batch_text.cuda()\n",
    "        p_model.eval()\n",
    "        preds = p_model(batch_text)\n",
    "        pred_class_index = preds.argmax()\n",
    "        pred_list.append(pred_class_index.item())\n",
    "    print(len(pred_list))\n",
    "    return pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc1fadf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 50000/50000 [01:53<00:00, 439.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'to_csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2288/3359126139.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpred_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./embedding_gru_best'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'./test.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mpred_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpred_array\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test_pred'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'down'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'to_csv'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    pred_list = predict('./embedding_gru_best_test_data_0.918','./test.csv')\n",
    "    pred_data = pd.DataFrame(pred_list)\n",
    "    pred_data.head()\n",
    "    pred_data.to_csv('test_pred.csv',sep='\\t',index=False)\n",
    "    print('down')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5c62b9",
   "metadata": {},
   "source": [
    "## 调参"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0f45d2",
   "metadata": {},
   "source": [
    "使用上面的模型在测试集上获得的得分是0.918，下面进行调参，看是否能获得更好的分数，首先，把只取前1000个字，改成测试取前不同个数的字，对比结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "361196a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def split_train_valid(csv_path,train_prec=0.75):\n",
    "    csv_data = pd.read_csv(csv_path,sep='\\t') \n",
    "    text_data = csv_data.text\n",
    "    label_data = csv_data.label\n",
    "    \n",
    "    mask = np.random.rand(len(text_data))<train_prec\n",
    "    train_text_data = text_data[mask].reset_index(drop=True)\n",
    "    valid_text_data = text_data[~mask].reset_index(drop=True)\n",
    "    train_label_data = label_data[mask].reset_index(drop=True)\n",
    "    valid_label_data = label_data[~mask].reset_index(drop=True)\n",
    "    \n",
    "    return train_text_data,valid_text_data,train_label_data,valid_label_data\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self,text_data,label_data,word_num):\n",
    "        self.text_data = text_data\n",
    "        self.label_data = label_data\n",
    "        self.word_num = word_num\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        #所有text内的token索引增加1，0空出来代表空格，将每个text控制在一定长度内\n",
    "        text_str = self.text_data[index]\n",
    "        text_list = [int(x)+1 for x in text_str.split()]\n",
    "        if len(text_list)>=self.word_num:\n",
    "            text_list = text_list[:self.word_num]\n",
    "        else:\n",
    "            text_list.extend([0]*(self.word_num-len(text_list)))\n",
    "        text_array = np.array(text_list)\n",
    "        label_array = np.array(self.label_data[index])\n",
    "        return text_array,label_array\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text_data)\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(7551,100)\n",
    "        self.rnn = nn.GRU(100,50,batch_first=True)\n",
    "        self.fc = nn.Linear(50,14)\n",
    "        \n",
    "    def forward(self,X):\n",
    "#         print('X:',X.shape)\n",
    "        X = self.embedding(X)\n",
    "#         print('embedding:',X.shape)\n",
    "        _,X = self.rnn(X)\n",
    "#         print('rnn:',X.shape)\n",
    "        X = X.squeeze(dim=0)\n",
    "#         print('squeeze:',X.shape)\n",
    "        y_hat = self.fc(X)\n",
    "#         print('y_hat:',y_hat.shape)\n",
    "        return y_hat\n",
    "\n",
    "    \n",
    "class MyTrain():\n",
    "    def __init__(self,max_epoch=1,random_seed=1,lr=0.001,out_dir='./',word_num= 1000):\n",
    "        self.max_epoch = max_epoch\n",
    "        self.random_seed = random_seed\n",
    "        self.lr = lr\n",
    "        self.out_dir = out_dir\n",
    "        self.iter = 0\n",
    "        self.word_num = word_num\n",
    "        \n",
    "    def fix_random(self):\n",
    "        import random\n",
    "        import numpy as np\n",
    "        import torch\n",
    "        random.seed(self.random_seed)\n",
    "        np.random.seed(self.random_seed)\n",
    "        torch.random.manual_seed(self.random_seed)\n",
    "        torch.cuda.random.manual_seed_all(self.random_seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        print(f'random seed:{self.random_seed}')\n",
    "        \n",
    "    def my_train(self):\n",
    "        \n",
    "        max_epoch,lr = self.max_epoch,self.lr\n",
    "        \n",
    "        if self.random_seed is not None:\n",
    "            self.fix_random()\n",
    "        \n",
    "        train_text_data,valid_text_data,train_label_data,valid_label_data = split_train_valid('./train_set.csv',train_prec=0.75)\n",
    "        train_dataset = MyDataset(train_text_data,train_label_data,self.word_num)\n",
    "        valid_dataset = MyDataset(valid_text_data,valid_label_data,self.word_num)\n",
    "        \n",
    "        my_model = MyModel()\n",
    "        my_optim = torch.optim.Adam(my_model.parameters(),lr=lr)\n",
    "        my_loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            my_model.cuda()\n",
    "            my_loss.cuda()\n",
    "        print(f'train device:{next(iter(my_model.parameters())).device}')  #显示训练设备\n",
    "        \n",
    "        best_f1_score = 0\n",
    "        epoch_index = 0\n",
    "\n",
    "        for epoch_index in range(max_epoch):\n",
    "            \n",
    "            loss_list = []\n",
    "            train_f1_score_list = []\n",
    "            valid_f1_score_list = []\n",
    "            train_dataloader = DataLoader(train_dataset,batch_size=120,shuffle=True)\n",
    "            valid_dataloader = DataLoader(valid_dataset,batch_size=40,shuffle=True)\n",
    "            my_dataloader = tqdm(train_dataloader)\n",
    "            \n",
    "            batch_index = 0\n",
    "                        \n",
    "            for train_data,valid_data in zip(my_dataloader,valid_dataloader):\n",
    "                                \n",
    "                my_model.train()  #将模型设置为训练模式\n",
    "                train_text,train_label = train_data\n",
    "                valid_text,valid_label = valid_data\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    train_text = train_text.cuda()\n",
    "                    train_label = train_label.cuda()\n",
    "                    valid_text = valid_text.cuda()\n",
    "                    valid_label = valid_label.cuda()\n",
    "                \n",
    "                train_y_hat = my_model(train_text)\n",
    "                batch_train_loss = my_loss(train_y_hat,train_label)\n",
    "                \n",
    "                my_optim.zero_grad()\n",
    "                batch_train_loss.backward()\n",
    "                my_optim.step()\n",
    "                my_optim.param_groups[0]['lr'] = lr*(0.8**(epoch_index%10))\n",
    "        \n",
    "                my_model.eval()  #将模型设置为验证模式\n",
    "                with torch.no_grad():\n",
    "                    valid_y_hat = my_model(valid_text)\n",
    "                    batch_valid_f1_score = self.f1_score(valid_y_hat.data,valid_label.data)\n",
    "                    valid_f1_score_list.append(batch_valid_f1_score)\n",
    "                    mean_valid_f1 = round(sum(valid_f1_score_list)/len(valid_f1_score_list),3)\n",
    "\n",
    "                    #显示batch结果\n",
    "                    batch_lr = round(my_optim.param_groups[0]['lr'],5)\n",
    "                    batch_loss = round(batch_train_loss.item(),4)\n",
    "                    loss_list.append(batch_loss)\n",
    "                    mean_loss = round((sum(loss_list)/len(loss_list)),3)\n",
    "\n",
    "                    batch_train_f1_score = self.f1_score(train_y_hat.data,train_label.data)\n",
    "                    train_f1_score_list.append(batch_train_f1_score)\n",
    "                    mean_train_f1 = round(sum(train_f1_score_list)/len(train_f1_score_list),3)\n",
    "\n",
    "                    my_dataloader.set_description(f'epoch:{epoch_index},batch:{batch_index},lr:{batch_lr},loss:{batch_loss},mean_loss:{mean_loss},train_f1:{mean_train_f1},valid_f1:{mean_valid_f1}')\n",
    "\n",
    "                    #存储模型\n",
    "                    if batch_valid_f1_score>0.96:\n",
    "                        torch.save(my_model.state_dict(),os.path.join(self.out_dir,f'embedding_gru_best_{self.word_num}word_valid_f1_score_{round(batch_valid_f1_score,4)}'))\n",
    "                        best_f1_score = batch_train_f1_score\n",
    "\n",
    "                    batch_index+=1\n",
    "            \n",
    "                \n",
    "    def f1_score(self,y_hat,label,eps=1e-8):\n",
    "        #y_hat(N,C),label(1)\n",
    "        y_hat = y_hat.cpu()\n",
    "        label = label.cpu()\n",
    "        preds_list = list(torch.argmax(y_hat,dim=1).numpy())\n",
    "        label_list = list(label.numpy())\n",
    "#         print(f'preds:{preds_list},label:{label_list}')\n",
    "        class_index_list = []\n",
    "        for class_index in label_list:\n",
    "            if class_index not in class_index_list:\n",
    "                class_index_list.append(class_index)\n",
    "\n",
    "        f1_score_list = []\n",
    "        for index in class_index_list:\n",
    "            if index not in preds_list:\n",
    "                sub_f1_score = 0\n",
    "            else:\n",
    "                tp = 0\n",
    "                fp = 0\n",
    "                fn = 0\n",
    "                for i in range(len(preds_list)):\n",
    "                    if preds_list[i] == index and label_list[i] == index:\n",
    "                        tp+=1\n",
    "                    if preds_list[i] == index and label_list[i] != index: \n",
    "                        fp+=1\n",
    "                    if preds_list[i] != index and label_list[i] == index:\n",
    "                        fn+=1\n",
    "                prec_val = tp/(tp+fp) \n",
    "                recall_val = tp/(tp+fn)\n",
    "                sub_f1_score = 2*(prec_val*recall_val)/(prec_val+recall_val+eps)\n",
    "            f1_score_list.append(sub_f1_score)\n",
    "\n",
    "        batch_f1_score = sum(f1_score_list)/len(f1_score_list)\n",
    "\n",
    "        return batch_f1_score\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b38ce7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word num: 50\n",
      "random seed:1\n",
      "train device:cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:0,batch:1248,lr:0.001,loss:0.5032,mean_loss:0.988,train_f1:0.49,valid_f1:0.534: 100%|▉| 1249/1251 [01:04<00:00, 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word num: 100\n",
      "random seed:1\n",
      "train device:cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:0,batch:1248,lr:0.001,loss:0.4708,mean_loss:1.019,train_f1:0.47,valid_f1:0.506: 100%|▉| 1249/1251 [01:01<00:00, 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word num: 300\n",
      "random seed:1\n",
      "train device:cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:0,batch:1248,lr:0.001,loss:0.4394,mean_loss:1.103,train_f1:0.432,valid_f1:0.476: 100%|▉| 1249/1251 [01:09<00:00, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word num: 500\n",
      "random seed:1\n",
      "train device:cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:0,batch:1248,lr:0.001,loss:0.4475,mean_loss:1.196,train_f1:0.386,valid_f1:0.43: 100%|▉| 1249/1251 [01:20<00:00, 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word num: 800\n",
      "random seed:1\n",
      "train device:cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:0,batch:1248,lr:0.001,loss:0.5433,mean_loss:1.261,train_f1:0.351,valid_f1:0.395: 100%|▉| 1249/1251 [01:28<00:00, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word num: 1000\n",
      "random seed:1\n",
      "train device:cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:0,batch:1248,lr:0.001,loss:0.7694,mean_loss:1.442,train_f1:0.264,valid_f1:0.304: 100%|▉| 1249/1251 [01:38<00:00, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word num: 2000\n",
      "random seed:1\n",
      "train device:cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:0,batch:1248,lr:0.001,loss:0.8041,mean_loss:1.791,train_f1:0.161,valid_f1:0.19: 100%|▉| 1249/1251 [05:02<00:00,  \n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    word_num_list = [50,100,300,500,800,1000,2000]\n",
    "    for word_num in word_num_list:\n",
    "        print('word num:',word_num)\n",
    "        MyTrain(max_epoch=1,random_seed=1,word_num=word_num).my_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d4720f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word num: 10\n",
      "random seed:1\n",
      "train device:cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:0,batch:1248,lr:0.001,loss:0.6856,mean_loss:1.24,train_f1:0.434,valid_f1:0.466: 100%|▉| 1249/1251 [00:56<00:00, 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word num: 20\n",
      "random seed:1\n",
      "train device:cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:0,batch:1248,lr:0.001,loss:0.5959,mean_loss:1.061,train_f1:0.496,valid_f1:0.523: 100%|▉| 1249/1251 [00:59<00:00, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word num: 30\n",
      "random seed:1\n",
      "train device:cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:0,batch:1248,lr:0.001,loss:0.5128,mean_loss:0.916,train_f1:0.528,valid_f1:0.566: 100%|▉| 1249/1251 [01:19<00:00, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word num: 40\n",
      "random seed:1\n",
      "train device:cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:0,batch:1248,lr:0.001,loss:0.4599,mean_loss:0.957,train_f1:0.513,valid_f1:0.555: 100%|▉| 1249/1251 [01:04<00:00, \n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    word_num_list = [10,20,30,40]\n",
    "    for word_num in word_num_list:\n",
    "        print('word num:',word_num)\n",
    "        MyTrain(max_epoch=1,random_seed=1,word_num=word_num).my_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b432b7da",
   "metadata": {},
   "source": [
    "每段text，前后各取word_num个字拼成一个新的text，下面是计算结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54b710ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def split_train_valid(csv_path,train_prec=0.75):\n",
    "    csv_data = pd.read_csv(csv_path,sep='\\t') \n",
    "    text_data = csv_data.text\n",
    "    label_data = csv_data.label\n",
    "    \n",
    "    mask = np.random.rand(len(text_data))<train_prec\n",
    "    train_text_data = text_data[mask].reset_index(drop=True)\n",
    "    valid_text_data = text_data[~mask].reset_index(drop=True)\n",
    "    train_label_data = label_data[mask].reset_index(drop=True)\n",
    "    valid_label_data = label_data[~mask].reset_index(drop=True)\n",
    "    \n",
    "    return train_text_data,valid_text_data,train_label_data,valid_label_data\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self,text_data,label_data,word_num):\n",
    "        self.text_data = text_data\n",
    "        self.label_data = label_data\n",
    "        self.word_num = word_num\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        #所有text内的token索引增加1，0空出来代表空格，将每个text控制在一定长度内\n",
    "        text_str = self.text_data[index]\n",
    "        text_list = [int(x)+1 for x in text_str.split()]\n",
    "        if len(text_list)>=2*self.word_num:\n",
    "            text_list = text_list[:self.word_num]+text_list[-1*self.word_num:]\n",
    "        else:\n",
    "            text_list.extend([0]*(2*self.word_num-len(text_list)))\n",
    "        text_array = np.array(text_list)\n",
    "        label_array = np.array(self.label_data[index])\n",
    "        return text_array,label_array\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text_data)\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(7551,100)\n",
    "        self.rnn = nn.GRU(100,50,batch_first=True)\n",
    "        self.fc = nn.Linear(50,14)\n",
    "        \n",
    "    def forward(self,X):\n",
    "#         print('X:',X.shape)\n",
    "        X = self.embedding(X)\n",
    "#         print('embedding:',X.shape)\n",
    "        _,X = self.rnn(X)\n",
    "#         print('rnn:',X.shape)\n",
    "        X = X.squeeze(dim=0)\n",
    "#         print('squeeze:',X.shape)\n",
    "        y_hat = self.fc(X)\n",
    "#         print('y_hat:',y_hat.shape)\n",
    "        return y_hat\n",
    "\n",
    "    \n",
    "class MyTrain():\n",
    "    def __init__(self,max_epoch=1,random_seed=1,lr=0.001,out_dir='./',word_num= 1000):\n",
    "        self.max_epoch = max_epoch\n",
    "        self.random_seed = random_seed\n",
    "        self.lr = lr\n",
    "        self.out_dir = out_dir\n",
    "        self.iter = 0\n",
    "        self.word_num = word_num\n",
    "        \n",
    "    def fix_random(self):\n",
    "        import random\n",
    "        import numpy as np\n",
    "        import torch\n",
    "        random.seed(self.random_seed)\n",
    "        np.random.seed(self.random_seed)\n",
    "        torch.random.manual_seed(self.random_seed)\n",
    "        torch.cuda.random.manual_seed_all(self.random_seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        print(f'random seed:{self.random_seed}')\n",
    "        \n",
    "    def my_train(self):\n",
    "        \n",
    "        max_epoch,lr = self.max_epoch,self.lr\n",
    "        \n",
    "        if self.random_seed is not None:\n",
    "            self.fix_random()\n",
    "        \n",
    "        train_text_data,valid_text_data,train_label_data,valid_label_data = split_train_valid('./train_set.csv',train_prec=0.75)\n",
    "        train_dataset = MyDataset(train_text_data,train_label_data,self.word_num)\n",
    "        valid_dataset = MyDataset(valid_text_data,valid_label_data,self.word_num)\n",
    "        \n",
    "        my_model = MyModel()\n",
    "        my_optim = torch.optim.Adam(my_model.parameters(),lr=lr)\n",
    "        my_loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            my_model.cuda()\n",
    "            my_loss.cuda()\n",
    "        print(f'train device:{next(iter(my_model.parameters())).device}')  #显示训练设备\n",
    "        \n",
    "        best_f1_score = 0\n",
    "        epoch_index = 0\n",
    "\n",
    "        for epoch_index in range(max_epoch):\n",
    "            \n",
    "            loss_list = []\n",
    "            train_f1_score_list = []\n",
    "            valid_f1_score_list = []\n",
    "            train_dataloader = DataLoader(train_dataset,batch_size=120,shuffle=True)\n",
    "            valid_dataloader = DataLoader(valid_dataset,batch_size=40,shuffle=True)\n",
    "            my_dataloader = tqdm(train_dataloader)\n",
    "            \n",
    "            batch_index = 0\n",
    "                        \n",
    "            for train_data,valid_data in zip(my_dataloader,valid_dataloader):\n",
    "                                \n",
    "                my_model.train()  #将模型设置为训练模式\n",
    "                train_text,train_label = train_data\n",
    "                valid_text,valid_label = valid_data\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    train_text = train_text.cuda()\n",
    "                    train_label = train_label.cuda()\n",
    "                    valid_text = valid_text.cuda()\n",
    "                    valid_label = valid_label.cuda()\n",
    "                \n",
    "                train_y_hat = my_model(train_text)\n",
    "                batch_train_loss = my_loss(train_y_hat,train_label)\n",
    "                \n",
    "                my_optim.zero_grad()\n",
    "                batch_train_loss.backward()\n",
    "                my_optim.step()\n",
    "                my_optim.param_groups[0]['lr'] = lr*(0.8**(epoch_index%10))\n",
    "        \n",
    "                my_model.eval()  #将模型设置为验证模式\n",
    "                with torch.no_grad():\n",
    "                    valid_y_hat = my_model(valid_text)\n",
    "                    batch_valid_f1_score = self.f1_score(valid_y_hat.data,valid_label.data)\n",
    "                    valid_f1_score_list.append(batch_valid_f1_score)\n",
    "                    mean_valid_f1 = round(sum(valid_f1_score_list)/len(valid_f1_score_list),3)\n",
    "\n",
    "                    #显示batch结果\n",
    "                    batch_lr = round(my_optim.param_groups[0]['lr'],5)\n",
    "                    batch_loss = round(batch_train_loss.item(),4)\n",
    "                    loss_list.append(batch_loss)\n",
    "                    mean_loss = round((sum(loss_list)/len(loss_list)),3)\n",
    "\n",
    "                    batch_train_f1_score = self.f1_score(train_y_hat.data,train_label.data)\n",
    "                    train_f1_score_list.append(batch_train_f1_score)\n",
    "                    mean_train_f1 = round(sum(train_f1_score_list)/len(train_f1_score_list),3)\n",
    "\n",
    "                    my_dataloader.set_description(f'epoch:{epoch_index},batch:{batch_index},lr:{batch_lr},loss:{batch_loss},mean_loss:{mean_loss},train_f1:{mean_train_f1},valid_f1:{mean_valid_f1}')\n",
    "\n",
    "                    #存储模型\n",
    "                    if mean_valid_f1>0.96:\n",
    "                        torch.save(my_model.state_dict(),os.path.join(self.out_dir,f'embedding_gru_best_{self.word_num}word_valid_f1_score_{mean_valid_f1}'))\n",
    "\n",
    "                    batch_index+=1\n",
    "            \n",
    "                \n",
    "    def f1_score(self,y_hat,label,eps=1e-8):\n",
    "        #y_hat(N,C),label(1)\n",
    "        y_hat = y_hat.cpu()\n",
    "        label = label.cpu()\n",
    "        preds_list = list(torch.argmax(y_hat,dim=1).numpy())\n",
    "        label_list = list(label.numpy())\n",
    "#         print(f'preds:{preds_list},label:{label_list}')\n",
    "        class_index_list = []\n",
    "        for class_index in label_list:\n",
    "            if class_index not in class_index_list:\n",
    "                class_index_list.append(class_index)\n",
    "\n",
    "        f1_score_list = []\n",
    "        for index in class_index_list:\n",
    "            if index not in preds_list:\n",
    "                sub_f1_score = 0\n",
    "            else:\n",
    "                tp = 0\n",
    "                fp = 0\n",
    "                fn = 0\n",
    "                for i in range(len(preds_list)):\n",
    "                    if preds_list[i] == index and label_list[i] == index:\n",
    "                        tp+=1\n",
    "                    if preds_list[i] == index and label_list[i] != index: \n",
    "                        fp+=1\n",
    "                    if preds_list[i] != index and label_list[i] == index:\n",
    "                        fn+=1\n",
    "                prec_val = tp/(tp+fp) \n",
    "                recall_val = tp/(tp+fn)\n",
    "                sub_f1_score = 2*(prec_val*recall_val)/(prec_val+recall_val+eps)\n",
    "            f1_score_list.append(sub_f1_score)\n",
    "\n",
    "        batch_f1_score = sum(f1_score_list)/len(f1_score_list)\n",
    "\n",
    "        return batch_f1_score\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ac62dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word num: 10\n",
      "random seed:1\n",
      "train device:cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:0,batch:1248,lr:0.001,loss:0.6808,mean_loss:1.243,train_f1:0.424,valid_f1:0.447: 100%|▉| 1249/1251 [00:57<00:00, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word num: 20\n",
      "random seed:1\n",
      "train device:cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:0,batch:1248,lr:0.001,loss:0.6044,mean_loss:1.189,train_f1:0.434,valid_f1:0.454: 100%|▉| 1249/1251 [01:03<00:00, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word num: 30\n",
      "random seed:1\n",
      "train device:cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:0,batch:1248,lr:0.001,loss:0.5977,mean_loss:1.112,train_f1:0.455,valid_f1:0.48: 100%|▉| 1249/1251 [00:53<00:00, 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word num: 40\n",
      "random seed:1\n",
      "train device:cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:0,batch:1248,lr:0.001,loss:0.5908,mean_loss:1.125,train_f1:0.449,valid_f1:0.473: 100%|▉| 1249/1251 [01:13<00:00, \n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    word_num_list = [10,20,30,40]\n",
    "    for word_num in word_num_list:\n",
    "        print('word num:',word_num)\n",
    "        MyTrain(max_epoch=1,random_seed=1,word_num=word_num).my_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a677916",
   "metadata": {},
   "source": [
    "可以看到，效果比只用前面word_num个字效果要差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6e40822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def split_train_valid(csv_path,train_prec=0.75):\n",
    "    csv_data = pd.read_csv(csv_path,sep='\\t') \n",
    "    text_data = csv_data.text\n",
    "    label_data = csv_data.label\n",
    "    \n",
    "    mask = np.random.rand(len(text_data))<train_prec\n",
    "    train_text_data = text_data[mask].reset_index(drop=True)\n",
    "    valid_text_data = text_data[~mask].reset_index(drop=True)\n",
    "    train_label_data = label_data[mask].reset_index(drop=True)\n",
    "    valid_label_data = label_data[~mask].reset_index(drop=True)\n",
    "    \n",
    "    return train_text_data,valid_text_data,train_label_data,valid_label_data\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self,text_data,label_data,word_num):\n",
    "        self.text_data = text_data\n",
    "        self.label_data = label_data\n",
    "        self.word_num = word_num\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        #所有text内的token索引增加1，0空出来代表空格，将每个text控制在一定长度内\n",
    "        text_str = self.text_data[index]\n",
    "        text_list = [int(x)+1 for x in text_str.split()]\n",
    "        if len(text_list)>=self.word_num:\n",
    "            text_list = text_list[:self.word_num]\n",
    "        else:\n",
    "            text_list.extend([0]*(self.word_num-len(text_list)))\n",
    "        text_array = np.array(text_list)\n",
    "        label_array = np.array(self.label_data[index])\n",
    "        return text_array,label_array\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text_data)\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(7551,100)\n",
    "        self.rnn = nn.GRU(100,50,batch_first=True)\n",
    "        self.fc = nn.Linear(50,14)\n",
    "        \n",
    "    def forward(self,X):\n",
    "#         print('X:',X.shape)\n",
    "        X = self.embedding(X)\n",
    "#         print('embedding:',X.shape)\n",
    "        _,X = self.rnn(X)\n",
    "#         print('rnn:',X.shape)\n",
    "        X = X.squeeze(dim=0)\n",
    "#         print('squeeze:',X.shape)\n",
    "        y_hat = self.fc(X)\n",
    "#         print('y_hat:',y_hat.shape)\n",
    "        return y_hat\n",
    "\n",
    "    \n",
    "class MyTrain():\n",
    "    def __init__(self,max_epoch=1,random_seed=1,lr=0.001,out_dir='./',word_num= 1000):\n",
    "        self.max_epoch = max_epoch\n",
    "        self.random_seed = random_seed\n",
    "        self.lr = lr\n",
    "        self.out_dir = out_dir\n",
    "        self.iter = 0\n",
    "        self.word_num = word_num\n",
    "        \n",
    "    def fix_random(self):\n",
    "        import random\n",
    "        import numpy as np\n",
    "        import torch\n",
    "        random.seed(self.random_seed)\n",
    "        np.random.seed(self.random_seed)\n",
    "        torch.random.manual_seed(self.random_seed)\n",
    "        torch.cuda.random.manual_seed_all(self.random_seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        print(f'random seed:{self.random_seed}')\n",
    "        \n",
    "    def my_train(self):\n",
    "        \n",
    "        max_epoch,lr = self.max_epoch,self.lr\n",
    "        \n",
    "        if self.random_seed is not None:\n",
    "            self.fix_random()\n",
    "        \n",
    "        train_text_data,valid_text_data,train_label_data,valid_label_data = split_train_valid('./train_set.csv',train_prec=0.75)\n",
    "        train_dataset = MyDataset(train_text_data,train_label_data,self.word_num)\n",
    "        valid_dataset = MyDataset(valid_text_data,valid_label_data,self.word_num)\n",
    "        \n",
    "        my_model = MyModel()\n",
    "        my_optim = torch.optim.Adam(my_model.parameters(),lr=lr)\n",
    "        my_loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            my_model.cuda()\n",
    "            my_loss.cuda()\n",
    "        print(f'train device:{next(iter(my_model.parameters())).device}')  #显示训练设备\n",
    "        \n",
    "        best_f1_score = 0\n",
    "        epoch_index = 0\n",
    "\n",
    "        for epoch_index in range(max_epoch):\n",
    "            \n",
    "            loss_list = []\n",
    "            train_f1_score_list = []\n",
    "            valid_f1_score_list = []\n",
    "            train_dataloader = DataLoader(train_dataset,batch_size=120,shuffle=True)\n",
    "            valid_dataloader = DataLoader(valid_dataset,batch_size=40,shuffle=True)\n",
    "            my_dataloader = tqdm(train_dataloader)\n",
    "            \n",
    "            batch_index = 0\n",
    "                        \n",
    "            for train_data,valid_data in zip(my_dataloader,valid_dataloader):\n",
    "                                \n",
    "                my_model.train()  #将模型设置为训练模式\n",
    "                train_text,train_label = train_data\n",
    "                valid_text,valid_label = valid_data\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    train_text = train_text.cuda()\n",
    "                    train_label = train_label.cuda()\n",
    "                    valid_text = valid_text.cuda()\n",
    "                    valid_label = valid_label.cuda()\n",
    "                \n",
    "                train_y_hat = my_model(train_text)\n",
    "                batch_train_loss = my_loss(train_y_hat,train_label)\n",
    "                \n",
    "                my_optim.zero_grad()\n",
    "                batch_train_loss.backward()\n",
    "                my_optim.step()\n",
    "                my_optim.param_groups[0]['lr'] = lr*(0.8**(epoch_index%10))\n",
    "        \n",
    "                my_model.eval()  #将模型设置为验证模式\n",
    "                with torch.no_grad():\n",
    "                    valid_y_hat = my_model(valid_text)\n",
    "                    batch_valid_f1_score = self.f1_score(valid_y_hat.data,valid_label.data)\n",
    "                    valid_f1_score_list.append(batch_valid_f1_score)\n",
    "                    mean_valid_f1 = round(sum(valid_f1_score_list)/len(valid_f1_score_list),3)\n",
    "\n",
    "                    #显示batch结果\n",
    "                    batch_lr = round(my_optim.param_groups[0]['lr'],5)\n",
    "                    batch_loss = round(batch_train_loss.item(),4)\n",
    "                    loss_list.append(batch_loss)\n",
    "                    mean_loss = round((sum(loss_list)/len(loss_list)),3)\n",
    "\n",
    "                    batch_train_f1_score = self.f1_score(train_y_hat.data,train_label.data)\n",
    "                    train_f1_score_list.append(batch_train_f1_score)\n",
    "                    mean_train_f1 = round(sum(train_f1_score_list)/len(train_f1_score_list),3)\n",
    "\n",
    "                    my_dataloader.set_description(f'epoch:{epoch_index},batch:{batch_index},lr:{batch_lr},loss:{batch_loss},mean_loss:{mean_loss},train_f1:{mean_train_f1},valid_f1:{mean_valid_f1}')\n",
    "                    \n",
    "                    batch_index+=1\n",
    "            #存储模型\n",
    "            if mean_valid_f1>0.96:\n",
    "                torch.save(my_model.state_dict(),os.path.join(self.out_dir,f'embedding_gru_best_{self.word_num}word_valid_f1_score_{mean_valid_f1}'))\n",
    "\n",
    "            \n",
    "                \n",
    "    def f1_score(self,y_hat,label,eps=1e-8):\n",
    "        #y_hat(N,C),label(1)\n",
    "        y_hat = y_hat.cpu()\n",
    "        label = label.cpu()\n",
    "        preds_list = list(torch.argmax(y_hat,dim=1).numpy())\n",
    "        label_list = list(label.numpy())\n",
    "#         print(f'preds:{preds_list},label:{label_list}')\n",
    "        class_index_list = []\n",
    "        for class_index in label_list:\n",
    "            if class_index not in class_index_list:\n",
    "                class_index_list.append(class_index)\n",
    "\n",
    "        f1_score_list = []\n",
    "        for index in class_index_list:\n",
    "            if index not in preds_list:\n",
    "                sub_f1_score = 0\n",
    "            else:\n",
    "                tp = 0\n",
    "                fp = 0\n",
    "                fn = 0\n",
    "                for i in range(len(preds_list)):\n",
    "                    if preds_list[i] == index and label_list[i] == index:\n",
    "                        tp+=1\n",
    "                    if preds_list[i] == index and label_list[i] != index: \n",
    "                        fp+=1\n",
    "                    if preds_list[i] != index and label_list[i] == index:\n",
    "                        fn+=1\n",
    "                prec_val = tp/(tp+fp) \n",
    "                recall_val = tp/(tp+fn)\n",
    "                sub_f1_score = 2*(prec_val*recall_val)/(prec_val+recall_val+eps)\n",
    "            f1_score_list.append(sub_f1_score)\n",
    "\n",
    "        batch_f1_score = sum(f1_score_list)/len(f1_score_list)\n",
    "\n",
    "        return batch_f1_score\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88cec79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word num: 10\n",
      "random seed:1\n",
      "train device:cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:0,batch:1248,lr:0.001,loss:0.6856,mean_loss:1.24,train_f1:0.434,valid_f1:0.466: 100%|▉| 1249/1251 [00:56<00:00, 2\n",
      "epoch:1,batch:1248,lr:0.0008,loss:0.8455,mean_loss:0.758,train_f1:0.678,valid_f1:0.674: 100%|▉| 1249/1251 [00:55<00:00,\n",
      "epoch:2,batch:1248,lr:0.00064,loss:0.8682,mean_loss:0.653,train_f1:0.729,valid_f1:0.708: 100%|▉| 1249/1251 [00:55<00:00\n",
      "epoch:3,batch:1248,lr:0.00051,loss:0.5282,mean_loss:0.594,train_f1:0.755,valid_f1:0.723: 100%|▉| 1249/1251 [00:55<00:00\n",
      "epoch:4,batch:1248,lr:0.00041,loss:0.6104,mean_loss:0.554,train_f1:0.772,valid_f1:0.733: 100%|▉| 1249/1251 [00:56<00:00\n",
      "epoch:5,batch:1248,lr:0.00033,loss:0.4169,mean_loss:0.525,train_f1:0.784,valid_f1:0.737: 100%|▉| 1249/1251 [00:56<00:00\n",
      "epoch:6,batch:1248,lr:0.00026,loss:0.5275,mean_loss:0.503,train_f1:0.794,valid_f1:0.738: 100%|▉| 1249/1251 [00:57<00:00\n",
      "epoch:7,batch:1248,lr:0.00021,loss:0.5297,mean_loss:0.486,train_f1:0.799,valid_f1:0.742: 100%|▉| 1249/1251 [00:55<00:00\n",
      "epoch:8,batch:1248,lr:0.00017,loss:0.3955,mean_loss:0.473,train_f1:0.803,valid_f1:0.745: 100%|▉| 1249/1251 [00:56<00:00\n",
      "epoch:9,batch:1248,lr:0.00013,loss:0.4595,mean_loss:0.462,train_f1:0.811,valid_f1:0.742: 100%|▉| 1249/1251 [00:56<00:00\n",
      "epoch:10,batch:1248,lr:0.001,loss:0.6518,mean_loss:0.502,train_f1:0.793,valid_f1:0.742: 100%|▉| 1249/1251 [00:56<00:00,\n",
      "epoch:11,batch:1248,lr:0.0008,loss:0.4889,mean_loss:0.456,train_f1:0.812,valid_f1:0.744: 100%|▉| 1249/1251 [00:58<00:00\n",
      "epoch:12,batch:1248,lr:0.00064,loss:0.3926,mean_loss:0.421,train_f1:0.826,valid_f1:0.751: 100%|▉| 1249/1251 [00:56<00:0\n",
      "epoch:13,batch:1248,lr:0.00051,loss:0.3695,mean_loss:0.393,train_f1:0.838,valid_f1:0.754: 100%|▉| 1249/1251 [00:56<00:0\n",
      "epoch:14,batch:1248,lr:0.00041,loss:0.4792,mean_loss:0.37,train_f1:0.847,valid_f1:0.75: 100%|▉| 1249/1251 [00:56<00:00,\n",
      "epoch:15,batch:1248,lr:0.00033,loss:0.4216,mean_loss:0.352,train_f1:0.855,valid_f1:0.754: 100%|▉| 1249/1251 [00:56<00:0\n",
      "epoch:16,batch:1248,lr:0.00026,loss:0.2577,mean_loss:0.337,train_f1:0.862,valid_f1:0.754: 100%|▉| 1249/1251 [00:56<00:0\n",
      "epoch:17,batch:1248,lr:0.00021,loss:0.2616,mean_loss:0.325,train_f1:0.866,valid_f1:0.754: 100%|▉| 1249/1251 [00:56<00:0\n",
      "epoch:18,batch:1248,lr:0.00017,loss:0.293,mean_loss:0.315,train_f1:0.87,valid_f1:0.752: 100%|▉| 1249/1251 [00:56<00:00,\n",
      "epoch:19,batch:1248,lr:0.00013,loss:0.1667,mean_loss:0.307,train_f1:0.875,valid_f1:0.755: 100%|▉| 1249/1251 [00:56<00:0\n",
      "epoch:20,batch:1248,lr:0.001,loss:0.3772,mean_loss:0.365,train_f1:0.849,valid_f1:0.749: 100%|▉| 1249/1251 [00:56<00:00,\n",
      "epoch:21,batch:1248,lr:0.0008,loss:0.2027,mean_loss:0.328,train_f1:0.863,valid_f1:0.752: 100%|▉| 1249/1251 [00:56<00:00\n",
      "epoch:22,batch:1248,lr:0.00064,loss:0.3103,mean_loss:0.299,train_f1:0.877,valid_f1:0.755: 100%|▉| 1249/1251 [00:56<00:0\n",
      "epoch:23,batch:1248,lr:0.00051,loss:0.323,mean_loss:0.276,train_f1:0.887,valid_f1:0.754: 100%|▉| 1249/1251 [00:56<00:00\n",
      "epoch:24,batch:1248,lr:0.00041,loss:0.3519,mean_loss:0.258,train_f1:0.895,valid_f1:0.754: 100%|▉| 1249/1251 [00:56<00:0\n",
      "epoch:25,batch:1248,lr:0.00033,loss:0.2754,mean_loss:0.242,train_f1:0.903,valid_f1:0.753: 100%|▉| 1249/1251 [00:55<00:0\n",
      "epoch:26,batch:1248,lr:0.00026,loss:0.2943,mean_loss:0.23,train_f1:0.908,valid_f1:0.753: 100%|▉| 1249/1251 [00:56<00:00\n",
      "epoch:27,batch:1248,lr:0.00021,loss:0.2672,mean_loss:0.22,train_f1:0.913,valid_f1:0.75: 100%|▉| 1249/1251 [00:56<00:00,\n",
      "epoch:28,batch:1248,lr:0.00017,loss:0.1297,mean_loss:0.212,train_f1:0.917,valid_f1:0.75: 100%|▉| 1249/1251 [00:53<00:00\n",
      "epoch:29,batch:1248,lr:0.00013,loss:0.2244,mean_loss:0.206,train_f1:0.921,valid_f1:0.749: 100%|▉| 1249/1251 [00:53<00:0\n",
      "epoch:30,batch:1248,lr:0.001,loss:0.2085,mean_loss:0.279,train_f1:0.883,valid_f1:0.744: 100%|▉| 1249/1251 [00:53<00:00,\n",
      "epoch:31,batch:1248,lr:0.0008,loss:0.2299,mean_loss:0.241,train_f1:0.899,valid_f1:0.746: 100%|▉| 1249/1251 [00:53<00:00\n",
      "epoch:32,batch:1248,lr:0.00064,loss:0.1882,mean_loss:0.214,train_f1:0.914,valid_f1:0.75: 100%|▉| 1249/1251 [00:53<00:00\n",
      "epoch:33,batch:1248,lr:0.00051,loss:0.1703,mean_loss:0.194,train_f1:0.923,valid_f1:0.748: 100%|▉| 1249/1251 [00:53<00:0\n",
      "epoch:34,batch:1248,lr:0.00041,loss:0.0991,mean_loss:0.178,train_f1:0.929,valid_f1:0.747: 100%|▉| 1249/1251 [00:53<00:0\n",
      "epoch:35,batch:1248,lr:0.00033,loss:0.0913,mean_loss:0.166,train_f1:0.937,valid_f1:0.746: 100%|▉| 1249/1251 [00:53<00:0\n",
      "epoch:36,batch:1248,lr:0.00026,loss:0.0993,mean_loss:0.156,train_f1:0.941,valid_f1:0.746: 100%|▉| 1249/1251 [00:53<00:0\n",
      "epoch:37,batch:1248,lr:0.00021,loss:0.1044,mean_loss:0.148,train_f1:0.946,valid_f1:0.747: 100%|▉| 1249/1251 [00:53<00:0\n",
      "epoch:38,batch:1248,lr:0.00017,loss:0.1117,mean_loss:0.141,train_f1:0.949,valid_f1:0.744: 100%|▉| 1249/1251 [00:53<00:0\n",
      "epoch:39,batch:1248,lr:0.00013,loss:0.1236,mean_loss:0.136,train_f1:0.951,valid_f1:0.747: 100%|▉| 1249/1251 [00:53<00:0\n",
      "epoch:40,batch:1248,lr:0.001,loss:0.1627,mean_loss:0.228,train_f1:0.904,valid_f1:0.738: 100%|▉| 1249/1251 [00:53<00:00,\n",
      "epoch:41,batch:1248,lr:0.0008,loss:0.2095,mean_loss:0.18,train_f1:0.926,valid_f1:0.743: 100%|▉| 1249/1251 [00:53<00:00,\n",
      "epoch:42,batch:1248,lr:0.00064,loss:0.1689,mean_loss:0.154,train_f1:0.94,valid_f1:0.745: 100%|▉| 1249/1251 [00:53<00:00\n",
      "epoch:43,batch:1248,lr:0.00051,loss:0.159,mean_loss:0.137,train_f1:0.947,valid_f1:0.742: 100%|▉| 1249/1251 [00:53<00:00\n",
      "epoch:44,batch:1248,lr:0.00041,loss:0.1256,mean_loss:0.124,train_f1:0.953,valid_f1:0.742: 100%|▉| 1249/1251 [00:53<00:0\n",
      "epoch:45,batch:1248,lr:0.00033,loss:0.0498,mean_loss:0.114,train_f1:0.959,valid_f1:0.741: 100%|▉| 1249/1251 [00:53<00:0\n",
      "epoch:46,batch:1248,lr:0.00026,loss:0.0738,mean_loss:0.106,train_f1:0.963,valid_f1:0.741: 100%|▉| 1249/1251 [00:53<00:0\n",
      "epoch:47,batch:1248,lr:0.00021,loss:0.1085,mean_loss:0.099,train_f1:0.966,valid_f1:0.741: 100%|▉| 1249/1251 [00:54<00:0\n",
      "epoch:48,batch:1248,lr:0.00017,loss:0.1147,mean_loss:0.093,train_f1:0.968,valid_f1:0.742: 100%|▉| 1249/1251 [00:53<00:0\n",
      "epoch:49,batch:1248,lr:0.00013,loss:0.1517,mean_loss:0.089,train_f1:0.97,valid_f1:0.741: 100%|▉| 1249/1251 [00:53<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word num: 20\n",
      "random seed:1\n",
      "train device:cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:0,batch:1248,lr:0.001,loss:0.5959,mean_loss:1.061,train_f1:0.496,valid_f1:0.523: 100%|▉| 1249/1251 [00:56<00:00, \n",
      "epoch:1,batch:1248,lr:0.0008,loss:0.6406,mean_loss:0.547,train_f1:0.745,valid_f1:0.748: 100%|▉| 1249/1251 [00:56<00:00,\n",
      "epoch:2,batch:1248,lr:0.00064,loss:0.4429,mean_loss:0.449,train_f1:0.798,valid_f1:0.786: 100%|▉| 1249/1251 [00:56<00:00\n",
      "epoch:3,batch:1248,lr:0.00051,loss:0.309,mean_loss:0.395,train_f1:0.827,valid_f1:0.8: 100%|▉| 1249/1251 [00:56<00:00, 2\n",
      "epoch:4,batch:1248,lr:0.00041,loss:0.4139,mean_loss:0.361,train_f1:0.845,valid_f1:0.81: 100%|▉| 1249/1251 [00:56<00:00,\n",
      "epoch:5,batch:1248,lr:0.00033,loss:0.2833,mean_loss:0.335,train_f1:0.856,valid_f1:0.816: 100%|▉| 1249/1251 [00:57<00:00\n",
      "epoch:6,batch:1248,lr:0.00026,loss:0.3046,mean_loss:0.316,train_f1:0.866,valid_f1:0.816: 100%|▉| 1249/1251 [00:56<00:00\n",
      "epoch:7,batch:1248,lr:0.00021,loss:0.2363,mean_loss:0.301,train_f1:0.873,valid_f1:0.819: 100%|▉| 1249/1251 [00:56<00:00\n",
      "epoch:8,batch:1248,lr:0.00017,loss:0.2763,mean_loss:0.29,train_f1:0.876,valid_f1:0.822: 100%|▉| 1249/1251 [00:56<00:00,\n",
      "epoch:9,batch:1248,lr:0.00013,loss:0.2197,mean_loss:0.281,train_f1:0.883,valid_f1:0.819: 100%|▉| 1249/1251 [00:56<00:00\n",
      "epoch:10,batch:1248,lr:0.001,loss:0.3461,mean_loss:0.319,train_f1:0.863,valid_f1:0.818: 100%|▉| 1249/1251 [00:56<00:00,\n",
      "epoch:11,batch:1248,lr:0.0008,loss:0.2459,mean_loss:0.28,train_f1:0.882,valid_f1:0.821: 100%|▉| 1249/1251 [00:56<00:00,\n",
      "epoch:12,batch:1248,lr:0.00064,loss:0.3004,mean_loss:0.25,train_f1:0.894,valid_f1:0.825: 100%|▉| 1249/1251 [00:56<00:00\n",
      "epoch:13,batch:1248,lr:0.00051,loss:0.1934,mean_loss:0.226,train_f1:0.907,valid_f1:0.828: 100%|▉| 1249/1251 [00:56<00:0\n",
      "epoch:14,batch:1248,lr:0.00041,loss:0.2094,mean_loss:0.207,train_f1:0.914,valid_f1:0.827: 100%|▉| 1249/1251 [00:56<00:0\n",
      "epoch:15,batch:1248,lr:0.00033,loss:0.1384,mean_loss:0.191,train_f1:0.921,valid_f1:0.828: 100%|▉| 1249/1251 [00:56<00:0\n",
      "epoch:16,batch:1248,lr:0.00026,loss:0.1329,mean_loss:0.179,train_f1:0.928,valid_f1:0.826: 100%|▉| 1249/1251 [00:56<00:0\n",
      "epoch:17,batch:1248,lr:0.00021,loss:0.1566,mean_loss:0.169,train_f1:0.933,valid_f1:0.825: 100%|▉| 1249/1251 [00:56<00:0\n",
      "epoch:18,batch:1248,lr:0.00017,loss:0.2512,mean_loss:0.161,train_f1:0.936,valid_f1:0.824: 100%|▉| 1249/1251 [00:56<00:0\n",
      "epoch:19,batch:1248,lr:0.00013,loss:0.0543,mean_loss:0.155,train_f1:0.939,valid_f1:0.822: 100%|▉| 1249/1251 [00:56<00:0\n",
      "epoch:20,batch:1248,lr:0.001,loss:0.3504,mean_loss:0.212,train_f1:0.91,valid_f1:0.82: 100%|▉| 1249/1251 [00:56<00:00, 2\n",
      "epoch:21,batch:1248,lr:0.0008,loss:0.0871,mean_loss:0.178,train_f1:0.924,valid_f1:0.823: 100%|▉| 1249/1251 [00:56<00:00\n",
      "epoch:22,batch:1248,lr:0.00064,loss:0.3437,mean_loss:0.153,train_f1:0.937,valid_f1:0.821: 100%|▉| 1249/1251 [00:56<00:0\n",
      "epoch:23,batch:1248,lr:0.00051,loss:0.0823,mean_loss:0.135,train_f1:0.946,valid_f1:0.821: 100%|▉| 1249/1251 [00:56<00:0\n",
      "epoch:24,batch:1248,lr:0.00041,loss:0.0896,mean_loss:0.12,train_f1:0.955,valid_f1:0.822: 100%|▉| 1249/1251 [00:56<00:00\n",
      "epoch:25,batch:1248,lr:0.00033,loss:0.0852,mean_loss:0.108,train_f1:0.96,valid_f1:0.819: 100%|▉| 1249/1251 [00:56<00:00\n",
      "epoch:26,batch:1248,lr:0.00026,loss:0.1521,mean_loss:0.099,train_f1:0.965,valid_f1:0.817: 100%|▉| 1249/1251 [00:56<00:0\n",
      "epoch:27,batch:1248,lr:0.00021,loss:0.0617,mean_loss:0.091,train_f1:0.968,valid_f1:0.819: 100%|▉| 1249/1251 [00:56<00:0\n",
      "epoch:28,batch:1248,lr:0.00017,loss:0.1332,mean_loss:0.085,train_f1:0.971,valid_f1:0.817: 100%|▉| 1249/1251 [00:56<00:0\n",
      "epoch:29,batch:1248,lr:0.00013,loss:0.0972,mean_loss:0.08,train_f1:0.973,valid_f1:0.815: 100%|▉| 1249/1251 [00:56<00:00\n",
      "epoch:30,batch:1248,lr:0.001,loss:0.2056,mean_loss:0.16,train_f1:0.929,valid_f1:0.815: 100%|▉| 1249/1251 [00:56<00:00, \n",
      "epoch:31,batch:1248,lr:0.0008,loss:0.1202,mean_loss:0.118,train_f1:0.95,valid_f1:0.815: 100%|▉| 1249/1251 [00:56<00:00,\n",
      "epoch:32,batch:1248,lr:0.00064,loss:0.133,mean_loss:0.094,train_f1:0.962,valid_f1:0.815: 100%|▉| 1249/1251 [00:56<00:00\n",
      "epoch:33,batch:1248,lr:0.00051,loss:0.0348,mean_loss:0.079,train_f1:0.972,valid_f1:0.817: 100%|▉| 1249/1251 [00:56<00:0\n",
      "epoch:34,batch:1248,lr:0.00041,loss:0.0261,mean_loss:0.068,train_f1:0.975,valid_f1:0.814: 100%|▉| 1249/1251 [00:56<00:0\n",
      "epoch:35,batch:1248,lr:0.00033,loss:0.0547,mean_loss:0.059,train_f1:0.981,valid_f1:0.817: 100%|▉| 1249/1251 [00:56<00:0\n",
      "epoch:36,batch:1248,lr:0.00026,loss:0.0699,mean_loss:0.052,train_f1:0.984,valid_f1:0.81: 100%|▉| 1249/1251 [00:56<00:00\n",
      "epoch:37,batch:1248,lr:0.00021,loss:0.0282,mean_loss:0.047,train_f1:0.986,valid_f1:0.813: 100%|▉| 1249/1251 [00:56<00:0\n",
      "epoch:38,batch:1248,lr:0.00017,loss:0.0443,mean_loss:0.043,train_f1:0.988,valid_f1:0.814: 100%|▉| 1249/1251 [00:56<00:0\n",
      "epoch:39,batch:1248,lr:0.00013,loss:0.0596,mean_loss:0.04,train_f1:0.989,valid_f1:0.814: 100%|▉| 1249/1251 [00:56<00:00\n",
      "epoch:40,batch:1248,lr:0.001,loss:0.1266,mean_loss:0.146,train_f1:0.934,valid_f1:0.806: 100%|▉| 1249/1251 [00:56<00:00,\n",
      "epoch:41,batch:1248,lr:0.0008,loss:0.0898,mean_loss:0.081,train_f1:0.966,valid_f1:0.813: 100%|▉| 1249/1251 [00:56<00:00\n",
      "epoch:42,batch:1248,lr:0.00064,loss:0.0385,mean_loss:0.059,train_f1:0.979,valid_f1:0.814: 100%|▉| 1249/1251 [00:56<00:0\n",
      "epoch:43,batch:1248,lr:0.00051,loss:0.0285,mean_loss:0.047,train_f1:0.984,valid_f1:0.812: 100%|▉| 1249/1251 [00:56<00:0\n",
      "epoch:44,batch:1248,lr:0.00041,loss:0.0263,mean_loss:0.039,train_f1:0.988,valid_f1:0.813: 100%|▉| 1249/1251 [00:56<00:0\n",
      "epoch:45,batch:1248,lr:0.00033,loss:0.0282,mean_loss:0.033,train_f1:0.99,valid_f1:0.812: 100%|▉| 1249/1251 [00:56<00:00\n",
      "epoch:46,batch:1248,lr:0.00026,loss:0.0103,mean_loss:0.029,train_f1:0.992,valid_f1:0.811: 100%|▉| 1249/1251 [00:57<00:0\n",
      "epoch:47,batch:1248,lr:0.00021,loss:0.0123,mean_loss:0.024,train_f1:0.994,valid_f1:0.809: 100%|▉| 1249/1251 [00:57<00:0\n",
      "epoch:48,batch:1248,lr:0.00017,loss:0.0086,mean_loss:0.022,train_f1:0.995,valid_f1:0.812: 100%|▉| 1249/1251 [00:57<00:0\n",
      "epoch:49,batch:1248,lr:0.00013,loss:0.1231,mean_loss:0.02,train_f1:0.995,valid_f1:0.809: 100%|▉| 1249/1251 [00:57<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word num: 30\n",
      "random seed:1\n",
      "train device:cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:0,batch:1248,lr:0.001,loss:0.5128,mean_loss:0.916,train_f1:0.528,valid_f1:0.566: 100%|▉| 1249/1251 [01:00<00:00, \n",
      "epoch:1,batch:1248,lr:0.0008,loss:0.4261,mean_loss:0.437,train_f1:0.78,valid_f1:0.792: 100%|▉| 1249/1251 [01:00<00:00, \n",
      "epoch:2,batch:1248,lr:0.00064,loss:0.3267,mean_loss:0.346,train_f1:0.835,valid_f1:0.827: 100%|▉| 1249/1251 [00:59<00:00\n",
      "epoch:3,batch:1248,lr:0.00051,loss:0.1965,mean_loss:0.3,train_f1:0.862,valid_f1:0.843: 100%|▉| 1249/1251 [01:00<00:00, \n",
      "epoch:4,batch:1248,lr:0.00041,loss:0.3427,mean_loss:0.27,train_f1:0.878,valid_f1:0.85: 100%|▉| 1249/1251 [01:00<00:00, \n",
      "epoch:5,batch:1248,lr:0.00033,loss:0.1555,mean_loss:0.247,train_f1:0.888,valid_f1:0.853: 100%|▉| 1249/1251 [01:00<00:00\n",
      "epoch:6,batch:1248,lr:0.00026,loss:0.2605,mean_loss:0.23,train_f1:0.897,valid_f1:0.853: 100%|▉| 1249/1251 [01:00<00:00,\n",
      "epoch:7,batch:1248,lr:0.00021,loss:0.143,mean_loss:0.218,train_f1:0.904,valid_f1:0.854: 100%|▉| 1249/1251 [01:00<00:00,\n",
      "epoch:8,batch:1248,lr:0.00017,loss:0.1723,mean_loss:0.207,train_f1:0.908,valid_f1:0.856: 100%|▉| 1249/1251 [01:00<00:00\n",
      "epoch:9,batch:1248,lr:0.00013,loss:0.1152,mean_loss:0.199,train_f1:0.913,valid_f1:0.857: 100%|▉| 1249/1251 [01:00<00:00\n",
      "epoch:10,batch:1248,lr:0.001,loss:0.2873,mean_loss:0.235,train_f1:0.893,valid_f1:0.854: 100%|▉| 1249/1251 [01:00<00:00,\n",
      "epoch:11,batch:1248,lr:0.0008,loss:0.1655,mean_loss:0.201,train_f1:0.912,valid_f1:0.859: 100%|▉| 1249/1251 [01:00<00:00\n",
      "epoch:12,batch:1248,lr:0.00064,loss:0.2816,mean_loss:0.176,train_f1:0.923,valid_f1:0.862: 100%|▉| 1249/1251 [01:00<00:0\n",
      "epoch:13,batch:1248,lr:0.00051,loss:0.1346,mean_loss:0.156,train_f1:0.932,valid_f1:0.863: 100%|▉| 1249/1251 [01:00<00:0\n",
      "epoch:14,batch:1248,lr:0.00041,loss:0.1652,mean_loss:0.139,train_f1:0.941,valid_f1:0.863: 100%|▉| 1249/1251 [01:00<00:0\n",
      "epoch:15,batch:1248,lr:0.00033,loss:0.0662,mean_loss:0.126,train_f1:0.948,valid_f1:0.863: 100%|▉| 1249/1251 [01:00<00:0\n",
      "epoch:16,batch:1248,lr:0.00026,loss:0.0816,mean_loss:0.116,train_f1:0.954,valid_f1:0.858: 100%|▉| 1249/1251 [00:59<00:0\n",
      "epoch:17,batch:1248,lr:0.00021,loss:0.1494,mean_loss:0.107,train_f1:0.957,valid_f1:0.86: 100%|▉| 1249/1251 [01:00<00:00\n",
      "epoch:18,batch:1248,lr:0.00017,loss:0.0982,mean_loss:0.1,train_f1:0.96,valid_f1:0.859: 100%|▉| 1249/1251 [01:00<00:00, \n",
      "epoch:19,batch:1248,lr:0.00013,loss:0.0358,mean_loss:0.095,train_f1:0.964,valid_f1:0.859: 100%|▉| 1249/1251 [01:00<00:0\n",
      "epoch:20,batch:1248,lr:0.001,loss:0.1397,mean_loss:0.15,train_f1:0.932,valid_f1:0.855: 100%|▉| 1249/1251 [01:00<00:00, \n",
      "epoch:21,batch:1248,lr:0.0008,loss:0.1089,mean_loss:0.119,train_f1:0.949,valid_f1:0.857: 100%|▉| 1249/1251 [01:00<00:00\n",
      "epoch:22,batch:1248,lr:0.00064,loss:0.1696,mean_loss:0.098,train_f1:0.959,valid_f1:0.856: 100%|▉| 1249/1251 [01:00<00:0\n",
      "epoch:23,batch:1248,lr:0.00051,loss:0.0563,mean_loss:0.083,train_f1:0.966,valid_f1:0.857: 100%|▉| 1249/1251 [01:00<00:0\n",
      "epoch:24,batch:1248,lr:0.00041,loss:0.0675,mean_loss:0.072,train_f1:0.972,valid_f1:0.857: 100%|▉| 1249/1251 [01:00<00:0\n",
      "epoch:25,batch:1248,lr:0.00033,loss:0.0425,mean_loss:0.062,train_f1:0.978,valid_f1:0.854: 100%|▉| 1249/1251 [00:59<00:0\n",
      "epoch:26,batch:1248,lr:0.00026,loss:0.1196,mean_loss:0.055,train_f1:0.981,valid_f1:0.855: 100%|▉| 1249/1251 [01:00<00:0\n",
      "epoch:27,batch:1248,lr:0.00021,loss:0.05,mean_loss:0.05,train_f1:0.984,valid_f1:0.855: 100%|▉| 1249/1251 [01:00<00:00, \n",
      "epoch:28,batch:1248,lr:0.00017,loss:0.0354,mean_loss:0.045,train_f1:0.986,valid_f1:0.853: 100%|▉| 1249/1251 [01:00<00:0\n",
      "epoch:29,batch:1248,lr:0.00013,loss:0.0453,mean_loss:0.041,train_f1:0.987,valid_f1:0.85: 100%|▉| 1249/1251 [00:59<00:00\n",
      "epoch:30,batch:1248,lr:0.001,loss:0.1853,mean_loss:0.12,train_f1:0.946,valid_f1:0.849: 100%|▉| 1249/1251 [01:00<00:00, \n",
      "epoch:31,batch:1248,lr:0.0008,loss:0.0445,mean_loss:0.076,train_f1:0.966,valid_f1:0.852: 100%|▉| 1249/1251 [01:00<00:00\n",
      "epoch:32,batch:1248,lr:0.00064,loss:0.1568,mean_loss:0.058,train_f1:0.977,valid_f1:0.853: 100%|▉| 1249/1251 [01:00<00:0\n",
      "epoch:33,batch:1248,lr:0.00051,loss:0.0539,mean_loss:0.046,train_f1:0.983,valid_f1:0.853: 100%|▉| 1249/1251 [01:00<00:0\n",
      "epoch:34,batch:1248,lr:0.00041,loss:0.0291,mean_loss:0.037,train_f1:0.988,valid_f1:0.849: 100%|▉| 1249/1251 [01:00<00:0\n",
      "epoch:35,batch:1248,lr:0.00033,loss:0.0198,mean_loss:0.032,train_f1:0.99,valid_f1:0.85: 100%|▉| 1249/1251 [01:00<00:00,\n",
      "epoch:36,batch:1248,lr:0.00026,loss:0.0128,mean_loss:0.027,train_f1:0.993,valid_f1:0.852: 100%|▉| 1249/1251 [01:00<00:0\n",
      "epoch:37,batch:1248,lr:0.00021,loss:0.0221,mean_loss:0.024,train_f1:0.994,valid_f1:0.849: 100%|▉| 1249/1251 [01:00<00:0\n",
      "epoch:38,batch:1248,lr:0.00017,loss:0.0049,mean_loss:0.021,train_f1:0.995,valid_f1:0.847: 100%|▉| 1249/1251 [00:59<00:0\n",
      "epoch:39,batch:1248,lr:0.00013,loss:0.0334,mean_loss:0.019,train_f1:0.996,valid_f1:0.849: 100%|▉| 1249/1251 [01:00<00:0\n",
      "epoch:40,batch:1248,lr:0.001,loss:0.1463,mean_loss:0.112,train_f1:0.95,valid_f1:0.844: 100%|▉| 1249/1251 [01:00<00:00, \n",
      "epoch:41,batch:1248,lr:0.0008,loss:0.0342,mean_loss:0.054,train_f1:0.977,valid_f1:0.851: 100%|▉| 1249/1251 [01:00<00:00\n",
      "epoch:42,batch:1248,lr:0.00064,loss:0.007,mean_loss:0.037,train_f1:0.985,valid_f1:0.849: 100%|▉| 1249/1251 [00:59<00:00\n",
      "epoch:43,batch:1248,lr:0.00051,loss:0.0221,mean_loss:0.027,train_f1:0.991,valid_f1:0.848: 100%|▉| 1249/1251 [01:00<00:0\n",
      "epoch:44,batch:1248,lr:0.00041,loss:0.0124,mean_loss:0.022,train_f1:0.993,valid_f1:0.849: 100%|▉| 1249/1251 [01:00<00:0\n",
      "epoch:45,batch:1248,lr:0.00033,loss:0.0172,mean_loss:0.017,train_f1:0.996,valid_f1:0.849: 100%|▉| 1249/1251 [01:00<00:0\n",
      "epoch:46,batch:1248,lr:0.00026,loss:0.0016,mean_loss:0.014,train_f1:0.997,valid_f1:0.847: 100%|▉| 1249/1251 [01:00<00:0\n",
      "epoch:47,batch:1248,lr:0.00021,loss:0.0053,mean_loss:0.012,train_f1:0.997,valid_f1:0.847: 100%|▉| 1249/1251 [01:00<00:0\n",
      "epoch:48,batch:1248,lr:0.00017,loss:0.019,mean_loss:0.01,train_f1:0.998,valid_f1:0.846: 100%|▉| 1249/1251 [01:00<00:00,\n",
      "epoch:49,batch:1248,lr:0.00013,loss:0.0043,mean_loss:0.009,train_f1:0.998,valid_f1:0.848: 100%|▉| 1249/1251 [00:59<00:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word num: 40\n",
      "random seed:1\n",
      "train device:cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:0,batch:1248,lr:0.001,loss:0.4599,mean_loss:0.957,train_f1:0.513,valid_f1:0.555: 100%|▉| 1249/1251 [01:02<00:00, \n",
      "epoch:1,batch:1248,lr:0.0008,loss:0.3588,mean_loss:0.423,train_f1:0.779,valid_f1:0.797: 100%|▉| 1249/1251 [01:02<00:00,\n",
      "epoch:2,batch:1248,lr:0.00064,loss:0.3769,mean_loss:0.34,train_f1:0.833,valid_f1:0.832: 100%|▉| 1249/1251 [01:03<00:00,\n",
      "epoch:3,batch:1248,lr:0.00051,loss:0.2402,mean_loss:0.295,train_f1:0.86,valid_f1:0.844: 100%|▉| 1249/1251 [01:02<00:00,\n",
      "epoch:4,batch:1248,lr:0.00041,loss:0.3819,mean_loss:0.266,train_f1:0.876,valid_f1:0.852: 100%|▉| 1249/1251 [01:02<00:00\n",
      "epoch:5,batch:1248,lr:0.00033,loss:0.1234,mean_loss:0.244,train_f1:0.889,valid_f1:0.856: 100%|▉| 1249/1251 [01:02<00:00\n",
      "epoch:6,batch:1248,lr:0.00026,loss:0.257,mean_loss:0.227,train_f1:0.899,valid_f1:0.856: 100%|▉| 1249/1251 [01:02<00:00,\n",
      "epoch:7,batch:1248,lr:0.00021,loss:0.1587,mean_loss:0.215,train_f1:0.906,valid_f1:0.859: 100%|▉| 1249/1251 [01:03<00:00\n",
      "epoch:8,batch:1248,lr:0.00017,loss:0.2003,mean_loss:0.205,train_f1:0.909,valid_f1:0.858: 100%|▉| 1249/1251 [01:02<00:00\n",
      "epoch:9,batch:1248,lr:0.00013,loss:0.1856,mean_loss:0.197,train_f1:0.914,valid_f1:0.859: 100%|▉| 1249/1251 [01:02<00:00\n",
      "epoch:10,batch:1248,lr:0.001,loss:0.2718,mean_loss:0.232,train_f1:0.894,valid_f1:0.857: 100%|▉| 1249/1251 [01:03<00:00,\n",
      "epoch:11,batch:1248,lr:0.0008,loss:0.0814,mean_loss:0.201,train_f1:0.911,valid_f1:0.86: 100%|▉| 1249/1251 [01:02<00:00,\n",
      "epoch:12,batch:1248,lr:0.00064,loss:0.1944,mean_loss:0.176,train_f1:0.921,valid_f1:0.859: 100%|▉| 1249/1251 [01:02<00:0\n",
      "epoch:13,batch:1248,lr:0.00051,loss:0.1323,mean_loss:0.157,train_f1:0.934,valid_f1:0.865: 100%|▉| 1249/1251 [01:02<00:0\n",
      "epoch:14,batch:1248,lr:0.00041,loss:0.2492,mean_loss:0.141,train_f1:0.942,valid_f1:0.864: 100%|▉| 1249/1251 [01:03<00:0\n",
      "epoch:15,batch:1248,lr:0.00033,loss:0.0633,mean_loss:0.128,train_f1:0.949,valid_f1:0.863: 100%|▉| 1249/1251 [01:02<00:0\n",
      "epoch:16,batch:1248,lr:0.00026,loss:0.144,mean_loss:0.118,train_f1:0.953,valid_f1:0.862: 100%|▉| 1249/1251 [01:02<00:00\n",
      "epoch:17,batch:1248,lr:0.00021,loss:0.1488,mean_loss:0.109,train_f1:0.958,valid_f1:0.862: 100%|▉| 1249/1251 [01:02<00:0\n",
      "epoch:18,batch:1248,lr:0.00017,loss:0.1287,mean_loss:0.103,train_f1:0.961,valid_f1:0.859: 100%|▉| 1249/1251 [01:02<00:0\n",
      "epoch:19,batch:1248,lr:0.00013,loss:0.0218,mean_loss:0.097,train_f1:0.964,valid_f1:0.858: 100%|▉| 1249/1251 [01:02<00:0\n",
      "epoch:20,batch:1248,lr:0.001,loss:0.0731,mean_loss:0.154,train_f1:0.931,valid_f1:0.856: 100%|▉| 1249/1251 [01:02<00:00,\n",
      "epoch:21,batch:1248,lr:0.0008,loss:0.0738,mean_loss:0.123,train_f1:0.948,valid_f1:0.86: 100%|▉| 1249/1251 [01:02<00:00,\n",
      "epoch:22,batch:1248,lr:0.00064,loss:0.0884,mean_loss:0.103,train_f1:0.958,valid_f1:0.859: 100%|▉| 1249/1251 [01:09<00:0\n",
      "epoch:23,batch:1248,lr:0.00051,loss:0.1199,mean_loss:0.088,train_f1:0.966,valid_f1:0.858: 100%|▉| 1249/1251 [01:09<00:0\n",
      "epoch:24,batch:1248,lr:0.00041,loss:0.0553,mean_loss:0.077,train_f1:0.971,valid_f1:0.859: 100%|▉| 1249/1251 [01:09<00:0\n",
      "epoch:25,batch:1248,lr:0.00033,loss:0.0989,mean_loss:0.067,train_f1:0.976,valid_f1:0.855: 100%|▉| 1249/1251 [01:09<00:0\n",
      "epoch:26,batch:1248,lr:0.00026,loss:0.0987,mean_loss:0.061,train_f1:0.98,valid_f1:0.854: 100%|▉| 1249/1251 [01:09<00:00\n",
      "epoch:27,batch:1248,lr:0.00021,loss:0.0459,mean_loss:0.055,train_f1:0.982,valid_f1:0.855: 100%|▉| 1249/1251 [01:09<00:0\n",
      "epoch:28,batch:1248,lr:0.00017,loss:0.077,mean_loss:0.05,train_f1:0.984,valid_f1:0.853: 100%|▉| 1249/1251 [01:09<00:00,\n",
      "epoch:29,batch:1248,lr:0.00013,loss:0.0108,mean_loss:0.046,train_f1:0.986,valid_f1:0.851: 100%|▉| 1249/1251 [01:09<00:0\n",
      "epoch:30,batch:1248,lr:0.001,loss:0.1745,mean_loss:0.123,train_f1:0.944,valid_f1:0.853: 100%|▉| 1249/1251 [01:09<00:00,\n",
      "epoch:31,batch:1248,lr:0.0008,loss:0.0961,mean_loss:0.08,train_f1:0.966,valid_f1:0.854: 100%|▉| 1249/1251 [01:09<00:00,\n",
      "epoch:32,batch:1248,lr:0.00064,loss:0.1551,mean_loss:0.063,train_f1:0.976,valid_f1:0.854: 100%|▉| 1249/1251 [01:09<00:0\n",
      "epoch:33,batch:1248,lr:0.00051,loss:0.0259,mean_loss:0.053,train_f1:0.982,valid_f1:0.855: 100%|▉| 1249/1251 [01:09<00:0\n",
      "epoch:34,batch:1248,lr:0.00041,loss:0.0201,mean_loss:0.044,train_f1:0.985,valid_f1:0.851: 100%|▉| 1249/1251 [01:09<00:0\n",
      "epoch:35,batch:1248,lr:0.00033,loss:0.0333,mean_loss:0.038,train_f1:0.988,valid_f1:0.853: 100%|▉| 1249/1251 [01:09<00:0\n",
      "epoch:36,batch:1248,lr:0.00026,loss:0.0135,mean_loss:0.033,train_f1:0.99,valid_f1:0.85: 100%|▉| 1249/1251 [01:09<00:00,\n",
      "epoch:37,batch:1248,lr:0.00021,loss:0.0161,mean_loss:0.029,train_f1:0.992,valid_f1:0.851: 100%|▉| 1249/1251 [01:09<00:0\n",
      "epoch:38,batch:1248,lr:0.00017,loss:0.0435,mean_loss:0.026,train_f1:0.993,valid_f1:0.85: 100%|▉| 1249/1251 [01:09<00:00\n",
      "epoch:39,batch:1248,lr:0.00013,loss:0.0124,mean_loss:0.023,train_f1:0.994,valid_f1:0.85: 100%|▉| 1249/1251 [01:09<00:00\n",
      "epoch:40,batch:1248,lr:0.001,loss:0.1128,mean_loss:0.109,train_f1:0.949,valid_f1:0.849: 100%|▉| 1249/1251 [01:09<00:00,\n",
      "epoch:41,batch:1248,lr:0.0008,loss:0.0378,mean_loss:0.059,train_f1:0.976,valid_f1:0.852: 100%|▉| 1249/1251 [01:09<00:00\n",
      "epoch:42,batch:1248,lr:0.00064,loss:0.0364,mean_loss:0.042,train_f1:0.985,valid_f1:0.853: 100%|▉| 1249/1251 [01:09<00:0\n",
      "epoch:43,batch:1248,lr:0.00051,loss:0.0658,mean_loss:0.034,train_f1:0.988,valid_f1:0.85: 100%|▉| 1249/1251 [01:09<00:00\n",
      "epoch:44,batch:1248,lr:0.00041,loss:0.0076,mean_loss:0.028,train_f1:0.991,valid_f1:0.85: 100%|▉| 1249/1251 [01:09<00:00\n",
      "epoch:45,batch:1248,lr:0.00033,loss:0.0056,mean_loss:0.022,train_f1:0.994,valid_f1:0.853: 100%|▉| 1249/1251 [01:09<00:0\n",
      "epoch:46,batch:1248,lr:0.00026,loss:0.0075,mean_loss:0.019,train_f1:0.995,valid_f1:0.85: 100%|▉| 1249/1251 [01:09<00:00\n",
      "epoch:47,batch:1248,lr:0.00021,loss:0.0055,mean_loss:0.016,train_f1:0.996,valid_f1:0.848: 100%|▉| 1249/1251 [01:09<00:0\n",
      "epoch:48,batch:1248,lr:0.00017,loss:0.0164,mean_loss:0.014,train_f1:0.996,valid_f1:0.849: 100%|▉| 1249/1251 [01:09<00:0\n",
      "epoch:49,batch:1248,lr:0.00013,loss:0.0043,mean_loss:0.012,train_f1:0.997,valid_f1:0.85: 100%|▉| 1249/1251 [01:09<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word num: 50\n",
      "random seed:1\n",
      "train device:cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:0,batch:1248,lr:0.001,loss:0.5032,mean_loss:0.988,train_f1:0.49,valid_f1:0.534: 100%|▉| 1249/1251 [01:06<00:00, 1\n",
      "epoch:1,batch:1248,lr:0.0008,loss:0.2788,mean_loss:0.417,train_f1:0.777,valid_f1:0.798: 100%|▉| 1249/1251 [01:07<00:00,\n",
      "epoch:2,batch:1248,lr:0.00064,loss:0.2759,mean_loss:0.328,train_f1:0.84,valid_f1:0.839: 100%|▉| 1249/1251 [01:06<00:00,\n",
      "epoch:3,batch:1248,lr:0.00051,loss:0.1784,mean_loss:0.284,train_f1:0.867,valid_f1:0.853: 100%|▉| 1249/1251 [01:07<00:00\n",
      "epoch:4,batch:1248,lr:0.00041,loss:0.3331,mean_loss:0.255,train_f1:0.885,valid_f1:0.862: 100%|▉| 1249/1251 [01:06<00:00\n",
      "epoch:5,batch:1248,lr:0.00033,loss:0.1801,mean_loss:0.234,train_f1:0.894,valid_f1:0.866: 100%|▉| 1249/1251 [01:06<00:00\n",
      "epoch:6,batch:1248,lr:0.00026,loss:0.2821,mean_loss:0.219,train_f1:0.904,valid_f1:0.866: 100%|▉| 1249/1251 [01:06<00:00\n",
      "epoch:7,batch:1248,lr:0.00021,loss:0.1501,mean_loss:0.206,train_f1:0.91,valid_f1:0.869: 100%|▉| 1249/1251 [01:05<00:00,\n",
      "epoch:8,batch:1248,lr:0.00017,loss:0.2696,mean_loss:0.197,train_f1:0.913,valid_f1:0.869: 100%|▉| 1249/1251 [01:05<00:00\n",
      "epoch:9,batch:1248,lr:0.00013,loss:0.1725,mean_loss:0.19,train_f1:0.919,valid_f1:0.869: 100%|▉| 1249/1251 [01:06<00:00,\n",
      "epoch:10,batch:1248,lr:0.001,loss:0.3335,mean_loss:0.222,train_f1:0.901,valid_f1:0.865: 100%|▉| 1249/1251 [01:05<00:00,\n",
      "epoch:11,batch:1248,lr:0.0008,loss:0.1011,mean_loss:0.192,train_f1:0.915,valid_f1:0.87: 100%|▉| 1249/1251 [01:06<00:00,\n",
      "epoch:12,batch:1248,lr:0.00064,loss:0.2316,mean_loss:0.168,train_f1:0.928,valid_f1:0.869: 100%|▉| 1249/1251 [01:06<00:0\n",
      "epoch:13,batch:1248,lr:0.00051,loss:0.1131,mean_loss:0.149,train_f1:0.936,valid_f1:0.872: 100%|▉| 1249/1251 [01:06<00:0\n",
      "epoch:14,batch:1248,lr:0.00041,loss:0.2021,mean_loss:0.134,train_f1:0.945,valid_f1:0.872: 100%|▉| 1249/1251 [01:05<00:0\n",
      "epoch:15,batch:1248,lr:0.00033,loss:0.0711,mean_loss:0.121,train_f1:0.951,valid_f1:0.869: 100%|▉| 1249/1251 [01:05<00:0\n",
      "epoch:16,batch:1248,lr:0.00026,loss:0.1561,mean_loss:0.111,train_f1:0.956,valid_f1:0.869: 100%|▉| 1249/1251 [01:05<00:0\n",
      "epoch:17,batch:1248,lr:0.00021,loss:0.108,mean_loss:0.104,train_f1:0.96,valid_f1:0.868: 100%|▉| 1249/1251 [01:05<00:00,\n",
      "epoch:18,batch:1248,lr:0.00017,loss:0.1243,mean_loss:0.097,train_f1:0.963,valid_f1:0.867: 100%|▉| 1249/1251 [01:05<00:0\n",
      "epoch:19,batch:1248,lr:0.00013,loss:0.0896,mean_loss:0.092,train_f1:0.966,valid_f1:0.865: 100%|▉| 1249/1251 [01:06<00:0\n",
      "epoch:20,batch:1248,lr:0.001,loss:0.0934,mean_loss:0.145,train_f1:0.935,valid_f1:0.864: 100%|▉| 1249/1251 [01:06<00:00,\n",
      "epoch:21,batch:1248,lr:0.0008,loss:0.0853,mean_loss:0.115,train_f1:0.951,valid_f1:0.868: 100%|▉| 1249/1251 [01:06<00:00\n",
      "epoch:22,batch:1248,lr:0.00064,loss:0.0767,mean_loss:0.097,train_f1:0.961,valid_f1:0.869: 100%|▉| 1249/1251 [01:05<00:0\n",
      "epoch:23,batch:1248,lr:0.00051,loss:0.1046,mean_loss:0.083,train_f1:0.968,valid_f1:0.866: 100%|▉| 1249/1251 [01:05<00:0\n",
      "epoch:24,batch:1248,lr:0.00041,loss:0.0225,mean_loss:0.072,train_f1:0.974,valid_f1:0.865: 100%|▉| 1249/1251 [01:06<00:0\n",
      "epoch:25,batch:1248,lr:0.00033,loss:0.0761,mean_loss:0.063,train_f1:0.979,valid_f1:0.864: 100%|▉| 1249/1251 [01:05<00:0\n",
      "epoch:26,batch:1248,lr:0.00026,loss:0.108,mean_loss:0.057,train_f1:0.981,valid_f1:0.862: 100%|▉| 1249/1251 [01:05<00:00\n",
      "epoch:27,batch:1248,lr:0.00021,loss:0.0312,mean_loss:0.051,train_f1:0.984,valid_f1:0.862: 100%|▉| 1249/1251 [01:06<00:0\n",
      "epoch:28,batch:1248,lr:0.00017,loss:0.0465,mean_loss:0.046,train_f1:0.986,valid_f1:0.86: 100%|▉| 1249/1251 [01:06<00:00\n",
      "epoch:29,batch:1248,lr:0.00013,loss:0.0389,mean_loss:0.043,train_f1:0.987,valid_f1:0.858: 100%|▉| 1249/1251 [01:06<00:0\n",
      "epoch:30,batch:1248,lr:0.001,loss:0.1478,mean_loss:0.116,train_f1:0.947,valid_f1:0.857: 100%|▉| 1249/1251 [01:06<00:00,\n",
      "epoch:31,batch:1248,lr:0.0008,loss:0.148,mean_loss:0.076,train_f1:0.968,valid_f1:0.863: 100%|▉| 1249/1251 [01:05<00:00,\n",
      "epoch:32,batch:1248,lr:0.00064,loss:0.1133,mean_loss:0.058,train_f1:0.978,valid_f1:0.862: 100%|▉| 1249/1251 [01:05<00:0\n",
      "epoch:33,batch:1248,lr:0.00051,loss:0.0185,mean_loss:0.047,train_f1:0.983,valid_f1:0.86: 100%|▉| 1249/1251 [01:06<00:00\n",
      "epoch:34,batch:1248,lr:0.00041,loss:0.0331,mean_loss:0.04,train_f1:0.986,valid_f1:0.858: 100%|▉| 1249/1251 [01:06<00:00\n",
      "epoch:35,batch:1248,lr:0.00033,loss:0.0086,mean_loss:0.034,train_f1:0.989,valid_f1:0.862: 100%|▉| 1249/1251 [01:06<00:0\n",
      "epoch:36,batch:1248,lr:0.00026,loss:0.0089,mean_loss:0.029,train_f1:0.992,valid_f1:0.858: 100%|▉| 1249/1251 [01:05<00:0\n",
      "epoch:37,batch:1248,lr:0.00021,loss:0.0213,mean_loss:0.025,train_f1:0.993,valid_f1:0.858: 100%|▉| 1249/1251 [01:05<00:0\n",
      "epoch:38,batch:1248,lr:0.00017,loss:0.0446,mean_loss:0.023,train_f1:0.994,valid_f1:0.857: 100%|▉| 1249/1251 [01:06<00:0\n",
      "epoch:39,batch:1248,lr:0.00013,loss:0.0184,mean_loss:0.02,train_f1:0.995,valid_f1:0.857: 100%|▉| 1249/1251 [01:05<00:00\n",
      "epoch:40,batch:1248,lr:0.001,loss:0.0316,mean_loss:0.104,train_f1:0.952,valid_f1:0.854: 100%|▉| 1249/1251 [01:06<00:00,\n",
      "epoch:41,batch:1248,lr:0.0008,loss:0.0633,mean_loss:0.052,train_f1:0.978,valid_f1:0.86: 100%|▉| 1249/1251 [01:05<00:00,\n",
      "epoch:42,batch:1248,lr:0.00064,loss:0.0351,mean_loss:0.037,train_f1:0.987,valid_f1:0.859: 100%|▉| 1249/1251 [01:03<00:0\n",
      "epoch:43,batch:1248,lr:0.00051,loss:0.0106,mean_loss:0.029,train_f1:0.99,valid_f1:0.859: 100%|▉| 1249/1251 [01:05<00:00\n",
      "epoch:44,batch:1248,lr:0.00041,loss:0.0065,mean_loss:0.024,train_f1:0.992,valid_f1:0.857: 100%|▉| 1249/1251 [01:05<00:0\n",
      "epoch:45,batch:1248,lr:0.00033,loss:0.003,mean_loss:0.019,train_f1:0.995,valid_f1:0.86: 100%|▉| 1249/1251 [01:06<00:00,\n",
      "epoch:46,batch:1248,lr:0.00026,loss:0.0186,mean_loss:0.016,train_f1:0.996,valid_f1:0.856: 100%|▉| 1249/1251 [01:06<00:0\n",
      "epoch:47,batch:1248,lr:0.00021,loss:0.0084,mean_loss:0.014,train_f1:0.997,valid_f1:0.857: 100%|▉| 1249/1251 [01:05<00:0\n",
      "epoch:48,batch:1248,lr:0.00017,loss:0.0554,mean_loss:0.012,train_f1:0.997,valid_f1:0.857: 100%|▉| 1249/1251 [01:05<00:0\n",
      "epoch:49,batch:1248,lr:0.00013,loss:0.0069,mean_loss:0.01,train_f1:0.998,valid_f1:0.855: 100%|▉| 1249/1251 [01:06<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word num: 100\n",
      "random seed:1\n",
      "train device:cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:0,batch:1248,lr:0.001,loss:0.4708,mean_loss:1.019,train_f1:0.47,valid_f1:0.506: 100%|▉| 1249/1251 [00:54<00:00, 2\n",
      "epoch:1,batch:1248,lr:0.0008,loss:0.2799,mean_loss:0.379,train_f1:0.802,valid_f1:0.819: 100%|▉| 1249/1251 [01:06<00:00,\n",
      "epoch:2,batch:1248,lr:0.00064,loss:0.2497,mean_loss:0.296,train_f1:0.86,valid_f1:0.858: 100%|▉| 1249/1251 [01:07<00:00,\n",
      "epoch:3,batch:1248,lr:0.00051,loss:0.2341,mean_loss:0.256,train_f1:0.884,valid_f1:0.869: 100%|▉| 1249/1251 [01:06<00:00\n",
      "epoch:4,batch:1248,lr:0.00041,loss:0.2924,mean_loss:0.23,train_f1:0.898,valid_f1:0.877: 100%|▉| 1249/1251 [01:06<00:00,\n",
      "epoch:5,batch:1248,lr:0.00033,loss:0.1201,mean_loss:0.211,train_f1:0.908,valid_f1:0.88: 100%|▉| 1249/1251 [01:07<00:00,\n",
      "epoch:6,batch:1248,lr:0.00026,loss:0.2448,mean_loss:0.197,train_f1:0.915,valid_f1:0.88: 100%|▉| 1249/1251 [01:07<00:00,\n",
      "epoch:7,batch:1248,lr:0.00021,loss:0.1354,mean_loss:0.186,train_f1:0.92,valid_f1:0.883: 100%|▉| 1249/1251 [01:06<00:00,\n",
      "epoch:8,batch:1248,lr:0.00017,loss:0.2392,mean_loss:0.177,train_f1:0.924,valid_f1:0.883: 100%|▉| 1249/1251 [01:06<00:00\n",
      "epoch:9,batch:1248,lr:0.00013,loss:0.1767,mean_loss:0.17,train_f1:0.928,valid_f1:0.883: 100%|▉| 1249/1251 [01:06<00:00,\n",
      "epoch:10,batch:1248,lr:0.001,loss:0.2834,mean_loss:0.2,train_f1:0.911,valid_f1:0.881: 100%|▉| 1249/1251 [01:06<00:00, 1\n",
      "epoch:11,batch:1248,lr:0.0008,loss:0.0921,mean_loss:0.173,train_f1:0.925,valid_f1:0.884: 100%|▉| 1249/1251 [01:06<00:00\n",
      "epoch:12,batch:1248,lr:0.00064,loss:0.2384,mean_loss:0.151,train_f1:0.935,valid_f1:0.886: 100%|▉| 1249/1251 [01:07<00:0\n",
      "epoch:13,batch:1248,lr:0.00051,loss:0.1234,mean_loss:0.135,train_f1:0.944,valid_f1:0.887: 100%|▉| 1249/1251 [01:06<00:0\n",
      "epoch:14,batch:1248,lr:0.00041,loss:0.1248,mean_loss:0.122,train_f1:0.95,valid_f1:0.888: 100%|▉| 1249/1251 [01:06<00:00\n",
      "epoch:15,batch:1248,lr:0.00033,loss:0.0967,mean_loss:0.11,train_f1:0.956,valid_f1:0.885: 100%|▉| 1249/1251 [01:06<00:00\n",
      "epoch:16,batch:1248,lr:0.00026,loss:0.1181,mean_loss:0.101,train_f1:0.961,valid_f1:0.885: 100%|▉| 1249/1251 [01:07<00:0\n",
      "epoch:17,batch:1248,lr:0.00021,loss:0.1143,mean_loss:0.094,train_f1:0.965,valid_f1:0.884: 100%|▉| 1249/1251 [01:07<00:0\n",
      "epoch:18,batch:1248,lr:0.00017,loss:0.0348,mean_loss:0.088,train_f1:0.968,valid_f1:0.882: 100%|▉| 1249/1251 [01:07<00:0\n",
      "epoch:19,batch:1248,lr:0.00013,loss:0.0196,mean_loss:0.084,train_f1:0.969,valid_f1:0.88: 100%|▉| 1249/1251 [01:06<00:00\n",
      "epoch:20,batch:1248,lr:0.001,loss:0.0462,mean_loss:0.133,train_f1:0.941,valid_f1:0.882: 100%|▉| 1249/1251 [01:06<00:00,\n",
      "epoch:21,batch:1248,lr:0.0008,loss:0.1123,mean_loss:0.106,train_f1:0.956,valid_f1:0.883: 100%|▉| 1249/1251 [01:06<00:00\n",
      "epoch:22,batch:1248,lr:0.00064,loss:0.0695,mean_loss:0.089,train_f1:0.965,valid_f1:0.884: 100%|▉| 1249/1251 [01:06<00:0\n",
      "epoch:23,batch:1248,lr:0.00051,loss:0.0689,mean_loss:0.077,train_f1:0.971,valid_f1:0.881: 100%|▉| 1249/1251 [01:06<00:0\n",
      "epoch:24,batch:1248,lr:0.00041,loss:0.0301,mean_loss:0.068,train_f1:0.975,valid_f1:0.881: 100%|▉| 1249/1251 [01:06<00:0\n",
      "epoch:25,batch:1248,lr:0.00033,loss:0.0451,mean_loss:0.059,train_f1:0.98,valid_f1:0.879: 100%|▉| 1249/1251 [01:06<00:00\n",
      "epoch:26,batch:1248,lr:0.00026,loss:0.0904,mean_loss:0.053,train_f1:0.982,valid_f1:0.877: 100%|▉| 1249/1251 [01:07<00:0\n",
      "epoch:27,batch:1248,lr:0.00021,loss:0.0321,mean_loss:0.048,train_f1:0.985,valid_f1:0.877: 100%|▉| 1249/1251 [01:06<00:0\n",
      "epoch:28,batch:1248,lr:0.00017,loss:0.0715,mean_loss:0.044,train_f1:0.986,valid_f1:0.876: 100%|▉| 1249/1251 [01:06<00:0\n",
      "epoch:29,batch:1248,lr:0.00013,loss:0.0302,mean_loss:0.04,train_f1:0.988,valid_f1:0.873: 100%|▉| 1249/1251 [01:06<00:00\n",
      "epoch:30,batch:1248,lr:0.001,loss:0.1067,mean_loss:0.102,train_f1:0.954,valid_f1:0.874: 100%|▉| 1249/1251 [01:07<00:00,\n",
      "epoch:31,batch:1248,lr:0.0008,loss:0.1722,mean_loss:0.07,train_f1:0.971,valid_f1:0.875: 100%|▉| 1249/1251 [01:06<00:00,\n",
      "epoch:32,batch:1248,lr:0.00064,loss:0.1152,mean_loss:0.055,train_f1:0.979,valid_f1:0.879: 100%|▉| 1249/1251 [01:06<00:0\n",
      "epoch:33,batch:1248,lr:0.00051,loss:0.0452,mean_loss:0.044,train_f1:0.985,valid_f1:0.876: 100%|▉| 1249/1251 [01:06<00:0\n",
      "epoch:34,batch:1248,lr:0.00041,loss:0.0086,mean_loss:0.038,train_f1:0.987,valid_f1:0.875: 100%|▉| 1249/1251 [01:06<00:0\n",
      "epoch:35,batch:1248,lr:0.00033,loss:0.0207,mean_loss:0.031,train_f1:0.991,valid_f1:0.875: 100%|▉| 1249/1251 [01:06<00:0\n",
      "epoch:36,batch:1248,lr:0.00026,loss:0.0099,mean_loss:0.027,train_f1:0.992,valid_f1:0.875: 100%|▉| 1249/1251 [01:06<00:0\n",
      "epoch:37,batch:1248,lr:0.00021,loss:0.0308,mean_loss:0.024,train_f1:0.993,valid_f1:0.873: 100%|▉| 1249/1251 [01:06<00:0\n",
      "epoch:38,batch:1248,lr:0.00017,loss:0.0065,mean_loss:0.021,train_f1:0.994,valid_f1:0.871: 100%|▉| 1249/1251 [01:06<00:0\n",
      "epoch:39,batch:1248,lr:0.00013,loss:0.0405,mean_loss:0.019,train_f1:0.995,valid_f1:0.873: 100%|▉| 1249/1251 [01:06<00:0\n",
      "epoch:40,batch:1248,lr:0.001,loss:0.0691,mean_loss:0.096,train_f1:0.955,valid_f1:0.872: 100%|▉| 1249/1251 [01:07<00:00,\n",
      "epoch:41,batch:1248,lr:0.0008,loss:0.0139,mean_loss:0.05,train_f1:0.979,valid_f1:0.874: 100%|▉| 1249/1251 [01:07<00:00,\n",
      "epoch:42,batch:1248,lr:0.00064,loss:0.0204,mean_loss:0.036,train_f1:0.987,valid_f1:0.876: 100%|▉| 1249/1251 [01:07<00:0\n",
      "epoch:43,batch:1248,lr:0.00051,loss:0.0406,mean_loss:0.027,train_f1:0.991,valid_f1:0.872: 100%|▉| 1249/1251 [01:06<00:0\n",
      "epoch:44,batch:1248,lr:0.00041,loss:0.018,mean_loss:0.023,train_f1:0.993,valid_f1:0.872: 100%|▉| 1249/1251 [01:06<00:00\n",
      "epoch:45,batch:1248,lr:0.00033,loss:0.0123,mean_loss:0.019,train_f1:0.994,valid_f1:0.874: 100%|▉| 1249/1251 [01:07<00:0\n",
      "epoch:46,batch:1248,lr:0.00026,loss:0.004,mean_loss:0.016,train_f1:0.995,valid_f1:0.87: 100%|▉| 1249/1251 [01:06<00:00,\n",
      "epoch:47,batch:1248,lr:0.00021,loss:0.0022,mean_loss:0.013,train_f1:0.997,valid_f1:0.871: 100%|▉| 1249/1251 [01:06<00:0\n",
      "epoch:48,batch:1248,lr:0.00017,loss:0.0027,mean_loss:0.012,train_f1:0.997,valid_f1:0.871: 100%|▉| 1249/1251 [01:06<00:0\n",
      "epoch:49,batch:1248,lr:0.00013,loss:0.0052,mean_loss:0.01,train_f1:0.998,valid_f1:0.87: 100%|▉| 1249/1251 [01:06<00:00,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word num: 300\n",
      "random seed:1\n",
      "train device:cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:0,batch:1248,lr:0.001,loss:0.4394,mean_loss:1.103,train_f1:0.432,valid_f1:0.476: 100%|▉| 1249/1251 [01:05<00:00, \n",
      "epoch:1,batch:1248,lr:0.0008,loss:0.3127,mean_loss:0.36,train_f1:0.816,valid_f1:0.831: 100%|▉| 1249/1251 [01:09<00:00, \n",
      "epoch:2,batch:1248,lr:0.00064,loss:0.2718,mean_loss:0.277,train_f1:0.864,valid_f1:0.869: 100%|▉| 1249/1251 [01:16<00:00\n",
      "epoch:3,batch:1248,lr:0.00051,loss:0.2046,mean_loss:0.237,train_f1:0.889,valid_f1:0.881: 100%|▉| 1249/1251 [01:16<00:00\n",
      "epoch:4,batch:1248,lr:0.00041,loss:0.3445,mean_loss:0.213,train_f1:0.904,valid_f1:0.889: 100%|▉| 1249/1251 [01:15<00:00\n",
      "epoch:5,batch:1248,lr:0.00033,loss:0.1453,mean_loss:0.195,train_f1:0.913,valid_f1:0.893: 100%|▉| 1249/1251 [01:15<00:00\n",
      "epoch:6,batch:1248,lr:0.00026,loss:0.2266,mean_loss:0.182,train_f1:0.921,valid_f1:0.893: 100%|▉| 1249/1251 [01:15<00:00\n",
      "epoch:7,batch:1248,lr:0.00021,loss:0.1746,mean_loss:0.172,train_f1:0.927,valid_f1:0.896: 100%|▉| 1249/1251 [01:15<00:00\n",
      "epoch:8,batch:1248,lr:0.00017,loss:0.1499,mean_loss:0.164,train_f1:0.93,valid_f1:0.897: 100%|▉| 1249/1251 [01:15<00:00,\n",
      "epoch:9,batch:1248,lr:0.00013,loss:0.0936,mean_loss:0.158,train_f1:0.933,valid_f1:0.895: 100%|▉| 1249/1251 [01:15<00:00\n",
      "epoch:10,batch:1248,lr:0.001,loss:0.256,mean_loss:0.182,train_f1:0.919,valid_f1:0.894: 100%|▉| 1249/1251 [01:15<00:00, \n",
      "epoch:11,batch:1248,lr:0.0008,loss:0.1707,mean_loss:0.158,train_f1:0.932,valid_f1:0.898: 100%|▉| 1249/1251 [01:14<00:00\n",
      "epoch:12,batch:1248,lr:0.00064,loss:0.1804,mean_loss:0.139,train_f1:0.94,valid_f1:0.9: 100%|▉| 1249/1251 [01:15<00:00, \n",
      "epoch:13,batch:1248,lr:0.00051,loss:0.1009,mean_loss:0.123,train_f1:0.949,valid_f1:0.899: 100%|▉| 1249/1251 [01:15<00:0\n",
      "epoch:14,batch:1248,lr:0.00041,loss:0.2021,mean_loss:0.112,train_f1:0.955,valid_f1:0.9: 100%|▉| 1249/1251 [01:14<00:00,\n",
      "epoch:15,batch:1248,lr:0.00033,loss:0.1318,mean_loss:0.102,train_f1:0.959,valid_f1:0.899: 100%|▉| 1249/1251 [01:14<00:0\n",
      "epoch:16,batch:1248,lr:0.00026,loss:0.1785,mean_loss:0.094,train_f1:0.964,valid_f1:0.899: 100%|▉| 1249/1251 [01:13<00:0\n",
      "epoch:17,batch:1248,lr:0.00021,loss:0.0684,mean_loss:0.087,train_f1:0.967,valid_f1:0.9: 100%|▉| 1249/1251 [01:14<00:00,\n",
      "epoch:18,batch:1248,lr:0.00017,loss:0.0553,mean_loss:0.082,train_f1:0.97,valid_f1:0.898: 100%|▉| 1249/1251 [01:15<00:00\n",
      "epoch:19,batch:1248,lr:0.00013,loss:0.0162,mean_loss:0.078,train_f1:0.971,valid_f1:0.897: 100%|▉| 1249/1251 [01:14<00:0\n",
      "epoch:20,batch:1248,lr:0.001,loss:0.0357,mean_loss:0.119,train_f1:0.949,valid_f1:0.895: 100%|▉| 1249/1251 [01:15<00:00,\n",
      "epoch:21,batch:1248,lr:0.0008,loss:0.0958,mean_loss:0.097,train_f1:0.96,valid_f1:0.899: 100%|▉| 1249/1251 [01:15<00:00,\n",
      "epoch:22,batch:1248,lr:0.00064,loss:0.0461,mean_loss:0.082,train_f1:0.967,valid_f1:0.898: 100%|▉| 1249/1251 [01:14<00:0\n",
      "epoch:23,batch:1248,lr:0.00051,loss:0.0433,mean_loss:0.071,train_f1:0.972,valid_f1:0.897: 100%|▉| 1249/1251 [01:14<00:0\n",
      "epoch:24,batch:1248,lr:0.00041,loss:0.0439,mean_loss:0.062,train_f1:0.977,valid_f1:0.895: 100%|▉| 1249/1251 [01:15<00:0\n",
      "epoch:25,batch:1248,lr:0.00033,loss:0.0256,mean_loss:0.054,train_f1:0.981,valid_f1:0.895: 100%|▉| 1249/1251 [01:14<00:0\n",
      "epoch:26,batch:1248,lr:0.00026,loss:0.0571,mean_loss:0.048,train_f1:0.983,valid_f1:0.894: 100%|▉| 1249/1251 [01:15<00:0\n",
      "epoch:27,batch:1248,lr:0.00021,loss:0.0428,mean_loss:0.044,train_f1:0.986,valid_f1:0.891: 100%|▉| 1249/1251 [01:14<00:0\n",
      "epoch:28,batch:1248,lr:0.00017,loss:0.0266,mean_loss:0.04,train_f1:0.987,valid_f1:0.892: 100%|▉| 1249/1251 [01:14<00:00\n",
      "epoch:29,batch:1248,lr:0.00013,loss:0.0231,mean_loss:0.037,train_f1:0.988,valid_f1:0.891: 100%|▉| 1249/1251 [01:15<00:0\n",
      "epoch:30,batch:1248,lr:0.001,loss:0.1561,mean_loss:0.093,train_f1:0.957,valid_f1:0.887: 100%|▉| 1249/1251 [01:14<00:00,\n",
      "epoch:31,batch:1248,lr:0.0008,loss:0.1467,mean_loss:0.065,train_f1:0.973,valid_f1:0.892: 100%|▉| 1249/1251 [01:14<00:00\n",
      "epoch:32,batch:1248,lr:0.00064,loss:0.0568,mean_loss:0.051,train_f1:0.98,valid_f1:0.893: 100%|▉| 1249/1251 [01:15<00:00\n",
      "epoch:33,batch:1248,lr:0.00051,loss:0.0206,mean_loss:0.041,train_f1:0.985,valid_f1:0.893: 100%|▉| 1249/1251 [01:14<00:0\n",
      "epoch:34,batch:1248,lr:0.00041,loss:0.0098,mean_loss:0.035,train_f1:0.988,valid_f1:0.892: 100%|▉| 1249/1251 [01:14<00:0\n",
      "epoch:35,batch:1248,lr:0.00033,loss:0.0218,mean_loss:0.029,train_f1:0.991,valid_f1:0.891: 100%|▉| 1249/1251 [01:14<00:0\n",
      "epoch:36,batch:1248,lr:0.00026,loss:0.0113,mean_loss:0.026,train_f1:0.992,valid_f1:0.89: 100%|▉| 1249/1251 [01:12<00:00\n",
      "epoch:37,batch:1248,lr:0.00021,loss:0.0778,mean_loss:0.023,train_f1:0.993,valid_f1:0.891: 100%|▉| 1249/1251 [01:15<00:0\n",
      "epoch:38,batch:1248,lr:0.00017,loss:0.0063,mean_loss:0.021,train_f1:0.994,valid_f1:0.89: 100%|▉| 1249/1251 [01:16<00:00\n",
      "epoch:39,batch:1248,lr:0.00013,loss:0.0737,mean_loss:0.018,train_f1:0.995,valid_f1:0.891: 100%|▉| 1249/1251 [01:14<00:0\n",
      "epoch:40,batch:1248,lr:0.001,loss:0.0464,mean_loss:0.082,train_f1:0.961,valid_f1:0.886: 100%|▉| 1249/1251 [01:14<00:00,\n",
      "epoch:41,batch:1248,lr:0.0008,loss:0.0295,mean_loss:0.048,train_f1:0.981,valid_f1:0.892: 100%|▉| 1249/1251 [01:14<00:00\n",
      "epoch:42,batch:1248,lr:0.00064,loss:0.0088,mean_loss:0.032,train_f1:0.988,valid_f1:0.892: 100%|▉| 1249/1251 [01:14<00:0\n",
      "epoch:43,batch:1248,lr:0.00051,loss:0.035,mean_loss:0.025,train_f1:0.991,valid_f1:0.892: 100%|▉| 1249/1251 [01:14<00:00\n",
      "epoch:44,batch:1248,lr:0.00041,loss:0.0083,mean_loss:0.022,train_f1:0.993,valid_f1:0.89: 100%|▉| 1249/1251 [01:14<00:00\n",
      "epoch:45,batch:1248,lr:0.00033,loss:0.0094,mean_loss:0.017,train_f1:0.995,valid_f1:0.89: 100%|▉| 1249/1251 [01:15<00:00\n",
      "epoch:46,batch:1248,lr:0.00026,loss:0.0048,mean_loss:0.014,train_f1:0.996,valid_f1:0.891: 100%|▉| 1249/1251 [01:14<00:0\n",
      "epoch:47,batch:1248,lr:0.00021,loss:0.0051,mean_loss:0.012,train_f1:0.997,valid_f1:0.889: 100%|▉| 1249/1251 [01:12<00:0\n",
      "epoch:48,batch:1248,lr:0.00017,loss:0.0182,mean_loss:0.011,train_f1:0.997,valid_f1:0.887: 100%|▉| 1249/1251 [01:09<00:0\n",
      "epoch:49,batch:1248,lr:0.00013,loss:0.0042,mean_loss:0.01,train_f1:0.998,valid_f1:0.889: 100%|▉| 1249/1251 [01:15<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word num: 500\n",
      "random seed:1\n",
      "train device:cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:0,batch:1248,lr:0.001,loss:0.4475,mean_loss:1.196,train_f1:0.386,valid_f1:0.43: 100%|▉| 1249/1251 [01:16<00:00, 1\n",
      "epoch:1,batch:1248,lr:0.0008,loss:0.3271,mean_loss:0.376,train_f1:0.802,valid_f1:0.823: 100%|▉| 1249/1251 [01:17<00:00,\n",
      "epoch:2,batch:1248,lr:0.00064,loss:0.2234,mean_loss:0.278,train_f1:0.866,valid_f1:0.873: 100%|▉| 1249/1251 [01:17<00:00\n",
      "epoch:3,batch:1248,lr:0.00051,loss:0.1362,mean_loss:0.236,train_f1:0.892,valid_f1:0.886: 100%|▉| 1249/1251 [01:17<00:00\n",
      "epoch:4,batch:1248,lr:0.00041,loss:0.2527,mean_loss:0.211,train_f1:0.907,valid_f1:0.895: 100%|▉| 1249/1251 [01:17<00:00\n",
      "epoch:5,batch:1248,lr:0.00033,loss:0.1419,mean_loss:0.193,train_f1:0.916,valid_f1:0.898: 100%|▉| 1249/1251 [01:17<00:00\n",
      "epoch:6,batch:1248,lr:0.00026,loss:0.2463,mean_loss:0.18,train_f1:0.925,valid_f1:0.899: 100%|▉| 1249/1251 [01:17<00:00,\n",
      "epoch:7,batch:1248,lr:0.00021,loss:0.1936,mean_loss:0.17,train_f1:0.93,valid_f1:0.902: 100%|▉| 1249/1251 [01:17<00:00, \n",
      "epoch:8,batch:1248,lr:0.00017,loss:0.1594,mean_loss:0.161,train_f1:0.932,valid_f1:0.903: 100%|▉| 1249/1251 [01:17<00:00\n",
      "epoch:9,batch:1248,lr:0.00013,loss:0.0781,mean_loss:0.155,train_f1:0.937,valid_f1:0.902: 100%|▉| 1249/1251 [01:17<00:00\n",
      "epoch:10,batch:1248,lr:0.001,loss:0.1819,mean_loss:0.178,train_f1:0.923,valid_f1:0.901: 100%|▉| 1249/1251 [01:17<00:00,\n",
      "epoch:11,batch:1248,lr:0.0008,loss:0.1495,mean_loss:0.154,train_f1:0.934,valid_f1:0.901: 100%|▉| 1249/1251 [01:17<00:00\n",
      "epoch:12,batch:1248,lr:0.00064,loss:0.1945,mean_loss:0.135,train_f1:0.941,valid_f1:0.906: 100%|▉| 1249/1251 [01:18<00:0\n",
      "epoch:13,batch:1248,lr:0.00051,loss:0.1249,mean_loss:0.121,train_f1:0.951,valid_f1:0.906: 100%|▉| 1249/1251 [01:18<00:0\n",
      "epoch:14,batch:1248,lr:0.00041,loss:0.1234,mean_loss:0.109,train_f1:0.955,valid_f1:0.905: 100%|▉| 1249/1251 [01:18<00:0\n",
      "epoch:15,batch:1248,lr:0.00033,loss:0.0863,mean_loss:0.099,train_f1:0.96,valid_f1:0.904: 100%|▉| 1249/1251 [01:18<00:00\n",
      "epoch:16,batch:1248,lr:0.00026,loss:0.2304,mean_loss:0.091,train_f1:0.964,valid_f1:0.905: 100%|▉| 1249/1251 [01:19<00:0\n",
      "epoch:17,batch:1248,lr:0.00021,loss:0.1324,mean_loss:0.085,train_f1:0.967,valid_f1:0.906: 100%|▉| 1249/1251 [01:18<00:0\n",
      "epoch:18,batch:1248,lr:0.00017,loss:0.0929,mean_loss:0.079,train_f1:0.97,valid_f1:0.903: 100%|▉| 1249/1251 [01:19<00:00\n",
      "epoch:19,batch:1248,lr:0.00013,loss:0.0091,mean_loss:0.075,train_f1:0.972,valid_f1:0.901: 100%|▉| 1249/1251 [01:18<00:0\n",
      "epoch:20,batch:1248,lr:0.001,loss:0.0935,mean_loss:0.115,train_f1:0.95,valid_f1:0.9: 100%|▉| 1249/1251 [01:18<00:00, 15\n",
      "epoch:21,batch:1248,lr:0.0008,loss:0.1521,mean_loss:0.093,train_f1:0.962,valid_f1:0.905: 100%|▉| 1249/1251 [01:20<00:00\n",
      "epoch:22,batch:1248,lr:0.00064,loss:0.051,mean_loss:0.08,train_f1:0.968,valid_f1:0.906: 100%|▉| 1249/1251 [01:18<00:00,\n",
      "epoch:23,batch:1248,lr:0.00051,loss:0.0341,mean_loss:0.069,train_f1:0.973,valid_f1:0.904: 100%|▉| 1249/1251 [01:19<00:0\n",
      "epoch:24,batch:1248,lr:0.00041,loss:0.0395,mean_loss:0.06,train_f1:0.977,valid_f1:0.902: 100%|▉| 1249/1251 [01:18<00:00\n",
      "epoch:25,batch:1248,lr:0.00033,loss:0.0502,mean_loss:0.054,train_f1:0.981,valid_f1:0.902: 100%|▉| 1249/1251 [01:19<00:0\n",
      "epoch:26,batch:1248,lr:0.00026,loss:0.0215,mean_loss:0.048,train_f1:0.983,valid_f1:0.903: 100%|▉| 1249/1251 [01:19<00:0\n",
      "epoch:27,batch:1248,lr:0.00021,loss:0.0181,mean_loss:0.044,train_f1:0.985,valid_f1:0.902: 100%|▉| 1249/1251 [01:18<00:0\n",
      "epoch:28,batch:1248,lr:0.00017,loss:0.0658,mean_loss:0.04,train_f1:0.987,valid_f1:0.901: 100%|▉| 1249/1251 [01:36<00:00\n",
      "epoch:29,batch:1248,lr:0.00013,loss:0.0332,mean_loss:0.037,train_f1:0.988,valid_f1:0.9: 100%|▉| 1249/1251 [01:35<00:00,\n",
      "epoch:30,batch:1248,lr:0.001,loss:0.1035,mean_loss:0.086,train_f1:0.962,valid_f1:0.896: 100%|▉| 1249/1251 [01:34<00:00,\n",
      "epoch:31,batch:1248,lr:0.0008,loss:0.0686,mean_loss:0.062,train_f1:0.974,valid_f1:0.901: 100%|▉| 1249/1251 [01:34<00:00\n",
      "epoch:32,batch:1248,lr:0.00064,loss:0.0573,mean_loss:0.051,train_f1:0.98,valid_f1:0.9: 100%|▉| 1249/1251 [01:35<00:00, \n",
      "epoch:33,batch:1248,lr:0.00051,loss:0.0207,mean_loss:0.04,train_f1:0.985,valid_f1:0.902: 100%|▉| 1249/1251 [01:34<00:00\n",
      "epoch:34,batch:1248,lr:0.00041,loss:0.0146,mean_loss:0.035,train_f1:0.988,valid_f1:0.901: 100%|▉| 1249/1251 [01:34<00:0\n",
      "epoch:35,batch:1248,lr:0.00033,loss:0.0159,mean_loss:0.032,train_f1:0.989,valid_f1:0.901: 100%|▉| 1249/1251 [01:35<00:0\n",
      "epoch:36,batch:1248,lr:0.00026,loss:0.0114,mean_loss:0.026,train_f1:0.992,valid_f1:0.9: 100%|▉| 1249/1251 [01:34<00:00,\n",
      "epoch:37,batch:1248,lr:0.00021,loss:0.0252,mean_loss:0.023,train_f1:0.992,valid_f1:0.899: 100%|▉| 1249/1251 [01:34<00:0\n",
      "epoch:38,batch:1248,lr:0.00017,loss:0.017,mean_loss:0.021,train_f1:0.993,valid_f1:0.897: 100%|▉| 1249/1251 [01:34<00:00\n",
      "epoch:39,batch:1248,lr:0.00013,loss:0.073,mean_loss:0.019,train_f1:0.994,valid_f1:0.898: 100%|▉| 1249/1251 [01:35<00:00\n",
      "epoch:40,batch:1248,lr:0.001,loss:0.0193,mean_loss:0.076,train_f1:0.965,valid_f1:0.893: 100%|▉| 1249/1251 [01:34<00:00,\n",
      "epoch:41,batch:1248,lr:0.0008,loss:0.0218,mean_loss:0.045,train_f1:0.981,valid_f1:0.901: 100%|▉| 1249/1251 [01:34<00:00\n",
      "epoch:42,batch:1248,lr:0.00064,loss:0.0046,mean_loss:0.035,train_f1:0.986,valid_f1:0.901: 100%|▉| 1249/1251 [01:34<00:0\n",
      "epoch:43,batch:1248,lr:0.00051,loss:0.0115,mean_loss:0.027,train_f1:0.99,valid_f1:0.9: 100%|▉| 1249/1251 [01:34<00:00, \n",
      "epoch:44,batch:1248,lr:0.00041,loss:0.004,mean_loss:0.023,train_f1:0.992,valid_f1:0.899: 100%|▉| 1249/1251 [01:34<00:00\n",
      "epoch:45,batch:1248,lr:0.00033,loss:0.0303,mean_loss:0.018,train_f1:0.994,valid_f1:0.9: 100%|▉| 1249/1251 [01:34<00:00,\n",
      "epoch:46,batch:1248,lr:0.00026,loss:0.0047,mean_loss:0.015,train_f1:0.996,valid_f1:0.898: 100%|▉| 1249/1251 [01:34<00:0\n",
      "epoch:47,batch:1248,lr:0.00021,loss:0.003,mean_loss:0.014,train_f1:0.996,valid_f1:0.899: 100%|▉| 1249/1251 [01:34<00:00\n",
      "epoch:48,batch:1248,lr:0.00017,loss:0.0105,mean_loss:0.012,train_f1:0.997,valid_f1:0.897: 100%|▉| 1249/1251 [01:34<00:0\n",
      "epoch:49,batch:1248,lr:0.00013,loss:0.0092,mean_loss:0.01,train_f1:0.998,valid_f1:0.896: 100%|▉| 1249/1251 [01:34<00:00\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    word_num_list = [10,20,30,40,50,100,300,500]\n",
    "    for word_num in word_num_list:\n",
    "        print('word num:',word_num)\n",
    "        MyTrain(max_epoch=50,random_seed=1,word_num=word_num).my_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "584d57e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word num: 600\n",
      "random seed:1\n",
      "train device:cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:0,batch:1248,lr:0.001,loss:0.4247,mean_loss:1.175,train_f1:0.391,valid_f1:0.435: 100%|▉| 1249/1251 [01:21<00:00, \n",
      "epoch:1,batch:1248,lr:0.0008,loss:0.3177,mean_loss:0.379,train_f1:0.792,valid_f1:0.819: 100%|▉| 1249/1251 [01:21<00:00,\n",
      "epoch:2,batch:1248,lr:0.00064,loss:0.1959,mean_loss:0.279,train_f1:0.863,valid_f1:0.873: 100%|▉| 1249/1251 [01:20<00:00\n",
      "epoch:3,batch:1248,lr:0.00051,loss:0.1696,mean_loss:0.236,train_f1:0.887,valid_f1:0.888: 100%|▉| 1249/1251 [01:21<00:00\n",
      "epoch:4,batch:1248,lr:0.00041,loss:0.23,mean_loss:0.21,train_f1:0.905,valid_f1:0.896: 100%|▉| 1249/1251 [01:20<00:00, 1\n",
      "epoch:5,batch:1248,lr:0.00033,loss:0.1209,mean_loss:0.192,train_f1:0.916,valid_f1:0.899: 100%|▉| 1249/1251 [01:20<00:00\n",
      "epoch:6,batch:1248,lr:0.00026,loss:0.2119,mean_loss:0.178,train_f1:0.924,valid_f1:0.898: 100%|▉| 1249/1251 [01:20<00:00\n",
      "epoch:7,batch:1248,lr:0.00021,loss:0.1694,mean_loss:0.168,train_f1:0.93,valid_f1:0.902: 100%|▉| 1249/1251 [01:20<00:00,\n",
      "epoch:8,batch:1248,lr:0.00017,loss:0.1676,mean_loss:0.16,train_f1:0.932,valid_f1:0.904: 100%|▉| 1249/1251 [01:21<00:00,\n",
      "epoch:9,batch:1248,lr:0.00013,loss:0.0772,mean_loss:0.154,train_f1:0.935,valid_f1:0.902: 100%|▉| 1249/1251 [01:20<00:00\n",
      "epoch:10,batch:1248,lr:0.001,loss:0.1648,mean_loss:0.178,train_f1:0.921,valid_f1:0.899: 100%|▉| 1249/1251 [01:20<00:00,\n",
      "epoch:11,batch:1248,lr:0.0008,loss:0.1957,mean_loss:0.153,train_f1:0.934,valid_f1:0.904: 100%|▉| 1249/1251 [01:20<00:00\n",
      "epoch:12,batch:1248,lr:0.00064,loss:0.1548,mean_loss:0.135,train_f1:0.942,valid_f1:0.904: 100%|▉| 1249/1251 [01:20<00:0\n",
      "epoch:13,batch:1248,lr:0.00051,loss:0.1603,mean_loss:0.12,train_f1:0.95,valid_f1:0.907: 100%|▉| 1249/1251 [01:20<00:00,\n",
      "epoch:14,batch:1248,lr:0.00041,loss:0.1529,mean_loss:0.108,train_f1:0.956,valid_f1:0.907: 100%|▉| 1249/1251 [01:20<00:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word num: 750\n",
      "random seed:1\n",
      "train device:cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:0,batch:1248,lr:0.001,loss:0.3909,mean_loss:1.24,train_f1:0.371,valid_f1:0.412: 100%|▉| 1249/1251 [01:25<00:00, 1\n",
      "epoch:1,batch:1248,lr:0.0008,loss:0.2937,mean_loss:0.388,train_f1:0.784,valid_f1:0.812: 100%|▉| 1249/1251 [01:25<00:00,\n",
      "epoch:2,batch:1248,lr:0.00064,loss:0.2253,mean_loss:0.28,train_f1:0.861,valid_f1:0.872: 100%|▉| 1249/1251 [01:25<00:00,\n",
      "epoch:3,batch:1248,lr:0.00051,loss:0.1515,mean_loss:0.235,train_f1:0.886,valid_f1:0.886: 100%|▉| 1249/1251 [01:23<00:00\n",
      "epoch:4,batch:1248,lr:0.00041,loss:0.2829,mean_loss:0.208,train_f1:0.902,valid_f1:0.895: 100%|▉| 1249/1251 [01:23<00:00\n",
      "epoch:5,batch:1248,lr:0.00033,loss:0.121,mean_loss:0.189,train_f1:0.915,valid_f1:0.899: 100%|▉| 1249/1251 [01:23<00:00,\n",
      "epoch:6,batch:1248,lr:0.00026,loss:0.2304,mean_loss:0.175,train_f1:0.923,valid_f1:0.9: 100%|▉| 1249/1251 [01:23<00:00, \n",
      "epoch:7,batch:1248,lr:0.00021,loss:0.1875,mean_loss:0.164,train_f1:0.928,valid_f1:0.905: 100%|▉| 1249/1251 [01:23<00:00\n",
      "epoch:8,batch:1248,lr:0.00017,loss:0.166,mean_loss:0.156,train_f1:0.932,valid_f1:0.906: 100%|▉| 1249/1251 [01:23<00:00,\n",
      "epoch:9,batch:1248,lr:0.00013,loss:0.0862,mean_loss:0.15,train_f1:0.936,valid_f1:0.902: 100%|▉| 1249/1251 [01:23<00:00,\n",
      "epoch:10,batch:1248,lr:0.001,loss:0.2519,mean_loss:0.172,train_f1:0.922,valid_f1:0.902: 100%|▉| 1249/1251 [01:23<00:00,\n",
      "epoch:11,batch:1248,lr:0.0008,loss:0.1773,mean_loss:0.149,train_f1:0.935,valid_f1:0.906: 100%|▉| 1249/1251 [01:23<00:00\n",
      "epoch:12,batch:1248,lr:0.00064,loss:0.1257,mean_loss:0.131,train_f1:0.944,valid_f1:0.909: 100%|▉| 1249/1251 [01:23<00:0\n",
      "epoch:13,batch:1248,lr:0.00051,loss:0.075,mean_loss:0.116,train_f1:0.952,valid_f1:0.91: 100%|▉| 1249/1251 [01:23<00:00,\n",
      "epoch:14,batch:1248,lr:0.00041,loss:0.1624,mean_loss:0.103,train_f1:0.958,valid_f1:0.908: 100%|▉| 1249/1251 [01:23<00:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word num: 900\n",
      "random seed:1\n",
      "train device:cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:0,batch:1248,lr:0.001,loss:0.6478,mean_loss:1.36,train_f1:0.313,valid_f1:0.352: 100%|▉| 1249/1251 [01:30<00:00, 1\n",
      "epoch:1,batch:1248,lr:0.0008,loss:0.3365,mean_loss:0.439,train_f1:0.74,valid_f1:0.781: 100%|▉| 1249/1251 [01:30<00:00, \n",
      "epoch:2,batch:1248,lr:0.00064,loss:0.3101,mean_loss:0.3,train_f1:0.845,valid_f1:0.86: 100%|▉| 1249/1251 [01:30<00:00, 1\n",
      "epoch:3,batch:1248,lr:0.00051,loss:0.18,mean_loss:0.245,train_f1:0.884,valid_f1:0.884: 100%|▉| 1249/1251 [01:30<00:00, \n",
      "epoch:4,batch:1248,lr:0.00041,loss:0.2665,mean_loss:0.214,train_f1:0.901,valid_f1:0.895: 100%|▉| 1249/1251 [01:30<00:00\n",
      "epoch:5,batch:1248,lr:0.00033,loss:0.157,mean_loss:0.193,train_f1:0.912,valid_f1:0.9: 100%|▉| 1249/1251 [01:31<00:00, 1\n",
      "epoch:6,batch:1248,lr:0.00026,loss:0.1694,mean_loss:0.179,train_f1:0.921,valid_f1:0.9: 100%|▉| 1249/1251 [01:30<00:00, \n",
      "epoch:7,batch:1248,lr:0.00021,loss:0.1491,mean_loss:0.168,train_f1:0.928,valid_f1:0.903: 100%|▉| 1249/1251 [01:30<00:00\n",
      "epoch:8,batch:1248,lr:0.00017,loss:0.1827,mean_loss:0.16,train_f1:0.931,valid_f1:0.904: 100%|▉| 1249/1251 [01:30<00:00,\n",
      "epoch:9,batch:1248,lr:0.00013,loss:0.1002,mean_loss:0.153,train_f1:0.935,valid_f1:0.903: 100%|▉| 1249/1251 [01:30<00:00\n",
      "epoch:10,batch:1248,lr:0.001,loss:0.1981,mean_loss:0.174,train_f1:0.922,valid_f1:0.903: 100%|▉| 1249/1251 [01:31<00:00,\n",
      "epoch:11,batch:1248,lr:0.0008,loss:0.1409,mean_loss:0.15,train_f1:0.935,valid_f1:0.906: 100%|▉| 1249/1251 [01:31<00:00,\n",
      "epoch:12,batch:1248,lr:0.00064,loss:0.1208,mean_loss:0.133,train_f1:0.944,valid_f1:0.91: 100%|▉| 1249/1251 [01:30<00:00\n",
      "epoch:13,batch:1248,lr:0.00051,loss:0.0837,mean_loss:0.118,train_f1:0.951,valid_f1:0.909: 100%|▉| 1249/1251 [01:31<00:0\n",
      "epoch:14,batch:1248,lr:0.00041,loss:0.1787,mean_loss:0.106,train_f1:0.957,valid_f1:0.91: 100%|▉| 1249/1251 [01:30<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word num: 1100\n",
      "random seed:1\n",
      "train device:cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:0,batch:1248,lr:0.001,loss:0.7379,mean_loss:1.395,train_f1:0.302,valid_f1:0.343: 100%|▉| 1249/1251 [01:41<00:00, \n",
      "epoch:1,batch:1248,lr:0.0008,loss:0.4003,mean_loss:0.46,train_f1:0.734,valid_f1:0.769: 100%|▉| 1249/1251 [01:40<00:00, \n",
      "epoch:2,batch:1248,lr:0.00064,loss:0.3051,mean_loss:0.31,train_f1:0.836,valid_f1:0.852: 100%|▉| 1249/1251 [01:41<00:00,\n",
      "epoch:3,batch:1248,lr:0.00051,loss:0.1967,mean_loss:0.255,train_f1:0.877,valid_f1:0.874: 100%|▉| 1249/1251 [01:40<00:00\n",
      "epoch:4,batch:1248,lr:0.00041,loss:0.2434,mean_loss:0.224,train_f1:0.899,valid_f1:0.89: 100%|▉| 1249/1251 [01:41<00:00,\n",
      "epoch:5,batch:1248,lr:0.00033,loss:0.1136,mean_loss:0.202,train_f1:0.912,valid_f1:0.898: 100%|▉| 1249/1251 [01:41<00:00\n",
      "epoch:6,batch:1248,lr:0.00026,loss:0.1932,mean_loss:0.187,train_f1:0.921,valid_f1:0.899: 100%|▉| 1249/1251 [01:44<00:00\n",
      "epoch:7,batch:1248,lr:0.00021,loss:0.2131,mean_loss:0.176,train_f1:0.927,valid_f1:0.902: 100%|▉| 1249/1251 [01:44<00:00\n",
      "epoch:8,batch:1248,lr:0.00017,loss:0.1619,mean_loss:0.167,train_f1:0.93,valid_f1:0.903: 100%|▉| 1249/1251 [01:44<00:00,\n",
      "epoch:9,batch:1248,lr:0.00013,loss:0.0689,mean_loss:0.16,train_f1:0.935,valid_f1:0.902: 100%|▉| 1249/1251 [01:44<00:00,\n",
      "epoch:10,batch:1248,lr:0.001,loss:0.1673,mean_loss:0.184,train_f1:0.921,valid_f1:0.899: 100%|▉| 1249/1251 [01:44<00:00,\n",
      "epoch:11,batch:1248,lr:0.0008,loss:0.204,mean_loss:0.159,train_f1:0.932,valid_f1:0.905: 100%|▉| 1249/1251 [01:44<00:00,\n",
      "epoch:12,batch:1248,lr:0.00064,loss:0.1241,mean_loss:0.141,train_f1:0.939,valid_f1:0.907: 100%|▉| 1249/1251 [01:44<00:0\n",
      "epoch:13,batch:1248,lr:0.00051,loss:0.0694,mean_loss:0.125,train_f1:0.95,valid_f1:0.908: 100%|▉| 1249/1251 [01:44<00:00\n",
      "epoch:14,batch:1248,lr:0.00041,loss:0.1724,mean_loss:0.114,train_f1:0.955,valid_f1:0.908: 100%|▉| 1249/1251 [01:44<00:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word num: 1300\n",
      "random seed:1\n",
      "train device:cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:0,batch:1248,lr:0.001,loss:0.8377,mean_loss:1.485,train_f1:0.245,valid_f1:0.284: 100%|▉| 1249/1251 [01:55<00:00, \n",
      "epoch:1,batch:1248,lr:0.0008,loss:0.466,mean_loss:0.546,train_f1:0.671,valid_f1:0.719: 100%|▉| 1249/1251 [01:54<00:00, \n",
      "epoch:2,batch:1248,lr:0.00064,loss:0.2931,mean_loss:0.349,train_f1:0.805,valid_f1:0.832: 100%|▉| 1249/1251 [01:54<00:00\n",
      "epoch:3,batch:1248,lr:0.00051,loss:0.1915,mean_loss:0.273,train_f1:0.861,valid_f1:0.867: 100%|▉| 1249/1251 [01:55<00:00\n",
      "epoch:4,batch:1248,lr:0.00041,loss:0.264,mean_loss:0.235,train_f1:0.888,valid_f1:0.885: 100%|▉| 1249/1251 [01:55<00:00,\n",
      "epoch:5,batch:1248,lr:0.00033,loss:0.1971,mean_loss:0.21,train_f1:0.903,valid_f1:0.892: 100%|▉| 1249/1251 [01:55<00:00,\n",
      "epoch:6,batch:1248,lr:0.00026,loss:0.2224,mean_loss:0.193,train_f1:0.916,valid_f1:0.896: 100%|▉| 1249/1251 [01:54<00:00\n",
      "epoch:7,batch:1248,lr:0.00021,loss:0.182,mean_loss:0.182,train_f1:0.922,valid_f1:0.9: 100%|▉| 1249/1251 [01:55<00:00, 1\n",
      "epoch:8,batch:1248,lr:0.00017,loss:0.1676,mean_loss:0.173,train_f1:0.926,valid_f1:0.901: 100%|▉| 1249/1251 [01:55<00:00\n",
      "epoch:9,batch:1248,lr:0.00013,loss:0.0855,mean_loss:0.166,train_f1:0.93,valid_f1:0.9: 100%|▉| 1249/1251 [01:54<00:00, 1\n",
      "epoch:10,batch:1248,lr:0.001,loss:0.2625,mean_loss:0.188,train_f1:0.918,valid_f1:0.899: 100%|▉| 1249/1251 [01:55<00:00,\n",
      "epoch:11,batch:1248,lr:0.0008,loss:0.1409,mean_loss:0.162,train_f1:0.93,valid_f1:0.906: 100%|▉| 1249/1251 [01:55<00:00,\n",
      "epoch:12,batch:1248,lr:0.00064,loss:0.1301,mean_loss:0.143,train_f1:0.939,valid_f1:0.908: 100%|▉| 1249/1251 [01:55<00:0\n",
      "epoch:13,batch:1248,lr:0.00051,loss:0.1071,mean_loss:0.127,train_f1:0.946,valid_f1:0.909: 100%|▉| 1249/1251 [01:54<00:0\n",
      "epoch:14,batch:1248,lr:0.00041,loss:0.1583,mean_loss:0.116,train_f1:0.953,valid_f1:0.911: 100%|▉| 1249/1251 [01:55<00:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word num: 1500\n",
      "random seed:1\n",
      "train device:cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:0,batch:1248,lr:0.001,loss:0.546,mean_loss:1.539,train_f1:0.267,valid_f1:0.299: 100%|▉| 1249/1251 [02:17<00:00,  \n",
      "epoch:1,batch:1248,lr:0.0008,loss:0.3225,mean_loss:0.421,train_f1:0.772,valid_f1:0.8: 100%|▉| 1249/1251 [02:04<00:00, 1\n",
      "epoch:2,batch:1248,lr:0.00064,loss:0.2327,mean_loss:0.294,train_f1:0.851,valid_f1:0.861: 100%|▉| 1249/1251 [02:04<00:00\n",
      "epoch:3,batch:1248,lr:0.00051,loss:0.1732,mean_loss:0.245,train_f1:0.883,valid_f1:0.878: 100%|▉| 1249/1251 [02:04<00:00\n",
      "epoch:4,batch:1248,lr:0.00041,loss:0.271,mean_loss:0.215,train_f1:0.9,valid_f1:0.89: 100%|▉| 1249/1251 [02:04<00:00, 10\n",
      "epoch:5,batch:1248,lr:0.00033,loss:0.1477,mean_loss:0.195,train_f1:0.911,valid_f1:0.895: 100%|▉| 1249/1251 [02:05<00:00\n",
      "epoch:6,batch:1248,lr:0.00026,loss:0.2166,mean_loss:0.181,train_f1:0.921,valid_f1:0.897: 100%|▉| 1249/1251 [02:04<00:00\n",
      "epoch:7,batch:1248,lr:0.00021,loss:0.1615,mean_loss:0.17,train_f1:0.927,valid_f1:0.899: 100%|▉| 1249/1251 [02:04<00:00,\n",
      "epoch:8,batch:1248,lr:0.00017,loss:0.1329,mean_loss:0.162,train_f1:0.931,valid_f1:0.9: 100%|▉| 1249/1251 [02:04<00:00, \n",
      "epoch:9,batch:1248,lr:0.00013,loss:0.1464,mean_loss:0.155,train_f1:0.934,valid_f1:0.899: 100%|▉| 1249/1251 [02:04<00:00\n",
      "epoch:10,batch:1248,lr:0.001,loss:0.2769,mean_loss:0.179,train_f1:0.92,valid_f1:0.899: 100%|▉| 1249/1251 [02:02<00:00, \n",
      "epoch:11,batch:1248,lr:0.0008,loss:0.177,mean_loss:0.153,train_f1:0.935,valid_f1:0.901: 100%|▉| 1249/1251 [02:01<00:00,\n",
      "epoch:12,batch:1248,lr:0.00064,loss:0.1921,mean_loss:0.134,train_f1:0.945,valid_f1:0.908: 100%|▉| 1249/1251 [02:02<00:0\n",
      "epoch:13,batch:1248,lr:0.00051,loss:0.0903,mean_loss:0.12,train_f1:0.951,valid_f1:0.909: 100%|▉| 1249/1251 [02:02<00:00\n",
      "epoch:14,batch:1248,lr:0.00041,loss:0.1177,mean_loss:0.108,train_f1:0.958,valid_f1:0.91: 100%|▉| 1249/1251 [02:02<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word num: 1800\n",
      "random seed:1\n",
      "train device:cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:0,batch:1248,lr:0.001,loss:0.7737,mean_loss:1.716,train_f1:0.193,valid_f1:0.223: 100%|▉| 1249/1251 [02:19<00:00, \n",
      "epoch:1,batch:1248,lr:0.0008,loss:0.363,mean_loss:0.51,train_f1:0.701,valid_f1:0.736: 100%|▉| 1249/1251 [02:19<00:00,  \n",
      "epoch:2,batch:1248,lr:0.00064,loss:0.3439,mean_loss:0.311,train_f1:0.851,valid_f1:0.858: 100%|▉| 1249/1251 [02:20<00:00\n",
      "epoch:3,batch:1248,lr:0.00051,loss:0.2392,mean_loss:0.249,train_f1:0.887,valid_f1:0.882: 100%|▉| 1249/1251 [02:19<00:00\n",
      "epoch:4,batch:1248,lr:0.00041,loss:0.2992,mean_loss:0.217,train_f1:0.907,valid_f1:0.895: 100%|▉| 1249/1251 [02:19<00:00\n",
      "epoch:5,batch:1248,lr:0.00033,loss:0.1671,mean_loss:0.196,train_f1:0.917,valid_f1:0.901: 100%|▉| 1249/1251 [02:19<00:00\n",
      "epoch:6,batch:1248,lr:0.00026,loss:0.2245,mean_loss:0.181,train_f1:0.927,valid_f1:0.902: 100%|▉| 1249/1251 [02:19<00:00\n",
      "epoch:7,batch:1248,lr:0.00021,loss:0.2048,mean_loss:0.17,train_f1:0.933,valid_f1:0.906: 100%|▉| 1249/1251 [02:20<00:00,\n",
      "epoch:8,batch:1248,lr:0.00017,loss:0.1447,mean_loss:0.161,train_f1:0.936,valid_f1:0.905: 100%|▉| 1249/1251 [02:20<00:00\n",
      "epoch:9,batch:1248,lr:0.00013,loss:0.0714,mean_loss:0.154,train_f1:0.94,valid_f1:0.907: 100%|▉| 1249/1251 [02:19<00:00,\n",
      "epoch:10,batch:1248,lr:0.001,loss:0.1733,mean_loss:0.178,train_f1:0.926,valid_f1:0.902: 100%|▉| 1249/1251 [02:19<00:00,\n",
      "epoch:11,batch:1248,lr:0.0008,loss:0.1295,mean_loss:0.153,train_f1:0.937,valid_f1:0.908: 100%|▉| 1249/1251 [02:20<00:00\n",
      "epoch:12,batch:1248,lr:0.00064,loss:0.2347,mean_loss:0.135,train_f1:0.946,valid_f1:0.91: 100%|▉| 1249/1251 [02:23<00:00\n",
      "epoch:13,batch:1248,lr:0.00051,loss:0.1164,mean_loss:0.12,train_f1:0.952,valid_f1:0.912: 100%|▉| 1249/1251 [02:22<00:00\n",
      "epoch:14,batch:1248,lr:0.00041,loss:0.1489,mean_loss:0.108,train_f1:0.96,valid_f1:0.913: 100%|▉| 1249/1251 [02:22<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word num: 2000\n",
      "random seed:1\n",
      "train device:cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:0,batch:1248,lr:0.001,loss:0.8041,mean_loss:1.791,train_f1:0.161,valid_f1:0.19: 100%|▉| 1249/1251 [02:35<00:00,  \n",
      "epoch:1,batch:1248,lr:0.0008,loss:0.4571,mean_loss:0.582,train_f1:0.633,valid_f1:0.674: 100%|▉| 1249/1251 [02:32<00:00,\n",
      "epoch:2,batch:1248,lr:0.00064,loss:0.3514,mean_loss:0.347,train_f1:0.826,valid_f1:0.84: 100%|▉| 1249/1251 [02:32<00:00,\n",
      "epoch:3,batch:1248,lr:0.00051,loss:0.2151,mean_loss:0.269,train_f1:0.875,valid_f1:0.873: 100%|▉| 1249/1251 [02:33<00:00\n",
      "epoch:4,batch:1248,lr:0.00041,loss:0.333,mean_loss:0.231,train_f1:0.897,valid_f1:0.887: 100%|▉| 1249/1251 [02:33<00:00,\n",
      "epoch:5,batch:1248,lr:0.00033,loss:0.1418,mean_loss:0.206,train_f1:0.911,valid_f1:0.894: 100%|▉| 1249/1251 [02:32<00:00\n",
      "epoch:6,batch:1248,lr:0.00026,loss:0.1934,mean_loss:0.19,train_f1:0.921,valid_f1:0.897: 100%|▉| 1249/1251 [02:37<00:00,\n",
      "epoch:7,batch:1248,lr:0.00021,loss:0.1734,mean_loss:0.178,train_f1:0.928,valid_f1:0.9: 100%|▉| 1249/1251 [02:31<00:00, \n",
      "epoch:8,batch:1248,lr:0.00017,loss:0.154,mean_loss:0.168,train_f1:0.931,valid_f1:0.901: 100%|▉| 1249/1251 [02:33<00:00,\n",
      "epoch:9,batch:1248,lr:0.00013,loss:0.1202,mean_loss:0.161,train_f1:0.935,valid_f1:0.901: 100%|▉| 1249/1251 [02:33<00:00\n",
      "epoch:10,batch:1248,lr:0.001,loss:0.1391,mean_loss:0.183,train_f1:0.925,valid_f1:0.901: 100%|▉| 1249/1251 [02:32<00:00,\n",
      "epoch:11,batch:1248,lr:0.0008,loss:0.1681,mean_loss:0.156,train_f1:0.936,valid_f1:0.906: 100%|▉| 1249/1251 [02:32<00:00\n",
      "epoch:12,batch:1248,lr:0.00064,loss:0.1517,mean_loss:0.136,train_f1:0.945,valid_f1:0.908: 100%|▉| 1249/1251 [02:34<00:0\n",
      "epoch:13,batch:1248,lr:0.00051,loss:0.0702,mean_loss:0.12,train_f1:0.954,valid_f1:0.91: 100%|▉| 1249/1251 [02:33<00:00,\n",
      "epoch:14,batch:1248,lr:0.00041,loss:0.1122,mean_loss:0.108,train_f1:0.959,valid_f1:0.91: 100%|▉| 1249/1251 [02:33<00:00\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    word_num_list = [600,750,900,1100,1300,1500,1800,2000]\n",
    "    for word_num in word_num_list:\n",
    "        print('word num:',word_num)\n",
    "        MyTrain(max_epoch=15,random_seed=1,word_num=word_num).my_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdf7eb4",
   "metadata": {},
   "source": [
    "上面可以看到，如果取前2000个字为最大范围的话，取到前1000个字的时候，验证集的f1成绩就已经到极限了，保持在0.91基本上不会再高了，可以理解为数据集里有一些比较长的文章，只看前2000个字不能看出到底是哪个分类，还需要充分利用后面的内容，下面采用另外一种思路来测试，每个text统一处理成2000个字，长度小于2000的text直接取用，长度大于2000的字符串，按照前500后500中间1000，总共2000来抽取，其中中间1000个字按照10*100的形式来抽取。  \n",
    "\n",
    "从下面的结果来看，没有什么作用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bf123a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def split_train_valid(csv_path,train_prec=0.75):\n",
    "    csv_data = pd.read_csv(csv_path,sep='\\t') \n",
    "    text_data = csv_data.text\n",
    "    label_data = csv_data.label\n",
    "    \n",
    "    mask = np.random.rand(len(text_data))<train_prec\n",
    "    train_text_data = text_data[mask].reset_index(drop=True)\n",
    "    valid_text_data = text_data[~mask].reset_index(drop=True)\n",
    "    train_label_data = label_data[mask].reset_index(drop=True)\n",
    "    valid_label_data = label_data[~mask].reset_index(drop=True)\n",
    "    \n",
    "    return train_text_data,valid_text_data,train_label_data,valid_label_data\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self,text_data,label_data,word_num):\n",
    "        self.text_data = text_data\n",
    "        self.label_data = label_data\n",
    "        self.word_num = word_num\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        #所有text内的token索引增加1，0空出来代表空格，将每个text控制在一定长度内\n",
    "        text_str = self.text_data[index]\n",
    "        text_list = [int(x)+1 for x in text_str.split()]\n",
    "        out_text_list = []\n",
    "        if len(text_list)>=2000:\n",
    "            start_text_list = text_list[:500]\n",
    "            out_text_list.extend(start_text_list)\n",
    "            for i in range(10):\n",
    "                index_gap = (len(text_list)-1000)//10\n",
    "                middle_text_list = text_list[500+(i*index_gap):500+(i*index_gap)+100]\n",
    "                out_text_list.extend(middle_text_list)\n",
    "            end_text_list = text_list[-500:]\n",
    "            out_text_list.extend(end_text_list)\n",
    "        else:\n",
    "            out_text_list = [x for x in text_list]\n",
    "            out_text_list.extend([0]*(2000-len(text_list)))\n",
    "        text_array = np.array(out_text_list)\n",
    "        label_array = np.array(self.label_data[index])\n",
    "        return text_array,label_array\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text_data)\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(7551,100)\n",
    "        self.rnn = nn.GRU(100,50,batch_first=True)\n",
    "        self.fc = nn.Linear(50,14)\n",
    "        \n",
    "    def forward(self,X):\n",
    "#         print('X:',X.shape)\n",
    "        X = self.embedding(X)\n",
    "#         print('embedding:',X.shape)\n",
    "        _,X = self.rnn(X)\n",
    "#         print('rnn:',X.shape)\n",
    "        X = X.squeeze(dim=0)\n",
    "#         print('squeeze:',X.shape)\n",
    "        y_hat = self.fc(X)\n",
    "#         print('y_hat:',y_hat.shape)\n",
    "        return y_hat\n",
    "\n",
    "    \n",
    "class MyTrain():\n",
    "    def __init__(self,max_epoch=1,random_seed=1,lr=0.001,out_dir='./',word_num= 1000):\n",
    "        self.max_epoch = max_epoch\n",
    "        self.random_seed = random_seed\n",
    "        self.lr = lr\n",
    "        self.out_dir = out_dir\n",
    "        self.iter = 0\n",
    "        self.word_num = word_num\n",
    "        \n",
    "    def fix_random(self):\n",
    "        import random\n",
    "        import numpy as np\n",
    "        import torch\n",
    "        random.seed(self.random_seed)\n",
    "        np.random.seed(self.random_seed)\n",
    "        torch.random.manual_seed(self.random_seed)\n",
    "        torch.cuda.random.manual_seed_all(self.random_seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        print(f'random seed:{self.random_seed}')\n",
    "        \n",
    "    def my_train(self):\n",
    "        \n",
    "        max_epoch,lr = self.max_epoch,self.lr\n",
    "        \n",
    "        if self.random_seed is not None:\n",
    "            self.fix_random()\n",
    "        \n",
    "        train_text_data,valid_text_data,train_label_data,valid_label_data = split_train_valid('./train_set.csv',train_prec=0.75)\n",
    "        train_dataset = MyDataset(train_text_data,train_label_data,self.word_num)\n",
    "        valid_dataset = MyDataset(valid_text_data,valid_label_data,self.word_num)\n",
    "        \n",
    "        my_model = MyModel()\n",
    "        my_optim = torch.optim.Adam(my_model.parameters(),lr=lr)\n",
    "        my_loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            my_model.cuda()\n",
    "            my_loss.cuda()\n",
    "        print(f'train device:{next(iter(my_model.parameters())).device}')  #显示训练设备\n",
    "        \n",
    "        best_f1_score = 0\n",
    "        epoch_index = 0\n",
    "\n",
    "        for epoch_index in range(max_epoch):\n",
    "            \n",
    "            loss_list = []\n",
    "            train_f1_score_list = []\n",
    "            valid_f1_score_list = []\n",
    "            train_dataloader = DataLoader(train_dataset,batch_size=120,shuffle=True)\n",
    "            valid_dataloader = DataLoader(valid_dataset,batch_size=40,shuffle=True)\n",
    "            my_dataloader = tqdm(train_dataloader)\n",
    "            \n",
    "            batch_index = 0\n",
    "                        \n",
    "            for train_data,valid_data in zip(my_dataloader,valid_dataloader):\n",
    "                                \n",
    "                my_model.train()  #将模型设置为训练模式\n",
    "                train_text,train_label = train_data\n",
    "                valid_text,valid_label = valid_data\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    train_text = train_text.cuda()\n",
    "                    train_label = train_label.cuda()\n",
    "                    valid_text = valid_text.cuda()\n",
    "                    valid_label = valid_label.cuda()\n",
    "                \n",
    "                train_y_hat = my_model(train_text)\n",
    "                batch_train_loss = my_loss(train_y_hat,train_label)\n",
    "                \n",
    "                my_optim.zero_grad()\n",
    "                batch_train_loss.backward()\n",
    "                my_optim.step()\n",
    "                my_optim.param_groups[0]['lr'] = lr*(0.8**(epoch_index%10))\n",
    "        \n",
    "                my_model.eval()  #将模型设置为验证模式\n",
    "                with torch.no_grad():\n",
    "                    valid_y_hat = my_model(valid_text)\n",
    "                    batch_valid_f1_score = self.f1_score(valid_y_hat.data,valid_label.data)\n",
    "                    valid_f1_score_list.append(batch_valid_f1_score)\n",
    "                    mean_valid_f1 = round(sum(valid_f1_score_list)/len(valid_f1_score_list),3)\n",
    "\n",
    "                    #显示batch结果\n",
    "                    batch_lr = round(my_optim.param_groups[0]['lr'],5)\n",
    "                    batch_loss = round(batch_train_loss.item(),4)\n",
    "                    loss_list.append(batch_loss)\n",
    "                    mean_loss = round((sum(loss_list)/len(loss_list)),3)\n",
    "\n",
    "                    batch_train_f1_score = self.f1_score(train_y_hat.data,train_label.data)\n",
    "                    train_f1_score_list.append(batch_train_f1_score)\n",
    "                    mean_train_f1 = round(sum(train_f1_score_list)/len(train_f1_score_list),3)\n",
    "\n",
    "                    my_dataloader.set_description(f'epoch:{epoch_index},batch:{batch_index},lr:{batch_lr},loss:{batch_loss},mean_loss:{mean_loss},train_f1:{mean_train_f1},valid_f1:{mean_valid_f1}')\n",
    "                    \n",
    "                    batch_index+=1\n",
    "            #存储模型\n",
    "            if mean_valid_f1>0.96:\n",
    "                torch.save(my_model.state_dict(),os.path.join(self.out_dir,f'embedding_gru_best_{self.word_num}word_valid_f1_score_{mean_valid_f1}'))\n",
    "\n",
    "            \n",
    "                \n",
    "    def f1_score(self,y_hat,label,eps=1e-8):\n",
    "        #y_hat(N,C),label(1)\n",
    "        y_hat = y_hat.cpu()\n",
    "        label = label.cpu()\n",
    "        preds_list = list(torch.argmax(y_hat,dim=1).numpy())\n",
    "        label_list = list(label.numpy())\n",
    "#         print(f'preds:{preds_list},label:{label_list}')\n",
    "        class_index_list = []\n",
    "        for class_index in label_list:\n",
    "            if class_index not in class_index_list:\n",
    "                class_index_list.append(class_index)\n",
    "\n",
    "        f1_score_list = []\n",
    "        for index in class_index_list:\n",
    "            if index not in preds_list:\n",
    "                sub_f1_score = 0\n",
    "            else:\n",
    "                tp = 0\n",
    "                fp = 0\n",
    "                fn = 0\n",
    "                for i in range(len(preds_list)):\n",
    "                    if preds_list[i] == index and label_list[i] == index:\n",
    "                        tp+=1\n",
    "                    if preds_list[i] == index and label_list[i] != index: \n",
    "                        fp+=1\n",
    "                    if preds_list[i] != index and label_list[i] == index:\n",
    "                        fn+=1\n",
    "                prec_val = tp/(tp+fp) \n",
    "                recall_val = tp/(tp+fn)\n",
    "                sub_f1_score = 2*(prec_val*recall_val)/(prec_val+recall_val+eps)\n",
    "            f1_score_list.append(sub_f1_score)\n",
    "\n",
    "        batch_f1_score = sum(f1_score_list)/len(f1_score_list)\n",
    "\n",
    "        return batch_f1_score\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8162add5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random seed:1\n",
      "train device:cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:0,batch:1248,lr:0.001,loss:1.0405,mean_loss:1.875,train_f1:0.157,valid_f1:0.177: 100%|▉| 1249/1251 [02:38<00:00, \n",
      "epoch:1,batch:1248,lr:0.0008,loss:0.6333,mean_loss:0.726,train_f1:0.555,valid_f1:0.599: 100%|▉| 1249/1251 [02:39<00:00,\n",
      "epoch:2,batch:1248,lr:0.00064,loss:0.5164,mean_loss:0.482,train_f1:0.734,valid_f1:0.754: 100%|▉| 1249/1251 [02:40<00:00\n",
      "epoch:3,batch:1248,lr:0.00051,loss:0.3069,mean_loss:0.369,train_f1:0.819,valid_f1:0.828: 100%|▉| 1249/1251 [02:39<00:00\n",
      "epoch:4,batch:1248,lr:0.00041,loss:0.4163,mean_loss:0.313,train_f1:0.845,valid_f1:0.848: 100%|▉| 1249/1251 [02:39<00:00\n",
      "epoch:5,batch:1248,lr:0.00033,loss:0.1698,mean_loss:0.278,train_f1:0.861,valid_f1:0.861: 100%|▉| 1249/1251 [02:39<00:00\n",
      "epoch:6,batch:1248,lr:0.00026,loss:0.2978,mean_loss:0.255,train_f1:0.878,valid_f1:0.869: 100%|▉| 1249/1251 [02:38<00:00\n",
      "epoch:7,batch:1248,lr:0.00021,loss:0.2523,mean_loss:0.238,train_f1:0.889,valid_f1:0.874: 100%|▉| 1249/1251 [02:38<00:00\n",
      "epoch:8,batch:1248,lr:0.00017,loss:0.179,mean_loss:0.227,train_f1:0.895,valid_f1:0.876: 100%|▉| 1249/1251 [02:39<00:00,\n",
      "epoch:9,batch:1248,lr:0.00013,loss:0.1043,mean_loss:0.216,train_f1:0.902,valid_f1:0.88: 100%|▉| 1249/1251 [02:38<00:00,\n",
      "epoch:10,batch:1248,lr:0.001,loss:0.2368,mean_loss:0.244,train_f1:0.887,valid_f1:0.877: 100%|▉| 1249/1251 [02:38<00:00,\n",
      "epoch:11,batch:1248,lr:0.0008,loss:0.2276,mean_loss:0.207,train_f1:0.909,valid_f1:0.888: 100%|▉| 1249/1251 [02:39<00:00\n",
      "epoch:12,batch:1248,lr:0.00064,loss:0.2373,mean_loss:0.181,train_f1:0.923,valid_f1:0.895: 100%|▉| 1249/1251 [02:38<00:0\n",
      "epoch:13,batch:1248,lr:0.00051,loss:0.1496,mean_loss:0.165,train_f1:0.932,valid_f1:0.9: 100%|▉| 1249/1251 [02:38<00:00,\n",
      "epoch:14,batch:1248,lr:0.00041,loss:0.1451,mean_loss:0.147,train_f1:0.941,valid_f1:0.899: 100%|▉| 1249/1251 [02:39<00:0\n",
      "epoch:15,batch:1248,lr:0.00033,loss:0.114,mean_loss:0.135,train_f1:0.948,valid_f1:0.901: 100%|▉| 1249/1251 [02:39<00:00\n",
      "epoch:16,batch:1248,lr:0.00026,loss:0.1325,mean_loss:0.124,train_f1:0.953,valid_f1:0.903: 100%|▉| 1249/1251 [02:38<00:0\n",
      "epoch:17,batch:1248,lr:0.00021,loss:0.1191,mean_loss:0.116,train_f1:0.956,valid_f1:0.902: 100%|▉| 1249/1251 [02:38<00:0\n",
      "epoch:18,batch:1248,lr:0.00017,loss:0.1138,mean_loss:0.109,train_f1:0.961,valid_f1:0.901: 100%|▉| 1249/1251 [02:38<00:0\n",
      "epoch:19,batch:1248,lr:0.00013,loss:0.0373,mean_loss:0.104,train_f1:0.963,valid_f1:0.9: 100%|▉| 1249/1251 [02:38<00:00,\n",
      "epoch:20,batch:1248,lr:0.001,loss:0.2166,mean_loss:0.151,train_f1:0.937,valid_f1:0.893: 100%|▉| 1249/1251 [02:39<00:00,\n",
      "epoch:21,batch:1248,lr:0.0008,loss:0.08,mean_loss:0.128,train_f1:0.948,valid_f1:0.9: 100%|▉| 1249/1251 [02:38<00:00,  7\n",
      "epoch:22,batch:1248,lr:0.00064,loss:0.0703,mean_loss:0.11,train_f1:0.958,valid_f1:0.903: 100%|▉| 1249/1251 [02:38<00:00\n",
      "epoch:23,batch:1248,lr:0.00051,loss:0.133,mean_loss:0.097,train_f1:0.963,valid_f1:0.9: 100%|▉| 1249/1251 [02:39<00:00, \n",
      "epoch:24,batch:1248,lr:0.00041,loss:0.0808,mean_loss:0.086,train_f1:0.968,valid_f1:0.9: 100%|▉| 1249/1251 [02:39<00:00,\n",
      "epoch:25,batch:1248,lr:0.00033,loss:0.0442,mean_loss:0.077,train_f1:0.973,valid_f1:0.902: 100%|▉| 1249/1251 [02:38<00:0\n",
      "epoch:26,batch:1248,lr:0.00026,loss:0.1106,mean_loss:0.07,train_f1:0.976,valid_f1:0.9: 100%|▉| 1249/1251 [02:39<00:00, \n",
      "epoch:27,batch:1248,lr:0.00021,loss:0.0554,mean_loss:0.064,train_f1:0.979,valid_f1:0.898: 100%|▉| 1249/1251 [02:37<00:0\n",
      "epoch:28,batch:1248,lr:0.00017,loss:0.1475,mean_loss:0.059,train_f1:0.981,valid_f1:0.899: 100%|▉| 1249/1251 [02:37<00:0\n",
      "epoch:29,batch:1248,lr:0.00013,loss:0.0613,mean_loss:0.055,train_f1:0.982,valid_f1:0.898: 100%|▉| 1249/1251 [02:36<00:0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        MyTrain(max_epoch=30,random_seed=1).my_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4545fa0d",
   "metadata": {},
   "source": [
    "尝试使用双向GRU,看对结果是否会有提升"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5983022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def split_train_valid(csv_path,train_prec=0.75):\n",
    "    csv_data = pd.read_csv(csv_path,sep='\\t') \n",
    "    text_data = csv_data.text\n",
    "    label_data = csv_data.label\n",
    "    \n",
    "    mask = np.random.rand(len(text_data))<train_prec\n",
    "    train_text_data = text_data[mask].reset_index(drop=True)\n",
    "    valid_text_data = text_data[~mask].reset_index(drop=True)\n",
    "    train_label_data = label_data[mask].reset_index(drop=True)\n",
    "    valid_label_data = label_data[~mask].reset_index(drop=True)\n",
    "    \n",
    "    return train_text_data,valid_text_data,train_label_data,valid_label_data\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self,text_data,label_data,word_num):\n",
    "        self.text_data = text_data\n",
    "        self.label_data = label_data\n",
    "        self.word_num = word_num\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        #所有text内的token索引增加1，0空出来代表空格，将每个text控制在一定长度内\n",
    "        text_str = self.text_data[index]\n",
    "        text_list = [int(x)+1 for x in text_str.split()]\n",
    "        if len(text_list)>=self.word_num:\n",
    "            text_list = text_list[:self.word_num]\n",
    "        else:\n",
    "            text_list.extend([0]*(self.word_num-len(text_list)))\n",
    "        text_array = np.array(text_list)\n",
    "        label_array = np.array(self.label_data[index])\n",
    "        return text_array,label_array\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text_data)\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(7551,101)\n",
    "        self.rnn = nn.GRU(101,50,batch_first=True,bidirectional=True)\n",
    "        self.fc = nn.Linear(100,14)\n",
    "        \n",
    "    def forward(self,X):\n",
    "#         print('X:',X.shape)\n",
    "        X = self.embedding(X)\n",
    "#         print('embedding:',X.shape)\n",
    "        _,X = self.rnn(X)\n",
    "#         print('rnn:',X.shape)\n",
    "        X = X.permute(1,0,2)\n",
    "        X = X.reshape((X.shape[0],-1))\n",
    "#         print('trans:',X.shape)\n",
    "        y_hat = self.fc(X)\n",
    "#         print('y_hat:',y_hat.shape)\n",
    "        return y_hat\n",
    "\n",
    "    \n",
    "class MyTrain():\n",
    "    def __init__(self,max_epoch=1,random_seed=1,lr=0.001,out_dir='./',word_num= 2674):\n",
    "        self.max_epoch = max_epoch\n",
    "        self.random_seed = random_seed\n",
    "        self.lr = lr\n",
    "        self.out_dir = out_dir\n",
    "        self.iter = 0\n",
    "        self.word_num = word_num\n",
    "        \n",
    "    def fix_random(self):\n",
    "        import random\n",
    "        import numpy as np\n",
    "        import torch\n",
    "        random.seed(self.random_seed)\n",
    "        np.random.seed(self.random_seed)\n",
    "        torch.random.manual_seed(self.random_seed)\n",
    "        torch.cuda.random.manual_seed_all(self.random_seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        print(f'random seed:{self.random_seed}')\n",
    "        \n",
    "    def my_train(self):\n",
    "        \n",
    "        max_epoch,lr = self.max_epoch,self.lr\n",
    "        \n",
    "        if self.random_seed is not None:\n",
    "            self.fix_random()\n",
    "        \n",
    "        train_text_data,valid_text_data,train_label_data,valid_label_data = split_train_valid('./train_set.csv',train_prec=0.75)\n",
    "        train_dataset = MyDataset(train_text_data,train_label_data,self.word_num)\n",
    "        valid_dataset = MyDataset(valid_text_data,valid_label_data,self.word_num)\n",
    "        \n",
    "        my_model = MyModel()\n",
    "        my_optim = torch.optim.Adam(my_model.parameters(),lr=lr)\n",
    "        my_loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            my_model.cuda()\n",
    "            my_loss.cuda()\n",
    "        print(f'train device:{next(iter(my_model.parameters())).device}')  #显示训练设备\n",
    "        \n",
    "        best_f1_score = 0\n",
    "        epoch_index = 0\n",
    "\n",
    "        for epoch_index in range(max_epoch):\n",
    "            \n",
    "            loss_list = []\n",
    "            train_f1_score_list = []\n",
    "            valid_f1_score_list = []\n",
    "            train_dataloader = DataLoader(train_dataset,batch_size=60,shuffle=True)\n",
    "            valid_dataloader = DataLoader(valid_dataset,batch_size=20,shuffle=True)\n",
    "            my_dataloader = tqdm(train_dataloader)\n",
    "            \n",
    "            batch_index = 0\n",
    "                        \n",
    "            for train_data,valid_data in zip(my_dataloader,valid_dataloader):\n",
    "                                \n",
    "                my_model.train()  #将模型设置为训练模式\n",
    "                train_text,train_label = train_data\n",
    "                valid_text,valid_label = valid_data\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    train_text = train_text.cuda()\n",
    "                    train_label = train_label.cuda()\n",
    "                    valid_text = valid_text.cuda()\n",
    "                    valid_label = valid_label.cuda()\n",
    "                \n",
    "                train_y_hat = my_model(train_text)\n",
    "                batch_train_loss = my_loss(train_y_hat,train_label)\n",
    "                \n",
    "                my_optim.zero_grad()\n",
    "                batch_train_loss.backward()\n",
    "                my_optim.step()\n",
    "                my_optim.param_groups[0]['lr'] = lr*(0.8**(epoch_index%10))\n",
    "        \n",
    "                my_model.eval()  #将模型设置为验证模式\n",
    "                with torch.no_grad():\n",
    "                    valid_y_hat = my_model(valid_text)\n",
    "                    batch_valid_f1_score = self.f1_score(valid_y_hat.data,valid_label.data)\n",
    "                    valid_f1_score_list.append(batch_valid_f1_score)\n",
    "                    mean_valid_f1 = round(sum(valid_f1_score_list)/len(valid_f1_score_list),3)\n",
    "\n",
    "                    #显示batch结果\n",
    "                    batch_lr = round(my_optim.param_groups[0]['lr'],5)\n",
    "                    batch_loss = round(batch_train_loss.item(),4)\n",
    "                    loss_list.append(batch_loss)\n",
    "                    mean_loss = round((sum(loss_list)/len(loss_list)),3)\n",
    "\n",
    "                    batch_train_f1_score = self.f1_score(train_y_hat.data,train_label.data)\n",
    "                    train_f1_score_list.append(batch_train_f1_score)\n",
    "                    mean_train_f1 = round(sum(train_f1_score_list)/len(train_f1_score_list),3)\n",
    "\n",
    "                    my_dataloader.set_description(f'epoch:{epoch_index},batch:{batch_index},lr:{batch_lr},loss:{batch_loss},mean_loss:{mean_loss},train_f1:{mean_train_f1},valid_f1:{mean_valid_f1}')\n",
    "\n",
    "                    #存储模型\n",
    "#                     if batch_valid_f1_score>0.96:\n",
    "#                         torch.save(my_model.state_dict(),os.path.join(self.out_dir,f'embedding_gru_best_{self.word_num}word_valid_f1_score_{round(batch_valid_f1_score,4)}'))\n",
    "#                         best_f1_score = batch_train_f1_score\n",
    "\n",
    "                    batch_index+=1\n",
    "            torch.save(my_model.state_dict(),os.path.join(self.out_dir,f'embedding_gru_best_{self.word_num}word_valid_f1_score_{round(batch_valid_f1_score,4)}'))\n",
    "\n",
    "            \n",
    "                \n",
    "    def f1_score(self,y_hat,label,eps=1e-8):\n",
    "        #y_hat(N,C),label(1)\n",
    "        y_hat = y_hat.cpu()\n",
    "        label = label.cpu()\n",
    "        preds_list = list(torch.argmax(y_hat,dim=1).numpy())\n",
    "        label_list = list(label.numpy())\n",
    "#         print(f'preds:{preds_list},label:{label_list}')\n",
    "        class_index_list = []\n",
    "        for class_index in label_list:\n",
    "            if class_index not in class_index_list:\n",
    "                class_index_list.append(class_index)\n",
    "\n",
    "        f1_score_list = []\n",
    "        for index in class_index_list:\n",
    "            if index not in preds_list:\n",
    "                sub_f1_score = 0\n",
    "            else:\n",
    "                tp = 0\n",
    "                fp = 0\n",
    "                fn = 0\n",
    "                for i in range(len(preds_list)):\n",
    "                    if preds_list[i] == index and label_list[i] == index:\n",
    "                        tp+=1\n",
    "                    if preds_list[i] == index and label_list[i] != index: \n",
    "                        fp+=1\n",
    "                    if preds_list[i] != index and label_list[i] == index:\n",
    "                        fn+=1\n",
    "                prec_val = tp/(tp+fp) \n",
    "                recall_val = tp/(tp+fn)\n",
    "                sub_f1_score = 2*(prec_val*recall_val)/(prec_val+recall_val+eps)\n",
    "            f1_score_list.append(sub_f1_score)\n",
    "\n",
    "        batch_f1_score = sum(f1_score_list)/len(f1_score_list)\n",
    "\n",
    "        return batch_f1_score\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54f53dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random seed:1\n",
      "train device:cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:0,batch:2496,lr:0.001,loss:0.2769,mean_loss:0.729,train_f1:0.635,valid_f1:0.685: 100%|▉| 2497/2502 [04:43<00:00, \n",
      "epoch:1,batch:2496,lr:0.0008,loss:0.2035,mean_loss:0.249,train_f1:0.889,valid_f1:0.898: 100%|▉| 2497/2502 [04:43<00:00,\n",
      "epoch:2,batch:2496,lr:0.00064,loss:0.3138,mean_loss:0.184,train_f1:0.922,valid_f1:0.918: 100%|▉| 2497/2502 [04:43<00:00\n",
      "epoch:3,batch:2496,lr:0.00051,loss:0.065,mean_loss:0.149,train_f1:0.937,valid_f1:0.922: 100%|▉| 2497/2502 [04:43<00:00,\n",
      "epoch:4,batch:2496,lr:0.00041,loss:0.0498,mean_loss:0.126,train_f1:0.947,valid_f1:0.927: 100%|▉| 2497/2502 [04:43<00:00\n",
      "epoch:5,batch:2496,lr:0.00033,loss:0.0228,mean_loss:0.108,train_f1:0.956,valid_f1:0.929: 100%|▉| 2497/2502 [04:37<00:00\n",
      "epoch:6,batch:2496,lr:0.00026,loss:0.1217,mean_loss:0.095,train_f1:0.963,valid_f1:0.928: 100%|▉| 2497/2502 [04:35<00:00\n",
      "epoch:7,batch:2496,lr:0.00021,loss:0.0343,mean_loss:0.084,train_f1:0.967,valid_f1:0.928: 100%|▉| 2497/2502 [04:34<00:00\n",
      "epoch:8,batch:2496,lr:0.00017,loss:0.103,mean_loss:0.075,train_f1:0.971,valid_f1:0.929: 100%|▉| 2497/2502 [04:35<00:00,\n",
      "epoch:9,batch:2496,lr:0.00013,loss:0.0117,mean_loss:0.069,train_f1:0.974,valid_f1:0.928: 100%|▉| 2497/2502 [04:35<00:00\n"
     ]
    }
   ],
   "source": [
    "   if __name__ == '__main__':\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        MyTrain(max_epoch=10,random_seed=1,word_num=2674).my_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21024f6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
