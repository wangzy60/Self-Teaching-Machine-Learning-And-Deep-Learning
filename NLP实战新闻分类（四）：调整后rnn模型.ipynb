{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5983022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def split_train_valid(csv_path,train_prec=0.75):\n",
    "    csv_data = pd.read_csv(csv_path,sep='\\t') \n",
    "    text_data = csv_data.text\n",
    "    label_data = csv_data.label\n",
    "    \n",
    "    mask = np.random.rand(len(text_data))<train_prec\n",
    "    train_text_data = text_data[mask].reset_index(drop=True)\n",
    "    valid_text_data = text_data[~mask].reset_index(drop=True)\n",
    "    train_label_data = label_data[mask].reset_index(drop=True)\n",
    "    valid_label_data = label_data[~mask].reset_index(drop=True)\n",
    "    \n",
    "    return train_text_data,valid_text_data,train_label_data,valid_label_data\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self,text_data,label_data,word_num):\n",
    "        self.text_data = text_data\n",
    "        self.label_data = label_data\n",
    "        self.word_num = word_num\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        #所有text内的token索引增加1，0空出来代表空格，将每个text控制在一定长度内\n",
    "        text_str = self.text_data[index]\n",
    "        text_list = [int(x)+1 for x in text_str.split()]\n",
    "        if len(text_list)>=self.word_num:\n",
    "            text_list = text_list[:self.word_num]\n",
    "        else:\n",
    "            text_list.extend([0]*(self.word_num-len(text_list)))\n",
    "        text_array = np.array(text_list)\n",
    "        label_array = np.array(self.label_data[index])\n",
    "        return text_array,label_array\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text_data)\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(7551,101)\n",
    "        self.rnn = nn.GRU(101,50,batch_first=True,bidirectional=True)\n",
    "        self.fc = nn.Linear(100,14)\n",
    "        \n",
    "    def forward(self,X):\n",
    "#         print('X:',X.shape)\n",
    "        X = self.embedding(X)\n",
    "#         print('embedding:',X.shape)\n",
    "        _,X = self.rnn(X)\n",
    "#         print('rnn:',X.shape)\n",
    "        X = X.permute(1,0,2)\n",
    "        X = X.reshape((X.shape[0],-1))\n",
    "#         print('trans:',X.shape)\n",
    "        y_hat = self.fc(X)\n",
    "#         print('y_hat:',y_hat.shape)\n",
    "        return y_hat\n",
    "\n",
    "    \n",
    "class MyTrain():\n",
    "    def __init__(self,max_epoch=1,random_seed=1,lr=0.001,out_dir='./',word_num= 2674):\n",
    "        self.max_epoch = max_epoch\n",
    "        self.random_seed = random_seed\n",
    "        self.lr = lr\n",
    "        self.out_dir = out_dir\n",
    "        self.iter = 0\n",
    "        self.word_num = word_num\n",
    "        \n",
    "    def fix_random(self):\n",
    "        import random\n",
    "        import numpy as np\n",
    "        import torch\n",
    "        random.seed(self.random_seed)\n",
    "        np.random.seed(self.random_seed)\n",
    "        torch.random.manual_seed(self.random_seed)\n",
    "        torch.cuda.random.manual_seed_all(self.random_seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        print(f'random seed:{self.random_seed}')\n",
    "        \n",
    "    def my_train(self):\n",
    "        \n",
    "        max_epoch,lr = self.max_epoch,self.lr\n",
    "        \n",
    "        if self.random_seed is not None:\n",
    "            self.fix_random()\n",
    "        \n",
    "        train_text_data,valid_text_data,train_label_data,valid_label_data = split_train_valid('./train_set.csv',train_prec=0.75)\n",
    "        train_dataset = MyDataset(train_text_data,train_label_data,self.word_num)\n",
    "        valid_dataset = MyDataset(valid_text_data,valid_label_data,self.word_num)\n",
    "        \n",
    "        my_model = MyModel()\n",
    "        my_optim = torch.optim.Adam(my_model.parameters(),lr=lr)\n",
    "        my_loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            my_model.cuda()\n",
    "            my_loss.cuda()\n",
    "        print(f'train device:{next(iter(my_model.parameters())).device}')  #显示训练设备\n",
    "        \n",
    "        best_f1_score = 0\n",
    "        epoch_index = 0\n",
    "\n",
    "        for epoch_index in range(max_epoch):\n",
    "            \n",
    "            loss_list = []\n",
    "            train_f1_score_list = []\n",
    "            valid_f1_score_list = []\n",
    "            train_dataloader = DataLoader(train_dataset,batch_size=60,shuffle=True)\n",
    "            valid_dataloader = DataLoader(valid_dataset,batch_size=20,shuffle=True)\n",
    "            my_dataloader = tqdm(train_dataloader)\n",
    "            \n",
    "            batch_index = 0\n",
    "                        \n",
    "            for train_data,valid_data in zip(my_dataloader,valid_dataloader):\n",
    "                                \n",
    "                my_model.train()  #将模型设置为训练模式\n",
    "                train_text,train_label = train_data\n",
    "                valid_text,valid_label = valid_data\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    train_text = train_text.cuda()\n",
    "                    train_label = train_label.cuda()\n",
    "                    valid_text = valid_text.cuda()\n",
    "                    valid_label = valid_label.cuda()\n",
    "                \n",
    "                train_y_hat = my_model(train_text)\n",
    "                batch_train_loss = my_loss(train_y_hat,train_label)\n",
    "                \n",
    "                my_optim.zero_grad()\n",
    "                batch_train_loss.backward()\n",
    "                my_optim.step()\n",
    "                my_optim.param_groups[0]['lr'] = lr*(0.8**(epoch_index%10))\n",
    "        \n",
    "                my_model.eval()  #将模型设置为验证模式\n",
    "                with torch.no_grad():\n",
    "                    valid_y_hat = my_model(valid_text)\n",
    "                    batch_valid_f1_score = self.f1_score(valid_y_hat.data,valid_label.data)\n",
    "                    valid_f1_score_list.append(batch_valid_f1_score)\n",
    "                    mean_valid_f1 = round(sum(valid_f1_score_list)/len(valid_f1_score_list),3)\n",
    "\n",
    "                    #显示batch结果\n",
    "                    batch_lr = round(my_optim.param_groups[0]['lr'],5)\n",
    "                    batch_loss = round(batch_train_loss.item(),4)\n",
    "                    loss_list.append(batch_loss)\n",
    "                    mean_loss = round((sum(loss_list)/len(loss_list)),3)\n",
    "\n",
    "                    batch_train_f1_score = self.f1_score(train_y_hat.data,train_label.data)\n",
    "                    train_f1_score_list.append(batch_train_f1_score)\n",
    "                    mean_train_f1 = round(sum(train_f1_score_list)/len(train_f1_score_list),3)\n",
    "\n",
    "                    my_dataloader.set_description(\n",
    "                        f'epoch:{epoch_index},batch:{batch_index},lr:{batch_lr},loss:{batch_loss},mean_loss:{mean_loss},train_f1:{mean_train_f1},valid_f1:{mean_valid_f1}')\n",
    "                    batch_index+=1\n",
    "                    \n",
    "            torch.save(my_model.state_dict(),os.path.join(\n",
    "                self.out_dir,\n",
    "                f'embedding_gru_best_{self.word_num}word_valid_f1_score_{round(mean_valid_f1,4)}'))\n",
    "        \n",
    "        return my_model\n",
    "        \n",
    "    def f1_score(self,y_hat,label,eps=1e-8):\n",
    "        #y_hat(N,C),label(1)\n",
    "        y_hat = y_hat.cpu()\n",
    "        label = label.cpu()\n",
    "        preds_list = list(torch.argmax(y_hat,dim=1).numpy())\n",
    "        label_list = list(label.numpy())\n",
    "#         print(f'preds:{preds_list},label:{label_list}')\n",
    "        class_index_list = []\n",
    "        for class_index in label_list:\n",
    "            if class_index not in class_index_list:\n",
    "                class_index_list.append(class_index)\n",
    "\n",
    "        f1_score_list = []\n",
    "        for index in class_index_list:\n",
    "            if index not in preds_list:\n",
    "                sub_f1_score = 0\n",
    "            else:\n",
    "                tp = 0\n",
    "                fp = 0\n",
    "                fn = 0\n",
    "                for i in range(len(preds_list)):\n",
    "                    if preds_list[i] == index and label_list[i] == index:\n",
    "                        tp+=1\n",
    "                    if preds_list[i] == index and label_list[i] != index: \n",
    "                        fp+=1\n",
    "                    if preds_list[i] != index and label_list[i] == index:\n",
    "                        fn+=1\n",
    "                prec_val = tp/(tp+fp) \n",
    "                recall_val = tp/(tp+fn)\n",
    "                sub_f1_score = 2*(prec_val*recall_val)/(prec_val+recall_val+eps)\n",
    "            f1_score_list.append(sub_f1_score)\n",
    "\n",
    "        batch_f1_score = sum(f1_score_list)/len(f1_score_list)\n",
    "\n",
    "        return batch_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54f53dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random seed:1\n",
      "train device:cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:0,batch:2496,lr:0.001,loss:0.2769,mean_loss:0.729,train_f1:0.635,valid_f1:0.685: 100%|▉| 2497/2502 [04:49<00:00, \n",
      "epoch:1,batch:2496,lr:0.0008,loss:0.2035,mean_loss:0.249,train_f1:0.889,valid_f1:0.898: 100%|▉| 2497/2502 [04:46<00:00,\n",
      "epoch:2,batch:2496,lr:0.00064,loss:0.3138,mean_loss:0.184,train_f1:0.922,valid_f1:0.918: 100%|▉| 2497/2502 [04:46<00:00\n",
      "epoch:3,batch:2496,lr:0.00051,loss:0.065,mean_loss:0.149,train_f1:0.937,valid_f1:0.922: 100%|▉| 2497/2502 [04:44<00:00,\n",
      "epoch:4,batch:2496,lr:0.00041,loss:0.0498,mean_loss:0.126,train_f1:0.947,valid_f1:0.927: 100%|▉| 2497/2502 [04:45<00:00\n",
      "epoch:5,batch:2496,lr:0.00033,loss:0.0228,mean_loss:0.108,train_f1:0.956,valid_f1:0.929: 100%|▉| 2497/2502 [04:46<00:00\n",
      "epoch:6,batch:2496,lr:0.00026,loss:0.1217,mean_loss:0.095,train_f1:0.963,valid_f1:0.928: 100%|▉| 2497/2502 [04:40<00:00\n",
      "epoch:7,batch:2496,lr:0.00021,loss:0.0343,mean_loss:0.084,train_f1:0.967,valid_f1:0.928: 100%|▉| 2497/2502 [04:44<00:00\n",
      "epoch:8,batch:2496,lr:0.00017,loss:0.103,mean_loss:0.075,train_f1:0.971,valid_f1:0.929: 100%|▉| 2497/2502 [04:43<00:00,\n",
      "epoch:9,batch:2496,lr:0.00013,loss:0.0117,mean_loss:0.069,train_f1:0.974,valid_f1:0.928: 100%|▉| 2497/2502 [04:44<00:00\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    rnn_train = MyTrain(max_epoch=10,random_seed=1,word_num=2674)\n",
    "    my_model = rnn_train.my_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dac6203",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
