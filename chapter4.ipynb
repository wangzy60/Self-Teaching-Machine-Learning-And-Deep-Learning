{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "885fd8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "247fea46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    '''\n",
    "    一个带有偏置的全链接层，偏置默认是零\n",
    "    '''\n",
    "    def __init__(self,W,b=0):\n",
    "        \n",
    "        self.params = {}\n",
    "        self.params['W'] = W\n",
    "        self.params['b'] = b\n",
    "        self.grads = {}\n",
    "        self.grads['W'] = np.zeros_like(self.params['W'])\n",
    "        self.grads['b'] = np.zeros_like(self.params['b'])\n",
    "        \n",
    "    def forward(self,X):\n",
    "        \n",
    "        self.params['X'] = X\n",
    "        \n",
    "        return np.dot(X,self.params['W'])+self.params['b']\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        \n",
    "        self.grads['W'] = np.dot(dout,self.params['X'].T)\n",
    "        self.grads['b'] = np.sum(dout,axis=0)\n",
    "        self.grads['X'] = np.dot(dout,self.params['W'].T)\n",
    "        \n",
    "        return self.grads['X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d91930a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding():\n",
    "    '''\n",
    "    根据单词在语义库中的id，返回权重列表中对应的权重数据，不带偏置\n",
    "    '''\n",
    "    def __init__(self,W):\n",
    "        \n",
    "        self.params = {}\n",
    "        self.params['W'] = W\n",
    "        self.grads = {}\n",
    "        self.grads['W'] = np.zeros_like(self.params['W'])\n",
    "        \n",
    "    def forward(self,X):\n",
    "        \n",
    "        #为了计算方便，X代表的数据集需要以单词的id形式表达，而不是one hot形式，如[0,3,2,4,2,1]，而不是[[0,0,0,0,1],[0,0,1,0,0]……]。\n",
    "        \n",
    "        if type(X) == list :\n",
    "            self.params['X'] = X\n",
    "            return self.params['W'][X]\n",
    "        \n",
    "        else:\n",
    "            print('X代表的数据集需要以单词的id形式表达')\n",
    "\n",
    "    def backward(self,dout):\n",
    "        \n",
    "        #dout中的第n行数据，乘以1之后，是W中第data_id行的导数\n",
    "        for n,data_id in enumerate(self.params['X']):\n",
    "            self.grads['W'][data_id] += dout[n]*1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc889cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试FeatEmbedding()类\n",
    "if __name__ == '__main__':\n",
    "    W =np.random.randn(5,5)\n",
    "    fe1 = Embedding(W)\n",
    "    X = [0,3,1]\n",
    "    dout = np.random.rand(3,5)\n",
    "    print('fe1.forward(X):\\n',fe1.forward(X),'\\n')\n",
    "    fe1.backward(dout)\n",
    "    print('fe1.params:\\n',fe1.params,'\\n')\n",
    "    print('dout:\\n',dout,'\\n')\n",
    "    print('fe1.grads:\\n',fe1.grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e72d8e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid():\n",
    "    \n",
    "    '''\n",
    "    一个sigmoid层，为了防止正向传播数值太小结果为零的情况，默认添加一个1e-8的微小数\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,eps=1e-8):\n",
    "        \n",
    "        self.eps = eps\n",
    "        self.params = {}\n",
    "        self.grads = {}\n",
    "    \n",
    "    def forward(self,X):\n",
    "        \n",
    "        y = 1/(1+np.exp(-1*X)) + self.eps\n",
    "        \n",
    "        self.y = y\n",
    "                \n",
    "        return y\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        \n",
    "        self.grads['X'] = dout*self.y*(1-self.y)\n",
    "        \n",
    "        return self.grads['X']\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df4a127",
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试sigmoid类\n",
    "if __name__ == '__main__':\n",
    "    s= Sigmoid()\n",
    "    X = np.random.randn(3,5)\n",
    "    dout = np.random.randn(3,5)\n",
    "    print('X:\\n',X,'\\n')\n",
    "    print('dout:\\n',dout,'\\n')\n",
    "    print('s.forward(X):\\n',s.forward(X),'\\n')\n",
    "    print('s.backward(dout):\\n',s.backward(dout),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e391b812",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyError():\n",
    "    \n",
    "    '''\n",
    "    交叉熵损失函数\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.params = {}\n",
    "        self.grads = {}\n",
    "        \n",
    "    def forward(self,X,y):\n",
    "        \n",
    "        total_loss = np.sum(-y*np.log(X)) \n",
    "        self.y = y\n",
    "        self.params['X'] = X\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def backward(self,dout=1):\n",
    "        \n",
    "        self.grads['X'] = (-1/self.params['X'])*self.y*dout\n",
    "        \n",
    "        return self.grads['X']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e6dd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#交叉熵损失测试\n",
    "if __name__ == '__main__':\n",
    "    tl = CrossEntropyError()\n",
    "    a=[0,1]\n",
    "    X = 0.5*np.random.choice(a,size=(10))+0.1\n",
    "    y = np.random.choice(a,size=(10))\n",
    "    print('X:\\n',X,'\\n')\n",
    "    print('y:\\n',y,'\\n')\n",
    "    print('tl.forward(X,y):\\n',tl.forward(X,y),'\\n')\n",
    "    print('tl.backward():\\n',tl.backward(),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "efc20204",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDot():\n",
    "    \n",
    "    '''\n",
    "    根据目标在语义库中的id，返回权重列表中对应的数据列，然后与对应的中间层神经元相乘后得到结果\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,W):\n",
    "        \n",
    "        #W表示的是输出层权重矩阵\n",
    "        \n",
    "        self.params = {}\n",
    "        self.params['W'] = W\n",
    "        self.grads = {}\n",
    "        self.grads['W'] = np.zeros_like(self.params['W'])\n",
    "        \n",
    "    def forward(self,X,y):\n",
    "        \n",
    "        #X代表的是CBOW模型中间层神经元的数值，y表示的是对应目标值的id，数据集需要以id形式表达成list，而不是one hot形式，如[0,3,2,4,2,1]，而不是[[0,0,0,0,1],[0,0,1,0,0]……]。\n",
    "        \n",
    "        if type(y) == list :\n",
    "            \n",
    "            self.params['X'] = X\n",
    "            self.y = y\n",
    "            \n",
    "            return np.sum(W.T[y]*X,axis=1)\n",
    "        \n",
    "        else:\n",
    "            print('目标数据集需要以列表的格式填入目标单词的id')\n",
    "\n",
    "    def backward(self,dout):\n",
    "        \n",
    "        #X的导数把\n",
    "        self.grads['X'] = dout.reshape(-1,1).repeat(len(self.params['W']),axis=1)*self.params['W'].T[self.y]\n",
    "        \n",
    "        #W的导数绝大部分是0，所以先把输入数据对应的权重列先找出来，然后再更新对应的权重，跟输入数据无权的权重不更新，还是0\n",
    "        for n,target_id in enumerate(self.y):\n",
    "            self.grads['W'][:,target_id] += dout[n]*X[n].T\n",
    "        \n",
    "        return self.grads['X']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abce7ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试EmbeddingDot()类\n",
    "if __name__ == '__main__':\n",
    "    W = np.random.randn(3,5)\n",
    "    X = np.random.randn(2,3)\n",
    "    y = [3,1]\n",
    "    dout = np.random.randn(2)\n",
    "    \n",
    "    print('W\\n',W,'\\n')\n",
    "    print('X\\n',X,'\\n')\n",
    "    print('y\\n',y,'\\n')\n",
    "    print('dout\\n',dout,'\\n')\n",
    "    \n",
    "    ed = EmbeddingDot(W)\n",
    "    print('ed.forward(X,y)\\n',ed.forward(X,y),'\\n')\n",
    "    print('ed.backward(dout)\\n',ed.backward(dout),'\\n')\n",
    "    print('dw:\\n',ed.grads['W'],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "21b6fc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  negative_sampling(corpus,co_matrix,word_to_id,id_to_word,postive_target,negative_sampling_size=2):\n",
    "    \n",
    "    '''\n",
    "    根据语料库、共现矩阵、已确定的正采样样本，需要负采样的个数，来生成包含正负采样的训练目标字典\n",
    "    '''\n",
    "    \n",
    "    p = np.sum(co_matrix,axis=0)/np.sum(co_matrix)  #根据共现矩阵计算每个单词的分布概率\n",
    "    p = np.power(p,0.75)/np.sum(np.power(p,0.75))   #将每个单词的概率通过0.75指数转换，以增大概率比较低的值的概率\n",
    "    max_id = max(corpus)+1  #找到语料库中单词的id的最大值+1，作为随机选取的对象\n",
    "    \n",
    "    negative_sampling = np.zeros((len(postive_target),negative_sampling_size))\n",
    "        \n",
    "    #分别对每一个正采样进行循环，随机选出一定数量的负采样样本id，且满足正负采样id不相同\n",
    "    for n,target_id in enumerate(postive_target):\n",
    "        go_on = False\n",
    "        while not go_on:\n",
    "            negative_id = np.random.choice(max_id,replace=False,size=negative_sampling_size,p=p)\n",
    "            if target_id not in negative_id:\n",
    "                go_on = True\n",
    "        negative_sampling[n] = negative_id\n",
    "    \n",
    "    return negative_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b629f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#negative_sampling()负采样功能测试\n",
    "if __name__ == '__main__':\n",
    "    from chapter2 import s2c,c2m\n",
    "    input_str = 'you say goodbye and i say hello.'\n",
    "    corpus = s2c(input_str)\n",
    "    word_to_id = corpus['word_to_id']\n",
    "    id_to_word = corpus['id_to_word']\n",
    "    co_matrix = c2m(corpus)\n",
    "    corpus = corpus['corpus']\n",
    "    postive_target = [1,2,3,4,5]\n",
    "    sampling_result = negative_sampling(corpus,co_matrix,word_to_id,id_to_word,postive_target,negative_sampling_size=2)\n",
    "    print(sampling_result.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f5d808e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creat_CBOW_train_data(corpus,word_to_id,id_to_word,window_size=1,out_format = 'one_hot'):\n",
    "    \n",
    "    '''\n",
    "    根据语料库，单词和id对应字典，id和单词对应字典，窗口大小，以CBOW模型，转换成包含feat和target两部分的train data\n",
    "    输出的格式可以选择'one-hot'和'id'\n",
    "    '''\n",
    "    \n",
    "    corpus_len = len(corpus)\n",
    "    word_to_id_len = len(word_to_id.items())\n",
    "    \n",
    "    if out_format == 'one_hot':\n",
    "        \n",
    "        train_feat = np.zeros((int(corpus_len-2*window_size),int(2*window_size),int(word_to_id_len)))\n",
    "        train_target = np.zeros((int(corpus_len-2*window_size),int(word_to_id_len)))\n",
    "        \n",
    "        for n,data_id in enumerate(corpus):                           #对于每一个语料库中的单词进行循环\n",
    "\n",
    "            if n-window_size >= 0 and n+window_size <= corpus_len-1:  #控制循环不超过语料库的边界\n",
    "\n",
    "                target_data = np.zeros(word_to_id_len)                  #生成train_target数据\n",
    "                target_data[data_id] =1\n",
    "                train_target[n-window_size] = target_data\n",
    "\n",
    "                for distance in range(1,window_size+1):                 #生成目标左边和右边的train_feat，一个train_feat包含多个数据,一个train_feat包含的数据数量是窗口大小的两倍\n",
    "\n",
    "                    left_data_index = n-distance\n",
    "                    right_data_index = n+distance\n",
    "\n",
    "                    left_data_id = corpus[left_data_index]\n",
    "                    right_data_id = corpus[right_data_index]\n",
    "\n",
    "                    left_data = np.zeros(word_to_id_len)\n",
    "                    left_data[left_data_id] = 1\n",
    "\n",
    "                    right_data = np.zeros(word_to_id_len)\n",
    "                    right_data[right_data_id] =1\n",
    "\n",
    "                    train_feat[n-window_size,window_size-distance] = left_data\n",
    "                    train_feat[n-window_size,window_size+distance-1] = right_data\n",
    "\n",
    "        return train_feat,train_target\n",
    "    \n",
    "    if out_format == 'id':\n",
    "        \n",
    "        train_feat = np.zeros((int(corpus_len-2*window_size),int(2*window_size)))\n",
    "        train_target = np.zeros(int(corpus_len-2*window_size))\n",
    "        \n",
    "        for n,data_id in enumerate(corpus):                           #对于每一个语料库中的单词进行循环\n",
    "\n",
    "            if n-window_size >= 0 and n+window_size <= corpus_len-1:  #控制循环不超过语料库的边界\n",
    "\n",
    "                train_target[n-window_size] = int(data_id)            #生成train_target数据\n",
    "\n",
    "                for distance in range(1,window_size+1):                 #生成目标左边和右边的train_feat，一个train_feat包含多个数据,一个train_feat包含的数据数量是窗口大小的两倍\n",
    "\n",
    "                    left_data_index = n-distance\n",
    "                    right_data_index = n+distance\n",
    "\n",
    "                    left_data_id = corpus[left_data_index]\n",
    "                    right_data_id = corpus[right_data_index]\n",
    "\n",
    "                    train_feat[n-window_size,window_size-distance] = left_data_id\n",
    "                    train_feat[n-window_size,window_size+distance-1] = right_data_id\n",
    "        \n",
    "        return train_feat,train_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "aea33edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def c2m(corpus,word_to_id,id_to_word,window_size=1):\n",
    "\n",
    "    '''\n",
    "    corpus to co-matrix\n",
    "    '''\n",
    "\n",
    "    co_matrix_dims = (len(word_to_id),len(word_to_id))\n",
    "    co_matrix = np.zeros(co_matrix_dims)\n",
    "    \n",
    "    for word_index,word_id in enumerate(corpus):\n",
    "        for applied_window_size in range(1,window_size+1):\n",
    "            \n",
    "            if word_index-applied_window_size >= 0:\n",
    "                left_word_index = word_index-applied_window_size\n",
    "                left_word_id = corpus[left_word_index]\n",
    "                co_matrix[word_id,left_word_id] +=1\n",
    "            \n",
    "            if word_index+applied_window_size <= len(corpus)-1:\n",
    "                right_word_index = word_index+applied_window_size\n",
    "                right_word_id = corpus[right_word_index]\n",
    "                co_matrix[word_id,right_word_id] +=1\n",
    "                \n",
    "    return np.array(co_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "105e8809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ptb_data(window_size=5,data_format = 'id',negative_sampling_size=4,batch_size=100):\n",
    "    import sys\n",
    "    import os\n",
    "    sys.path.append('..')\n",
    "    import pickle\n",
    "\n",
    "    dataset_dir = os.path.abspath('')\n",
    "    vocab_path = dataset_dir + '\\\\ptb' + '\\\\ptb.vocab.pkl'\n",
    "    save_path = dataset_dir + '\\\\ptb' + '\\\\ptb.train.npy'\n",
    "\n",
    "    with open(vocab_path, 'rb') as f:\n",
    "        word_to_id, id_to_word = pickle.load(f)\n",
    "        corpus = np.load(save_path)\n",
    "\n",
    "    co_matrix = c2m(corpus,word_to_id,id_to_word,window_size=window_size)\n",
    "    train_feat,train_postive_target = creat_CBOW_train_data(corpus,word_to_id,id_to_word,window_size=window_size,out_format = data_format)\n",
    "    train_negative_target = negative_sampling(corpus,co_matrix,word_to_id,id_to_word,train_postive_target,negative_sampling_size=negative_sampling_size)\n",
    "    \n",
    "    for i in range(int(len(train_postive_target)/batch_size)+1):\n",
    "        yield train_feat[batch_size*i:min(batch_size*(i+1),len(train_postive_target))],train_postive_target[batch_size*i:min(batch_size*(i+1),len(train_postive_target))],train_negative_target[batch_size*i:min(batch_size*(i+1),len(train_postive_target))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "92c218d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "down\n"
     ]
    }
   ],
   "source": [
    "#测试get_ptb_data\n",
    "if __name__ == '__main__':\n",
    "    batch_size_ptb_data = get_ptb_data()\n",
    "    train_feat,train_postive_target,train_negative_targe = next(batch_size_ptb_data)\n",
    "    print('down')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "00c26eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  2.,  3.,  4.,  6.,  7.,  8.,  9., 10.],\n",
       "       [ 1.,  2.,  3.,  4.,  5.,  7.,  8.,  9., 10., 11.],\n",
       "       [ 2.,  3.,  4.,  5.,  6.,  8.,  9., 10., 11., 12.],\n",
       "       [ 3.,  4.,  5.,  6.,  7.,  9., 10., 11., 12., 13.],\n",
       "       [ 4.,  5.,  6.,  7.,  8., 10., 11., 12., 13., 14.],\n",
       "       [ 5.,  6.,  7.,  8.,  9., 11., 12., 13., 14., 15.],\n",
       "       [ 6.,  7.,  8.,  9., 10., 12., 13., 14., 15., 16.],\n",
       "       [ 7.,  8.,  9., 10., 11., 13., 14., 15., 16., 17.],\n",
       "       [ 8.,  9., 10., 11., 12., 14., 15., 16., 17., 18.],\n",
       "       [ 9., 10., 11., 12., 13., 15., 16., 17., 18., 19.],\n",
       "       [10., 11., 12., 13., 14., 16., 17., 18., 19., 20.],\n",
       "       [11., 12., 13., 14., 15., 17., 18., 19., 20., 21.],\n",
       "       [12., 13., 14., 15., 16., 18., 19., 20., 21., 22.],\n",
       "       [13., 14., 15., 16., 17., 19., 20., 21., 22., 23.],\n",
       "       [14., 15., 16., 17., 18., 20., 21., 22., 23., 24.],\n",
       "       [15., 16., 17., 18., 19., 21., 22., 23., 24., 25.],\n",
       "       [16., 17., 18., 19., 20., 22., 23., 24., 25., 26.],\n",
       "       [17., 18., 19., 20., 21., 23., 24., 25., 26., 27.],\n",
       "       [18., 19., 20., 21., 22., 24., 25., 26., 27., 28.],\n",
       "       [19., 20., 21., 22., 23., 25., 26., 27., 28., 29.],\n",
       "       [20., 21., 22., 23., 24., 26., 27., 28., 29., 30.],\n",
       "       [21., 22., 23., 24., 25., 27., 28., 29., 30., 31.],\n",
       "       [22., 23., 24., 25., 26., 28., 29., 30., 31., 32.],\n",
       "       [23., 24., 25., 26., 27., 29., 30., 31., 32., 33.],\n",
       "       [24., 25., 26., 27., 28., 30., 31., 32., 33., 34.],\n",
       "       [25., 26., 27., 28., 29., 31., 32., 33., 34., 35.],\n",
       "       [26., 27., 28., 29., 30., 32., 33., 34., 35., 36.],\n",
       "       [27., 28., 29., 30., 31., 33., 34., 35., 36., 37.],\n",
       "       [28., 29., 30., 31., 32., 34., 35., 36., 37., 38.],\n",
       "       [29., 30., 31., 32., 33., 35., 36., 37., 38., 27.],\n",
       "       [30., 31., 32., 33., 34., 36., 37., 38., 27., 24.],\n",
       "       [31., 32., 33., 34., 35., 37., 38., 27., 24., 39.],\n",
       "       [32., 33., 34., 35., 36., 38., 27., 24., 39., 26.],\n",
       "       [33., 34., 35., 36., 37., 27., 24., 39., 26., 40.],\n",
       "       [34., 35., 36., 37., 38., 24., 39., 26., 40., 41.],\n",
       "       [35., 36., 37., 38., 27., 39., 26., 40., 41., 42.],\n",
       "       [36., 37., 38., 27., 24., 26., 40., 41., 42., 26.],\n",
       "       [37., 38., 27., 24., 39., 40., 41., 42., 26., 43.],\n",
       "       [38., 27., 24., 39., 26., 41., 42., 26., 43., 32.],\n",
       "       [27., 24., 39., 26., 40., 42., 26., 43., 32., 44.],\n",
       "       [24., 39., 26., 40., 41., 26., 43., 32., 44., 45.],\n",
       "       [39., 26., 40., 41., 42., 43., 32., 44., 45., 46.],\n",
       "       [26., 40., 41., 42., 26., 32., 44., 45., 46., 24.],\n",
       "       [40., 41., 42., 26., 43., 44., 45., 46., 24., 47.],\n",
       "       [41., 42., 26., 43., 32., 45., 46., 24., 47., 26.],\n",
       "       [42., 26., 43., 32., 44., 46., 24., 47., 26., 27.],\n",
       "       [26., 43., 32., 44., 45., 24., 47., 26., 27., 28.],\n",
       "       [43., 32., 44., 45., 46., 47., 26., 27., 28., 29.],\n",
       "       [32., 44., 45., 46., 24., 26., 27., 28., 29., 48.],\n",
       "       [44., 45., 46., 24., 47., 27., 28., 29., 48., 49.],\n",
       "       [45., 46., 24., 47., 26., 28., 29., 48., 49., 41.],\n",
       "       [46., 24., 47., 26., 27., 29., 48., 49., 41., 42.],\n",
       "       [24., 47., 26., 27., 28., 48., 49., 41., 42., 50.],\n",
       "       [47., 26., 27., 28., 29., 49., 41., 42., 50., 51.],\n",
       "       [26., 27., 28., 29., 48., 41., 42., 50., 51., 52.],\n",
       "       [27., 28., 29., 48., 49., 42., 50., 51., 52., 53.],\n",
       "       [28., 29., 48., 49., 41., 50., 51., 52., 53., 54.],\n",
       "       [29., 48., 49., 41., 42., 51., 52., 53., 54., 55.],\n",
       "       [48., 49., 41., 42., 50., 52., 53., 54., 55., 35.],\n",
       "       [49., 41., 42., 50., 51., 53., 54., 55., 35., 36.],\n",
       "       [41., 42., 50., 51., 52., 54., 55., 35., 36., 37.],\n",
       "       [42., 50., 51., 52., 53., 55., 35., 36., 37., 42.],\n",
       "       [50., 51., 52., 53., 54., 35., 36., 37., 42., 56.],\n",
       "       [51., 52., 53., 54., 55., 36., 37., 42., 56., 57.],\n",
       "       [52., 53., 54., 55., 35., 37., 42., 56., 57., 58.],\n",
       "       [53., 54., 55., 35., 36., 42., 56., 57., 58., 59.],\n",
       "       [54., 55., 35., 36., 37., 56., 57., 58., 59., 24.],\n",
       "       [55., 35., 36., 37., 42., 57., 58., 59., 24., 35.],\n",
       "       [35., 36., 37., 42., 56., 58., 59., 24., 35., 60.],\n",
       "       [36., 37., 42., 56., 57., 59., 24., 35., 60., 42.],\n",
       "       [37., 42., 56., 57., 58., 24., 35., 60., 42., 61.],\n",
       "       [42., 56., 57., 58., 59., 35., 60., 42., 61., 62.],\n",
       "       [56., 57., 58., 59., 24., 60., 42., 61., 62., 63.],\n",
       "       [57., 58., 59., 24., 35., 42., 61., 62., 63., 64.],\n",
       "       [58., 59., 24., 35., 60., 61., 62., 63., 64., 65.],\n",
       "       [59., 24., 35., 60., 42., 62., 63., 64., 65., 66.],\n",
       "       [24., 35., 60., 42., 61., 63., 64., 65., 66., 67.],\n",
       "       [35., 60., 42., 61., 62., 64., 65., 66., 67., 68.],\n",
       "       [60., 42., 61., 62., 63., 65., 66., 67., 68., 69.],\n",
       "       [42., 61., 62., 63., 64., 66., 67., 68., 69., 70.],\n",
       "       [61., 62., 63., 64., 65., 67., 68., 69., 70., 35.],\n",
       "       [62., 63., 64., 65., 66., 68., 69., 70., 35., 71.],\n",
       "       [63., 64., 65., 66., 67., 69., 70., 35., 71., 72.],\n",
       "       [64., 65., 66., 67., 68., 70., 35., 71., 72., 42.],\n",
       "       [65., 66., 67., 68., 69., 35., 71., 72., 42., 73.],\n",
       "       [66., 67., 68., 69., 70., 71., 72., 42., 73., 74.],\n",
       "       [67., 68., 69., 70., 35., 72., 42., 73., 74., 75.],\n",
       "       [68., 69., 70., 35., 71., 42., 73., 74., 75., 35.],\n",
       "       [69., 70., 35., 71., 72., 73., 74., 75., 35., 46.],\n",
       "       [70., 35., 71., 72., 42., 74., 75., 35., 46., 42.],\n",
       "       [35., 71., 72., 42., 73., 75., 35., 46., 42., 76.],\n",
       "       [71., 72., 42., 73., 74., 35., 46., 42., 76., 77.],\n",
       "       [72., 42., 73., 74., 75., 46., 42., 76., 77., 64.],\n",
       "       [42., 73., 74., 75., 35., 42., 76., 77., 64., 78.],\n",
       "       [73., 74., 75., 35., 46., 76., 77., 64., 78., 79.],\n",
       "       [74., 75., 35., 46., 42., 77., 64., 78., 79., 80.],\n",
       "       [75., 35., 46., 42., 76., 64., 78., 79., 80., 27.],\n",
       "       [35., 46., 42., 76., 77., 78., 79., 80., 27., 28.],\n",
       "       [46., 42., 76., 77., 64., 79., 80., 27., 28., 81.],\n",
       "       [42., 76., 77., 64., 78., 80., 27., 28., 81., 82.]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    train_feat[0,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8db1bcf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.,  6.,  7.,  8.,  9., 10., 11., 12., 13., 14., 15., 16., 17.,\n",
       "       18., 19., 20., 21., 22., 23., 24., 25., 26., 27., 28., 29., 30.,\n",
       "       31., 32., 33., 34., 35., 36., 37., 38., 27., 24., 39., 26., 40.,\n",
       "       41., 42., 26., 43., 32., 44., 45., 46., 24., 47., 26., 27., 28.,\n",
       "       29., 48., 49., 41., 42., 50., 51., 52., 53., 54., 55., 35., 36.,\n",
       "       37., 42., 56., 57., 58., 59., 24., 35., 60., 42., 61., 62., 63.,\n",
       "       64., 65., 66., 67., 68., 69., 70., 35., 71., 72., 42., 73., 74.,\n",
       "       75., 35., 46., 42., 76., 77., 64., 78., 79.])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_postive_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d5f174d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4703., 8878., 1332., 2017.],\n",
       "       [6652., 1159., 2936., 1705.],\n",
       "       [9993., 5072., 2393.,  590.],\n",
       "       [ 152.,   42., 2109., 9307.],\n",
       "       [  32.,  871.,   64.,  449.],\n",
       "       [ 493.,   27.,   26., 5608.],\n",
       "       [  42., 3397., 3012.,  241.],\n",
       "       [  24., 2978.,   80.,  566.],\n",
       "       [ 109., 1435., 2411., 3745.],\n",
       "       [  64., 4683.,   40.,  874.],\n",
       "       [8606., 2152., 1436., 3316.],\n",
       "       [  41.,  154.,   64.,  703.],\n",
       "       [ 100., 4098., 1442.,  532.],\n",
       "       [ 935.,  885.,   40.,  988.],\n",
       "       [ 468., 1722.,   26., 7639.],\n",
       "       [3181.,   32., 6091.,   34.],\n",
       "       [ 157., 6528., 4399., 2652.],\n",
       "       [ 154.,   24., 4434., 1154.],\n",
       "       [5871., 5027., 3495., 3680.],\n",
       "       [1887., 6219., 9281.,  948.],\n",
       "       [3910., 1763., 1200.,   35.],\n",
       "       [2866.,  200.,   37., 2629.],\n",
       "       [ 718.,  101., 1378.,   48.],\n",
       "       [ 181.,  806., 4794., 2303.],\n",
       "       [  24.,  225.,   32.,   95.],\n",
       "       [4626., 1085., 4228.,   98.],\n",
       "       [1217., 3433., 1106., 4278.],\n",
       "       [  27.,  229., 1077., 7683.],\n",
       "       [3466., 2549., 5950., 1035.],\n",
       "       [5651., 2621., 5502., 1876.],\n",
       "       [1819.,  337., 1079.,   64.],\n",
       "       [1890.,  109., 9718., 2640.],\n",
       "       [ 233.,  878.,  187., 1479.],\n",
       "       [3428.,  357., 3103.,  502.],\n",
       "       [2494., 1153.,  151., 3752.],\n",
       "       [4052., 1314.,   26., 2805.],\n",
       "       [ 101., 1408., 1139.,   34.],\n",
       "       [5602.,  251., 1907., 6240.],\n",
       "       [1578.,  169.,  231.,  751.],\n",
       "       [6291., 1469.,  237.,   42.],\n",
       "       [1339.,   78., 2720.,  476.],\n",
       "       [9235., 3444.,   24.,  636.],\n",
       "       [1162.,  468.,  119., 7479.],\n",
       "       [ 443.,  277.,  154.,  711.],\n",
       "       [2584., 9887., 4294., 3085.],\n",
       "       [ 141., 1760., 6355., 5016.],\n",
       "       [  35., 2042.,   27., 1151.],\n",
       "       [2302.,  199.,   48.,  458.],\n",
       "       [  93., 4727.,  131., 4078.],\n",
       "       [ 101., 2339.,  896., 8226.],\n",
       "       [ 504., 7893.,   64.,  488.],\n",
       "       [6979.,  154.,  352.,  911.],\n",
       "       [1211., 4809., 4943.,  863.],\n",
       "       [1471., 1695., 7627., 3365.],\n",
       "       [ 230., 6100., 1914., 4606.],\n",
       "       [1817., 3725.,  114., 1123.],\n",
       "       [ 275., 6324.,  256.,  718.],\n",
       "       [4056.,  489., 2539.,  740.],\n",
       "       [1578., 5073., 5125., 9966.],\n",
       "       [1506.,   42., 3317.,  266.],\n",
       "       [3677., 3216., 5907., 1040.],\n",
       "       [ 243.,  874., 1123.,  284.],\n",
       "       [ 409.,  324., 2470.,   93.],\n",
       "       [ 108., 1487.,  908.,  169.],\n",
       "       [2757., 3962., 2073., 1644.],\n",
       "       [1860., 4269., 6187., 3150.],\n",
       "       [ 551., 3090., 5850.,  154.],\n",
       "       [1400., 1055., 4364., 4935.],\n",
       "       [  24., 7633., 8482., 2574.],\n",
       "       [  32.,  245., 7372.,  302.],\n",
       "       [2680.,   35., 1876., 9427.],\n",
       "       [5129., 7591., 2472., 5691.],\n",
       "       [ 817., 8164., 1119., 7546.],\n",
       "       [ 990.,  315.,  108.,  431.],\n",
       "       [ 335.,  109.,  757.,  198.],\n",
       "       [  42.,  119.,   54., 1195.],\n",
       "       [  51.,  247.,  154., 6299.],\n",
       "       [  79.,   42.,   32.,  817.],\n",
       "       [2229., 2479., 6329.,   24.],\n",
       "       [ 803., 2360., 2925., 5800.],\n",
       "       [9647., 1193.,   32.,  152.],\n",
       "       [  27., 1184.,   80., 6789.],\n",
       "       [  42., 7736.,  109.,   32.],\n",
       "       [  32., 9341.,   30., 1792.],\n",
       "       [ 130., 3161., 6237.,   24.],\n",
       "       [  38., 1769., 1081., 1800.],\n",
       "       [  27., 2010.,  119.,  307.],\n",
       "       [9704., 6229., 1068.,   28.],\n",
       "       [1617.,  468.,  849., 1919.],\n",
       "       [6190.,   32.,   48., 4661.],\n",
       "       [3843., 1811., 1119., 6845.],\n",
       "       [3679., 6248., 2302., 1050.],\n",
       "       [ 566., 4918.,   26.,  417.],\n",
       "       [  34.,  930.,  538., 2152.],\n",
       "       [1042., 4259., 1784., 2008.],\n",
       "       [ 370., 1260., 4912., 3638.],\n",
       "       [4670., 4467., 7239.,   64.],\n",
       "       [1085., 8834., 9941., 2862.],\n",
       "       [ 256.,  148., 2670., 9697.],\n",
       "       [  42., 1443.,  159., 1888.]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_negative_targe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433afd6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
