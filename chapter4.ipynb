{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d473114f",
   "metadata": {},
   "source": [
    "### 窗口大小为2的skip_gram模型结构\n",
    "##### Embedding     \n",
    "W1(len_of_co_matrix,hyperparams1),X(batch_size,1)  →  Y1(batch_size,hyperparams1)]    \n",
    "##### Embeddingdot1(left1/2/3/4 target 需要都为正或都为负)\n",
    "W2(hyperparams1,len_of_co_matrix),Y1(batch_size,hyperparams1),left1 target(batch_size,1)→Y2_1(batch_size,1)\n",
    "##### Embeddingdot2\n",
    "W4(hyperparams1,len_of_co_matrix),Y1(batch_size,hyperparams1),left2 target(batch_size,1)→Y2_2(batch_size,1)\n",
    "##### Embeddingdot3\n",
    "W3(hyperparams1,len_of_co_matrix),Y1(batch_size,hyperparams1),right1 target(batch_size,1)→Y2_3(batch_size,1)\n",
    "##### Embeddingdot4\n",
    "W5(hyperparams1,len_of_co_matrix),Y1(batch_size,hyperparams1),right2 target(batch_size,1)→Y2_4(batch_size,1)\n",
    "##### Sigmoid1\n",
    "Y2_1（batch_size,1）→S1（batch_size,1）\n",
    "##### Sigmoid2\n",
    "Y2_2（batch_size,1）→S2（batch_size,1）\n",
    "##### Sigmoid3\n",
    "Y2_3（batch_size,1）→S3（batch_size,1）\n",
    "##### Sigmoid4\n",
    "Y2_4（batch_size,1）→S4（batch_size,1）\n",
    "##### CrossEntropyLoss1\n",
    "S1（batch_size,1）,postive label/negative label（batch_size,1）→L1（1）\n",
    "##### CrossEntropyLoss2\n",
    "S2（batch_size,1）,postive label/negative label（batch_size,1）→L2（1）\n",
    "##### CrossEntropyLoss3\n",
    "S3（batch_size,1）,postive label/negative label（batch_size,1）→L3（1）\n",
    "##### CrossEntropyLoss4\n",
    "S4（batch_size,1）,postive label/negative label（batch_size,1）→L4（1）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "885fd8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d91930a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding():\n",
    "    '''\n",
    "    根据单词在语义库中的id，返回权重列表中对应的权重数据，不带偏置\n",
    "    '''\n",
    "    def __init__(self,W):\n",
    "        \n",
    "        self.params = {}\n",
    "        self.params['W'] = W\n",
    "        self.grads = {}\n",
    "        \n",
    "    def forward(self,X):\n",
    "        \n",
    "        #为了计算方便，X代表的数据集需要以单词的id形式表达，而不是one hot形式，如[0,3,2,4,2,1]，而不是[[0,0,0,0,1],[0,0,1,0,0]……]。\n",
    "        \n",
    "        if int(max(X)) > 1 :\n",
    "            X = [int(i) for i in X]\n",
    "            self.params['X'] = X\n",
    "            return self.params['W'][X]\n",
    "        \n",
    "        else:\n",
    "            print('X代表的数据集需要以单词的id形式表达')\n",
    "\n",
    "    def backward(self,dout):\n",
    "        \n",
    "        #生成导数之前先归零，避免循环过程中导数不断叠加\n",
    "        self.grads['W'] = np.zeros_like(self.params['W'])\n",
    "\n",
    "        #dout中的第n行数据，乘以1之后，是W中第data_id行的导数\n",
    "        for n,data_id in enumerate(self.params['X']):\n",
    "            \n",
    "            self.grads['W'][data_id] += dout[n]*1\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "efc20204",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDot():\n",
    "    \n",
    "    '''\n",
    "    根据目标在语义库中的id，返回权重列表中对应的数据列，然后与对应的中间层神经元相乘后得到结果\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,W):\n",
    "        \n",
    "        #W表示的是输出层权重矩阵\n",
    "        \n",
    "        self.params = {}\n",
    "        self.params['W'] = W\n",
    "        self.grads = {}\n",
    "        self.grads['W'] = np.zeros_like(self.params['W'])\n",
    "        \n",
    "    def forward(self,X,y_id):\n",
    "        \n",
    "        #X代表的是CBOW模型中间层神经元的数值，y_id表示的是对应目标值的id，数据集需要以id形式表达成list，而不是one hot形式，如[0,3,2,4,2,1]，而不是[[0,0,0,0,1],[0,0,1,0,0]……]。\n",
    "        if len(y_id) != 0 and len(X) != 0:\n",
    "\n",
    "            y_id = [int(i) for i in y_id]\n",
    "            self.params['X'] = X\n",
    "            self.y = y_id\n",
    "            \n",
    "            return np.sum(self.params['W'].T[y_id]*X,axis=1)\n",
    "        \n",
    "        else:\n",
    "            print('数据集需要以列表的格式填入目标单词的id')\n",
    "\n",
    "    def backward(self,dout):\n",
    "        \n",
    "        #生成导数之前先归零，避免循环过程中导数不断叠加\n",
    "        self.grads['X'] = np.zeros_like(self.params['X'])\n",
    "        self.grads['W'] = np.zeros_like(self.params['W'])\n",
    "        \n",
    "        #X的导数\n",
    "        self.grads['X'] = dout.reshape(-1,1).repeat(len(self.params['W']),axis=1)*self.params['W'].T[self.y]\n",
    "        \n",
    "        #W的导数绝大部分是0，所以先把输入数据对应的权重列先找出来，然后再更新对应的权重，跟输入数据无权的权重不更新，还是0\n",
    "        for n,target_id in enumerate(self.y):\n",
    "            self.grads['W'][:,target_id] += dout[n] * (self.params['X'][n]).T\n",
    "        \n",
    "        return self.grads['X']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e72d8e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid():\n",
    "    \n",
    "    '''\n",
    "    一个sigmoid层，为了防止正向传播数值太小结果为零的情况，默认添加一个1e-8的微小数\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,eps=1e-8):\n",
    "        \n",
    "        self.eps = eps\n",
    "        self.params = {}\n",
    "        self.grads = {}\n",
    "    \n",
    "    def forward(self,X):\n",
    "        \n",
    "        y = 1/(1+np.exp(-1*X)) + self.eps\n",
    "        \n",
    "        self.y = y\n",
    "                \n",
    "        return y\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        \n",
    "        self.grads['X'] = dout*self.y*(1-self.y)\n",
    "        \n",
    "        return self.grads['X']\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e391b812",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss():\n",
    "    \n",
    "    '''\n",
    "    交叉熵损失函数\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.params = {}\n",
    "        self.grads = {}\n",
    "        \n",
    "    def forward(self,X,y):\n",
    "        \n",
    "        total_loss = np.sum(-y*np.log(X)) \n",
    "        self.y = y\n",
    "        self.params['X'] = X\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def backward(self,dout=1):\n",
    "        \n",
    "        self.grads['X'] = (-1/self.params['X'])*self.y*dout\n",
    "        \n",
    "        return self.grads['X']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dcb5e0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreproess():\n",
    "    \n",
    "    '''\n",
    "    根据corpus,word_to_id,id_to_word三个数据，生成可供skip—gram模型训练的完整数据，当不提供原始数据时，默认使用ptb数据集\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,window_size=2,data_format = 'id',negative_sampling_size=4,negative_sampling_params =0.75,module= 'cbow'):\n",
    "        \n",
    "        '''\n",
    "        data_format = 'id' or 'one_hot'\n",
    "        module= 'skip_gram' or 'cbow'\n",
    "        '''\n",
    "        \n",
    "        self.window_size = window_size\n",
    "        self.data_format = data_format\n",
    "        self.negative_sampling_size = negative_sampling_size\n",
    "        self.negative_sampling_params = negative_sampling_params\n",
    "        self.module = module\n",
    "        \n",
    "        self.corpus = None\n",
    "        self.word_to_id = None\n",
    "        self.id_to_word = None\n",
    "        self.co_matrix = None\n",
    "        \n",
    "        self.train_feat = None\n",
    "        self.train_postive_target = None\n",
    "        self.train_negative_target = None\n",
    "#         self.train_postive_target_with_label = None\n",
    "#         self.train_negative_target_with_label = None\n",
    "        self.train_data = None\n",
    "        \n",
    "    def get_ptb_data(self):\n",
    "        \n",
    "        '''\n",
    "        需要将当前文件与ptb文件夹放在同一目录下使用，以获得数据集的相关数据\n",
    "        '''\n",
    "        \n",
    "        #获取corpus,word_to_id,id_to_word数据\n",
    "        import sys\n",
    "        import os\n",
    "        sys.path.append('..')\n",
    "        import pickle\n",
    "\n",
    "        dataset_dir = os.path.abspath('')\n",
    "        vocab_path = dataset_dir + '\\\\ptb' + '\\\\ptb.vocab.pkl'\n",
    "        save_path = dataset_dir + '\\\\ptb' + '\\\\ptb.train.npy'\n",
    "\n",
    "        with open(vocab_path, 'rb') as f:\n",
    "            self.word_to_id, self.id_to_word = pickle.load(f)\n",
    "            self.corpus = np.load(save_path)\n",
    "        \n",
    "        print('ptb数据集的corpus,word_to_id,id_to_word数据已写入实例属性！')\n",
    "        \n",
    "        #获取 train_feat,train_postive_target_with_label,train_negative_target_with_label数据\n",
    "        \n",
    "        self.c2m()\n",
    "        self.postive_sampling()\n",
    "        self.negative_sampling()\n",
    "#         self.creat_label()\n",
    "        \n",
    "        #每个单词,包含(负采样数量+1)组训练数据，一组标签是正确的数据，另外几组标签是错误的数据。每组数据包含1个feat,window_size*2个target和 window_size*2个label\n",
    "        \n",
    "        if self.data_format == 'id' and self.module == 'skip_gram':\n",
    "            \n",
    "            #数据的总数量为feat*（正确采样数量+负采样数量），数据的宽度为feat+label+2*window_size\n",
    "            train_data_shape = (int(self.train_feat.shape[0]*(1+self.negative_sampling_size)),int(self.train_postive_target.shape[1]+2))\n",
    "            \n",
    "            self.train_data = np.zeros(train_data_shape)\n",
    "            \n",
    "            for n,feat_id in enumerate(self.train_feat):                              #对于语料库中的第n个单词\n",
    "                \n",
    "                self.train_data[5*n:5*(n+1),0] = self.train_feat[n]                   #往第n个单词的数据中添加feat\n",
    "                \n",
    "                self.train_data[5*n,1:-1] = self.train_postive_target[n]              #往第n个单词的数据中添加带label的postive_target数据\n",
    "                self.train_data[5*n,-1] = 1                                           #为postive_target添加label,真值的label为1\n",
    "                \n",
    "                self.train_data[5*n+1:5*(n+1),1:-1] = self.train_negative_target[n].T   #往第n个单词的数据中添加带label的negative_target数据\n",
    "                self.train_data[5*n+1:5*(n+1),-1] = 0                                   #为negative_target添加label,假值的label为0\n",
    "            \n",
    "            print('skip_gram模式下，id形式的训练数据生成完毕！')\n",
    "            \n",
    "            return\n",
    "        \n",
    "    def get_file_data(self,file_addr):\n",
    "        \n",
    "        '''\n",
    "        当数据集为自己设定的数据集时，使用这个功能获得数据集的corpus,word_to_id,id_to_word数据\n",
    "        '''\n",
    "        \n",
    "        with open(file_addr,'r') as f:\n",
    "            input_str = f.readlines()\n",
    "            \n",
    "        if len(input_str) == 0:\n",
    "            print('数据文件读取失败，请检查get_file_data()功能')\n",
    "            return\n",
    "        \n",
    "        input_str = str(input_str)\n",
    "        input_str = input_str.lower()\n",
    "        input_str = input_str.replace('.',' .')\n",
    "\n",
    "        words = input_str.split(' ')\n",
    "\n",
    "        word_to_id = {}\n",
    "        id_to_word = {}\n",
    "\n",
    "        for word in words:\n",
    "            if word not in word_to_id:\n",
    "                new_id = len(word_to_id)\n",
    "                word_to_id[word] = new_id\n",
    "                id_to_word[new_id] = word\n",
    "\n",
    "        corpus = [word_to_id[word] for word in words]\n",
    "        corpus = np.array(corpus)\n",
    "        \n",
    "        self.corpus = corpus\n",
    "        self.word_to_id = word_to_id\n",
    "        self.id_to_word = id_to_word\n",
    "\n",
    "        print('自选文件数据集的corpus,word_to_id,id_to_word数据已写入实例属性！')\n",
    "        \n",
    "        return \n",
    "    \n",
    "    def c2m(self):\n",
    "\n",
    "        '''\n",
    "        根据corpus数据生成co-matrix\n",
    "        '''\n",
    "        \n",
    "        if len(self.corpus) == 0 or len(self.word_to_id) == 0 or len(self.id_to_word) == 0:\n",
    "            print('corpus，word_to_id,id_to_word数据未写入，请先使用get_file_data()或get_ptb_data()功能写入以上三个数据')\n",
    "            return\n",
    "        \n",
    "        co_matrix_dims = (len(self.word_to_id),len(self.word_to_id))\n",
    "        co_matrix = np.zeros(co_matrix_dims)\n",
    "\n",
    "        for word_index,word_id in enumerate(self.corpus):\n",
    "            for applied_window_size in range(1,self.window_size+1):\n",
    "\n",
    "                if word_index-applied_window_size >= 0:\n",
    "                    left_word_index = word_index-applied_window_size\n",
    "                    left_word_id = self.corpus[left_word_index]\n",
    "                    co_matrix[word_id,left_word_id] +=1\n",
    "\n",
    "                if word_index+applied_window_size <= len(self.corpus)-1:\n",
    "                    right_word_index = word_index+applied_window_size\n",
    "                    right_word_id = self.corpus[right_word_index]\n",
    "                    co_matrix[word_id,right_word_id] +=1\n",
    "\n",
    "        self.co_matrix = co_matrix\n",
    "        \n",
    "        print('co_matrix创建完成！')\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def postive_sampling(self):\n",
    "\n",
    "        '''\n",
    "        根据语料库，单词和id对应字典，id和单词对应字典，窗口大小，默认以CBOW模型，转换成包含feat和postive target两部分的train data\n",
    "        输出的格式可以选择'one-hot'和'id'\n",
    "        '''\n",
    "        corpus_len = len(self.corpus)\n",
    "        word_to_id_len = len(self.word_to_id.items())\n",
    "\n",
    "        if self.data_format == 'one_hot':\n",
    "\n",
    "            train_feat = np.zeros((int(corpus_len-2*self.window_size),int(2*self.window_size),int(word_to_id_len)))\n",
    "            train_target = np.zeros((int(corpus_len-2*self.window_size),int(word_to_id_len)))\n",
    "\n",
    "            for n,data_id in enumerate(self.corpus):                           #对于每一个语料库中的单词进行循环\n",
    "\n",
    "                if n-self.window_size >= 0 and n+self.window_size <= corpus_len-1:  #控制循环不超过语料库的边界\n",
    "\n",
    "                    target_data = np.zeros(word_to_id_len)                  #生成train_target数据\n",
    "                    target_data[data_id] =1\n",
    "                    train_target[n-self.window_size] = target_data\n",
    "\n",
    "                    for distance in range(1,self.window_size+1):                 #生成目标左边和右边的train_feat，一个train_feat包含多个数据,一个train_feat包含的数据数量是窗口大小的两倍\n",
    "\n",
    "                        left_data_index = n-distance\n",
    "                        right_data_index = n+distance\n",
    "\n",
    "                        left_data_id = self.corpus[left_data_index]\n",
    "                        right_data_id = self.corpus[right_data_index]\n",
    "\n",
    "                        left_data = np.zeros(word_to_id_len)\n",
    "                        left_data[left_data_id] = 1\n",
    "\n",
    "                        right_data = np.zeros(word_to_id_len)\n",
    "                        right_data[right_data_id] =1\n",
    "\n",
    "                        train_feat[n-self.window_size,self.window_size-distance] = left_data\n",
    "                        train_feat[n-self.window_size,self.window_size+distance-1] = right_data\n",
    "\n",
    "            if self.module == 'cbow':\n",
    "                self.train_feat = train_feat\n",
    "                self.train_postive_target = train_target\n",
    "                print('基于cbow的正采样数据生成完毕！')\n",
    "            \n",
    "            if self.module == 'skip_gram':\n",
    "                self.train_feat = train_target\n",
    "                self.train_postive_target = train_feat \n",
    "                print('基于skip_gram的正采样数据生成完毕！')\n",
    "                \n",
    "            return \n",
    "\n",
    "        if self.data_format == 'id':\n",
    "\n",
    "            train_feat = np.zeros((int(corpus_len-2*self.window_size),int(2*self.window_size)))\n",
    "            train_target = np.zeros(int(corpus_len-2*self.window_size))\n",
    "\n",
    "            for n,data_id in enumerate(self.corpus):                           #对于每一个语料库中的单词进行循环\n",
    "\n",
    "                if n-self.window_size >= 0 and n+self.window_size <= corpus_len-1:  #控制循环不超过语料库的边界\n",
    "\n",
    "                    train_target[n-self.window_size] = int(data_id)            #生成train_target数据\n",
    "\n",
    "                    for distance in range(1,self.window_size+1):                 #生成目标左边和右边的train_feat，一个train_feat包含多个数据,一个train_feat包含的数据数量是窗口大小的两倍\n",
    "\n",
    "                        left_data_index = n-distance\n",
    "                        right_data_index = n+distance\n",
    "\n",
    "                        left_data_id = self.corpus[left_data_index]\n",
    "                        right_data_id = self.corpus[right_data_index]\n",
    "\n",
    "                        train_feat[n-self.window_size,self.window_size-distance] = left_data_id\n",
    "                        train_feat[n-self.window_size,self.window_size+distance-1] = right_data_id\n",
    "\n",
    "            if self.module == 'cbow':\n",
    "                self.train_feat = train_feat\n",
    "                self.train_postive_target = train_target\n",
    "                print('基于cbow的正采样数据生成完毕！')\n",
    "            \n",
    "            if self.module == 'skip_gram':\n",
    "                self.train_feat = train_target\n",
    "                self.train_postive_target = train_feat \n",
    "                print('基于skip_gram的正采样数据生成完毕！')\n",
    "                \n",
    "            return \n",
    "    \n",
    "    def  negative_sampling(self):\n",
    "\n",
    "        '''\n",
    "        根据语料库、共现矩阵、已确定的正采样样本，需要负采样的个数，来生成包负采样的训练数据\n",
    "        '''\n",
    "        \n",
    "        if len(self.train_postive_target) == 0:\n",
    "            print('正采样数据为空，请先获得正采样数据')\n",
    "            return\n",
    "\n",
    "        p = np.sum(self.co_matrix,axis=0)/np.sum(self.co_matrix)  #根据共现矩阵计算每个单词的分布概率\n",
    "        p = np.power(p,0.75)/np.sum(np.power(p,0.75))   #将每个单词的概率通过0.75指数转换，以增大概率比较低的值的概率\n",
    "        max_id = max(self.corpus)+1  #找到语料库中单词的id的最大值+1，作为随机选取的对象\n",
    "        \n",
    "        negative_sampling = np.zeros(self.train_postive_target.shape + (self.negative_sampling_size,))  \n",
    "\n",
    "        if self.module == 'cbow':   #cbow和skip_gram模式下，正采样数据的维度不一样，简单起见分开进行处理\n",
    "            \n",
    "            #分别对每一个正采样进行循环，随机选出一定数量的负采样样本id，且满足正负采样id不相同\n",
    "            for n,target_id in enumerate(self.train_postive_target):\n",
    "\n",
    "                go_on = False\n",
    "\n",
    "                while not go_on:\n",
    "                    negative_id = np.random.choice(max_id,replace=False,size=self.negative_sampling_size,p=p)\n",
    "                    if target_id not in negative_id:\n",
    "                        go_on = True\n",
    "\n",
    "                negative_sampling[n] = negative_id\n",
    "\n",
    "            self.train_negative_target = negative_sampling\n",
    "\n",
    "            print('cbow负采样数据生成完毕！')\n",
    "\n",
    "            return \n",
    "        \n",
    "        if self.module == 'skip_gram':  #skip_gram模式下正采样数据是二个维度的array，所以要遍历两层\n",
    "            \n",
    "            #分别对每一个正采样进行循环，随机选出一定数量的负采样样本id，且满足正负采样id不相同\n",
    "            for i in range(self.train_postive_target.shape[0]):\n",
    "                \n",
    "                for j in range(self.train_postive_target.shape[1]):\n",
    "                    \n",
    "                    target_id = self.train_postive_target[i,j]\n",
    "                    \n",
    "                    go_on = False\n",
    "\n",
    "                    while not go_on:\n",
    "                        \n",
    "                        negative_id = np.random.choice(max_id,replace=False,size=self.negative_sampling_size,p=p)\n",
    "                        if target_id not in negative_id:\n",
    "                            go_on = True\n",
    "\n",
    "                    negative_sampling[i,j] = negative_id\n",
    "            \n",
    "            self.train_negative_target = negative_sampling\n",
    "\n",
    "            print('skip_gram负采样数据生成完毕！')\n",
    "\n",
    "            return \n",
    "    \n",
    "#     def creat_label(self):\n",
    "        \n",
    "#         '''\n",
    "#         在正样本和后样本后面加上一列0/1的数值，表示其是正标签还是负标签，其中1表示正标签，0表示负标签\n",
    "#         '''\n",
    "#         if len(self.train_postive_target) == 0 or len(self.train_negative_target) == 0:\n",
    "            \n",
    "#             print('正样本或负样本数据不存在，请先写入正负样本')\n",
    "            \n",
    "#             return\n",
    "        \n",
    "#         if self.data_format == 'id' and self.module == 'skip_gram':\n",
    "            \n",
    "#             #提取正采样数据的形状（2维），在最后一维的尺寸上加1，作为包含标签和id的新数据的维度\n",
    "#             train_postive_target_with_label_shape = list(self.train_postive_target.shape)          \n",
    "#             train_postive_target_with_label_shape[-1] += 1\n",
    "#             train_postive_target_with_label_shape = tuple(train_postive_target_with_label_shape)\n",
    "            \n",
    "#             self.train_postive_target_with_label = np.zeros(train_postive_target_with_label_shape)\n",
    "            \n",
    "#             self.train_postive_target_with_label[:,:-1] = self.train_postive_target   #skip_gram下的正采样为二维数据，把正采样获得的id数据放入\n",
    "#             self.train_postive_target_with_label[:,-1] = 1                            #把标签数据放入第二个维度的最后一列，这里是正采样所有标签列放数字1\n",
    "            \n",
    "#             #提取负采样数据的形状（3维），在最后一维的尺寸上加1，作为包含标签和id的新数据的维度\n",
    "#             train_negative_target_with_label_shape = list(self.train_negative_target.shape)          \n",
    "#             train_negative_target_with_label_shape[-1] += 1\n",
    "#             train_negative_target_with_label_shape = tuple(train_negative_target_with_label_shape)\n",
    "            \n",
    "#             self.train_negative_target_with_label = np.zeros(train_negative_target_with_label_shape)\n",
    "            \n",
    "#             self.train_negative_target_with_label[:,:,:-1] = self.train_negative_target   #skip_gram下的负采样为三维数据，把负采样获得的id数据放入\n",
    "#             self.train_negative_target_with_label[:,:,-1] = 0                            #把标签数据放入第三个维度的最后一列，这里是负采样所有标签列放数字0\n",
    "            \n",
    "#             print('skip_gram模式下正负采样标签添加完成！')\n",
    "            \n",
    "#             return\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "169b792a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram():\n",
    "    \n",
    "    def __init__(self,corpus,word_to_id,id_to_word,co_matrix,weight_width=300,window_size=2,batch_size=100):\n",
    "        \n",
    "        from collections import OrderedDict\n",
    "        \n",
    "        self.corpus = corpus\n",
    "        self.word_to_id = word_to_id\n",
    "        self.id_to_word = id_to_word\n",
    "        self.co_matrix = co_matrix\n",
    "        self.weight_width = weight_width\n",
    "        self.batch_size = batch_size\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        self.layers = OrderedDict()\n",
    "        self.params = {}\n",
    "        \n",
    "        #生成第一个embedding层\n",
    "        self.params['W0'] = np.random.randn(len(co_matrix),weight_width)\n",
    "        self.layers['embedding1'] = Embedding(self.params['W0'])\n",
    "        \n",
    "        #从embedding_dot层开始，分成2*window_size条路线，每条路线都包含embedding_dot、sigmoid,crossentropyloss三个层\n",
    "        for i in range(2*self.window_size): \n",
    "            \n",
    "            W_name = 'W'+ str(i+1)\n",
    "            embedding_dot_layer_name = 'embedding_dot' + str(i+1)\n",
    "            sigmoid_layer_name = 'sigmoid' + str(i+1)\n",
    "            crossentropyloss_layer_name = 'cross_entropy_loss' + str(i+1)\n",
    "            \n",
    "            self.params[W_name] = np.random.randn(weight_width,len(co_matrix))\n",
    "            self.layers[embedding_dot_layer_name] = EmbeddingDot(self.params[W_name])\n",
    "                    \n",
    "            self.layers[sigmoid_layer_name] = Sigmoid()\n",
    "        \n",
    "            self.layers[crossentropyloss_layer_name] = CrossEntropyLoss()\n",
    "        \n",
    "    def skipgram_forward(self,X,y_id,y_label):\n",
    "        \n",
    "        #embedding层向前传播\n",
    "        Y1 = self.layers['embedding1'].forward(X)\n",
    "        \n",
    "        loss_list = []\n",
    "        \n",
    "        #剩下的层形成2*window_size个一样的分支\n",
    "        for i in range(2*self.window_size):\n",
    "            \n",
    "            embedding_dot_layer_name = 'embedding_dot' + str(i+1)\n",
    "            sigmoid_layer_name = 'sigmoid' + str(i+1)\n",
    "            crossentropyloss_layer_name = 'cross_entropy_loss' + str(i+1) \n",
    "            \n",
    "            Y2 = self.layers[embedding_dot_layer_name].forward(Y1,y_id[:,i])\n",
    "        \n",
    "            S1 = self.layers[sigmoid_layer_name].forward(Y2)\n",
    "\n",
    "            L1 = self.layers[crossentropyloss_layer_name].forward(S1,y_label)\n",
    "            \n",
    "            loss_list.append(L1)\n",
    "        \n",
    "        #最后返回2*window_size个分支的总和\n",
    "        return np.sum(loss_list)\n",
    "    \n",
    "    def skipgram_backward(self):\n",
    "        \n",
    "        #分别计算2*window_size个分支的导数，这2*window_size个分支的导数叠加汇总之后传给embedding层\n",
    "        \n",
    "        Y2_dout = np.zeros((self.batch_size,self.weight_width),dtype=np.float32)\n",
    "        \n",
    "        for i in range(2*self.window_size):\n",
    "            \n",
    "            embedding_dot_layer_name = 'embedding_dot' + str(i+1)\n",
    "            sigmoid_layer_name = 'sigmoid' + str(i+1)\n",
    "            crossentropyloss_layer_name = 'cross_entropy_loss' + str(i+1)\n",
    "        \n",
    "            L1_dout = self.layers[crossentropyloss_layer_name].backward()\n",
    "\n",
    "            S1_dout = self.layers[sigmoid_layer_name].backward(L1_dout)\n",
    "                                \n",
    "            Y2_dout += self.layers[embedding_dot_layer_name].backward(S1_dout)\n",
    "            \n",
    "        #将叠加汇总后的Y2_dout导数传给embedding层\n",
    "        \n",
    "        self.layers['embedding1'].backward(Y2_dout)\n",
    "                        \n",
    "    def skipgram_predict(self,X):\n",
    "        \n",
    "        '''\n",
    "        输入一个单词，预测下一个单词是什么\n",
    "        '''\n",
    "        \n",
    "        #当输入的预测数据是单词的编号时\n",
    "        if type(X) == int:\n",
    "            \n",
    "            word_id = X\n",
    "                        \n",
    "            try:\n",
    "                word_content = self.id_to_word[word_id]\n",
    "                        \n",
    "            except:\n",
    "                print('预测的单词不在词库中，请重新输入')\n",
    "                        \n",
    "                return\n",
    "\n",
    "        #当输入的预测数据是单词字符串时\n",
    "        if type(X) == str:\n",
    "            \n",
    "            try:\n",
    "                word_id =  self.word_to_id[X]\n",
    "            \n",
    "            except:\n",
    "                print('预测的单词不在词库中，请重新输入')\n",
    "                        \n",
    "                return\n",
    "                        \n",
    "        Y1 = self.layers['embedding1'].skipgram_forward(word_id)\n",
    "        \n",
    "        #这里准备预测的是下一个字是什么，所以用的是训练数据右边第一个字对应的embeddingdot层来预测\n",
    "        #预测的是y_id，这里跟求损失不同，需要让embedding的计算结果和embeddingdot的所有W进行矩阵点乘，然后找到结果中最大数值的编号，即为预测结果的id\n",
    "        embedding_dot_layer_name = 'embedding_dot' + str(self.window_size) \n",
    "        y_id = np.argmax(np.dot(Y1,self.layers[embedding_dot_layer_name].params['W']))\n",
    "                        \n",
    "        y_word = self.id_to_word[y_id]\n",
    "                        \n",
    "        return y_word\n",
    "        \n",
    "    def train(self,input_data,iter=500,lr=0.01):\n",
    "                        \n",
    "        import matplotlib.pyplot as plt\n",
    "                        \n",
    "        L_list = []\n",
    "        iter_index_list = []\n",
    "        \n",
    "        fig = plt.figure(figsize=(10,8))\n",
    "                        \n",
    "        #循环取出每个batch_size数据，原始数据ptb_dat的shape为(4647925, 6)\n",
    "        for i in range(iter):\n",
    "                        \n",
    "            train_index = np.random.choice(int(len(input_data)),self.batch_size,replace=False)\n",
    "            X = input_data[train_index][:,1]\n",
    "            y_id = input_data[train_index][:,1:-1]\n",
    "            y_label = input_data[train_index][:,-1]\n",
    "            \n",
    "            #计算loss,反向传播\n",
    "            loss = self.skipgram_forward(X,y_id,y_label)\n",
    "            self.skipgram_backward()\n",
    "            \n",
    "            #更新所有的W\n",
    "            self.layers['embedding1'].params['W'] -= lr*self.layers['embedding1'].grads['W']\n",
    "\n",
    "            for n in range(2*self.window_size): \n",
    "\n",
    "                embedding_dot_layer_name = 'embedding_dot' + str(n+1) \n",
    "                self.layers[embedding_dot_layer_name].params['W'] -= lr*self.layers[embedding_dot_layer_name].grads['W']\n",
    "            \n",
    "            if i%100 == 0:\n",
    "                print(f'第{i}次Loss：',loss)\n",
    "                                        \n",
    "                L_list.append(loss)\n",
    "                iter_index_list.append(i)\n",
    "        \n",
    "        #画图\n",
    "        plt.plot(iter_index_list,L_list)                    \n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "762a583d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ptb数据集的corpus,word_to_id,id_to_word数据已写入实例属性！\n",
      "co_matrix创建完成！\n",
      "基于skip_gram的正采样数据生成完毕！\n",
      "skip_gram负采样数据生成完毕！\n",
      "skip_gram模式下，id形式的训练数据生成完毕！\n",
      "(4647925, 6) \n",
      "down!\n",
      "第0次Loss： 3998.033771054588\n",
      "第100次Loss： 2633.4739887779056\n",
      "第200次Loss： 2426.676558683836\n",
      "第300次Loss： 1960.9177477414826\n",
      "第400次Loss： 1868.5998755974274\n",
      "第500次Loss： 2162.1144414161927\n",
      "第600次Loss： 1610.1670449870353\n",
      "第700次Loss： 1589.711527301477\n",
      "第800次Loss： 1446.4437000473076\n",
      "第900次Loss： 1212.38738142306\n",
      "第1000次Loss： 1244.8215184649298\n",
      "第1100次Loss： 1399.006168985245\n",
      "第1200次Loss： 1412.532908127887\n",
      "第1300次Loss： 1156.423019719069\n",
      "第1400次Loss： 1260.6992968661352\n",
      "第1500次Loss： 1188.0061046598512\n",
      "第1600次Loss： 841.1094704137956\n",
      "第1700次Loss： 1018.5943163266144\n",
      "第1800次Loss： 974.6265570562198\n",
      "第1900次Loss： 873.711878439\n",
      "第2000次Loss： 1134.0679954141513\n",
      "第2100次Loss： 1144.1103916171828\n",
      "第2200次Loss： 829.156174592585\n",
      "第2300次Loss： 984.4495662502814\n",
      "第2400次Loss： 869.4589807567338\n",
      "第2500次Loss： 815.1951224098103\n",
      "第2600次Loss： 818.5368301373567\n",
      "第2700次Loss： 880.574852195722\n",
      "第2800次Loss： 959.765750868873\n",
      "第2900次Loss： 849.7549438042988\n",
      "第3000次Loss： 643.9325429083234\n",
      "第3100次Loss： 638.9240525016668\n",
      "第3200次Loss： 655.814667786396\n",
      "第3300次Loss： 752.304872449126\n",
      "第3400次Loss： 538.4863683356575\n",
      "第3500次Loss： 749.3794829524977\n",
      "第3600次Loss： 669.2942431320764\n",
      "第3700次Loss： 540.6135818504071\n",
      "第3800次Loss： 538.0874029477293\n",
      "第3900次Loss： 576.7064691090578\n",
      "第4000次Loss： 648.7367319073218\n",
      "第4100次Loss： 576.5973244450738\n",
      "第4200次Loss： 628.5396502721953\n",
      "第4300次Loss： 587.2496374745348\n",
      "第4400次Loss： 456.93383808348017\n",
      "第4500次Loss： 448.0893846669241\n",
      "第4600次Loss： 686.2790841281877\n",
      "第4700次Loss： 477.239134341261\n",
      "第4800次Loss： 332.08733033029637\n",
      "第4900次Loss： 616.2552362332391\n",
      "第5000次Loss： 504.6334910460065\n",
      "第5100次Loss： 338.49632707557817\n",
      "第5200次Loss： 485.4589754844272\n",
      "第5300次Loss： 449.82649061505896\n",
      "第5400次Loss： 432.8424873045812\n",
      "第5500次Loss： 457.6335289689412\n",
      "第5600次Loss： 541.6615033343967\n",
      "第5700次Loss： 372.31615743115884\n",
      "第5800次Loss： 420.3958851108842\n",
      "第5900次Loss： 384.4729181790542\n",
      "第6000次Loss： 514.3745122995791\n",
      "第6100次Loss： 346.31269725473675\n",
      "第6200次Loss： 394.16908665175254\n",
      "第6300次Loss： 394.787560417878\n",
      "第6400次Loss： 407.5479776276892\n",
      "第6500次Loss： 488.8789069075442\n",
      "第6600次Loss： 377.37741817359245\n",
      "第6700次Loss： 280.9805924688015\n",
      "第6800次Loss： 355.8447024917636\n",
      "第6900次Loss： 522.5943483623746\n",
      "第7000次Loss： 291.2154709034245\n",
      "第7100次Loss： 398.66382158134945\n",
      "第7200次Loss： 382.7294756168675\n",
      "第7300次Loss： 259.4401777794127\n",
      "第7400次Loss： 406.0384114636221\n",
      "第7500次Loss： 299.57898036139056\n",
      "第7600次Loss： 403.4746340011864\n",
      "第7700次Loss： 249.5644719545349\n",
      "第7800次Loss： 299.5772914702127\n",
      "第7900次Loss： 277.4707500672049\n",
      "第8000次Loss： 340.95051403069317\n",
      "第8100次Loss： 425.58278563007264\n",
      "第8200次Loss： 233.25268873212684\n",
      "第8300次Loss： 205.75341027527924\n",
      "第8400次Loss： 254.146295050426\n",
      "第8500次Loss： 257.5386068543497\n",
      "第8600次Loss： 134.24845724843112\n",
      "第8700次Loss： 344.37289637247466\n",
      "第8800次Loss： 274.82582993311087\n",
      "第8900次Loss： 304.5038742984926\n",
      "第9000次Loss： 304.6656800414116\n",
      "第9100次Loss： 254.44930131486228\n",
      "第9200次Loss： 288.54813462938193\n",
      "第9300次Loss： 177.01585243264324\n",
      "第9400次Loss： 289.49992527169377\n",
      "第9500次Loss： 297.36080666955803\n",
      "第9600次Loss： 131.3010209181286\n",
      "第9700次Loss： 228.6217531352121\n",
      "第9800次Loss： 261.04780936348527\n",
      "第9900次Loss： 275.24937484661086\n",
      "第10000次Loss： 175.78118056927457\n",
      "第10100次Loss： 306.1119648639569\n",
      "第10200次Loss： 183.92046076605766\n",
      "第10300次Loss： 165.35015611559635\n",
      "第10400次Loss： 278.7310339309358\n",
      "第10500次Loss： 251.44545055583615\n",
      "第10600次Loss： 198.83545863895884\n",
      "第10700次Loss： 193.228601984079\n",
      "第10800次Loss： 178.040824364252\n",
      "第10900次Loss： 264.8443352311393\n",
      "第11000次Loss： 171.54822603510488\n",
      "第11100次Loss： 233.0067715957121\n",
      "第11200次Loss： 145.32264619791152\n",
      "第11300次Loss： 186.78173399794906\n",
      "第11400次Loss： 126.27583901989132\n",
      "第11500次Loss： 215.5471683408259\n",
      "第11600次Loss： 198.68398682723915\n",
      "第11700次Loss： 262.85018510123666\n",
      "第11800次Loss： 153.81050673267762\n",
      "第11900次Loss： 203.84381948620893\n",
      "第12000次Loss： 204.00757087210167\n",
      "第12100次Loss： 189.96098237444141\n",
      "第12200次Loss： 101.3206865917993\n",
      "第12300次Loss： 152.04894942631128\n",
      "第12400次Loss： 222.34155437078965\n",
      "第12500次Loss： 160.40668844261972\n",
      "第12600次Loss： 207.0148058495667\n",
      "第12700次Loss： 160.4459677675806\n",
      "第12800次Loss： 332.08488784631453\n",
      "第12900次Loss： 156.17341672952864\n",
      "第13000次Loss： 116.44484237361269\n",
      "第13100次Loss： 195.4133466611309\n",
      "第13200次Loss： 139.26652749560225\n",
      "第13300次Loss： 81.2539500224538\n",
      "第13400次Loss： 114.91541677831353\n",
      "第13500次Loss： 139.79839490964474\n",
      "第13600次Loss： 214.0033801722317\n",
      "第13700次Loss： 148.64538955876873\n",
      "第13800次Loss： 175.21626596178686\n",
      "第13900次Loss： 110.3592498996625\n",
      "第14000次Loss： 107.38128059394265\n",
      "第14100次Loss： 111.49920589508602\n",
      "第14200次Loss： 250.77429215229412\n",
      "第14300次Loss： 107.49803576221655\n",
      "第14400次Loss： 86.8124334581372\n",
      "第14500次Loss： 119.35195918845373\n",
      "第14600次Loss： 159.2546277547406\n",
      "第14700次Loss： 54.63156901103767\n",
      "第14800次Loss： 53.8174748964196\n",
      "第14900次Loss： 160.05389561855804\n",
      "第15000次Loss： 145.7133085077117\n",
      "第15100次Loss： 111.88334035604574\n",
      "第15200次Loss： 146.62086158419226\n",
      "第15300次Loss： 129.8901791207415\n",
      "第15400次Loss： 142.03586506005357\n",
      "第15500次Loss： 77.76333170748143\n",
      "第15600次Loss： 116.22906500408241\n",
      "第15700次Loss： 93.03168461121622\n",
      "第15800次Loss： 70.42077129417686\n",
      "第15900次Loss： 142.34619017124479\n",
      "第16000次Loss： 79.83736422262518\n",
      "第16100次Loss： 157.7421073916439\n",
      "第16200次Loss： 91.72692669639409\n",
      "第16300次Loss： 89.78347582967896\n",
      "第16400次Loss： 82.2291566158344\n",
      "第16500次Loss： 61.89162407152065\n",
      "第16600次Loss： 91.47766694178068\n",
      "第16700次Loss： 40.29751871891973\n",
      "第16800次Loss： 60.61436388470773\n",
      "第16900次Loss： 126.30579858842347\n",
      "第17000次Loss： 168.01630996909992\n",
      "第17100次Loss： 67.24998100652228\n",
      "第17200次Loss： 65.89446978772821\n",
      "第17300次Loss： 131.69734311606018\n",
      "第17400次Loss： 64.54980847106822\n",
      "第17500次Loss： 150.90718206534493\n",
      "第17600次Loss： 138.68382934598282\n",
      "第17700次Loss： 130.34672121588022\n",
      "第17800次Loss： 152.5040514037725\n",
      "第17900次Loss： 71.81484999420232\n",
      "第18000次Loss： 79.97028653674039\n",
      "第18100次Loss： 116.39291201339368\n",
      "第18200次Loss： 123.36803584708697\n",
      "第18300次Loss： 91.4259611478371\n",
      "第18400次Loss： 71.72872533125013\n",
      "第18500次Loss： 51.766391394883804\n",
      "第18600次Loss： 75.17528480691358\n",
      "第18700次Loss： 91.70545829470419\n",
      "第18800次Loss： 70.47681923839443\n",
      "第18900次Loss： 55.10131274011728\n",
      "第19000次Loss： 37.975063486985405\n",
      "第19100次Loss： 84.19478224916838\n",
      "第19200次Loss： 65.71775171123483\n",
      "第19300次Loss： 62.27766751353235\n",
      "第19400次Loss： 64.24067977323341\n",
      "第19500次Loss： 65.01225251905637\n",
      "第19600次Loss： 131.83091086093418\n",
      "第19700次Loss： 45.14339763035206\n",
      "第19800次Loss： 70.12979703901458\n",
      "第19900次Loss： 45.17752965879948\n",
      "第20000次Loss： 71.38227715342475\n",
      "第20100次Loss： 40.86526361438996\n",
      "第20200次Loss： 111.15303967626556\n",
      "第20300次Loss： 84.30675919430809\n",
      "第20400次Loss： 54.96976025736811\n",
      "第20500次Loss： 76.80650870087331\n",
      "第20600次Loss： 97.72732966034013\n",
      "第20700次Loss： 72.92263280898024\n",
      "第20800次Loss： 85.84252114774785\n",
      "第20900次Loss： 74.93892392337156\n",
      "第21000次Loss： 69.5288331400015\n",
      "第21100次Loss： 18.582259642566356\n",
      "第21200次Loss： 57.540766055028634\n",
      "第21300次Loss： 67.33403900924705\n",
      "第21400次Loss： 28.0390010335679\n",
      "第21500次Loss： 109.37442033639176\n",
      "第21600次Loss： 54.14330553077082\n",
      "第21700次Loss： 74.29864134189606\n",
      "第21800次Loss： 60.57662895358639\n",
      "第21900次Loss： 62.44071586082874\n",
      "第22000次Loss： 105.98428674949106\n",
      "第22100次Loss： 37.792166715794835\n",
      "第22200次Loss： 85.44459514936588\n",
      "第22300次Loss： 46.18199813029579\n",
      "第22400次Loss： 93.31391235522375\n",
      "第22500次Loss： 16.74379387150876\n",
      "第22600次Loss： 54.20694959590854\n",
      "第22700次Loss： 32.08129993751855\n",
      "第22800次Loss： 96.45714973197245\n",
      "第22900次Loss： 34.73679696061356\n",
      "第23000次Loss： 59.34901467948151\n",
      "第23100次Loss： 61.71514722893021\n",
      "第23200次Loss： 28.034881257823173\n",
      "第23300次Loss： 60.72733792229348\n",
      "第23400次Loss： 39.28921688727854\n",
      "第23500次Loss： 84.29347158838698\n",
      "第23600次Loss： 31.438336315319788\n",
      "第23700次Loss： 49.12073364677019\n",
      "第23800次Loss： 35.8895284977806\n",
      "第23900次Loss： 67.70359217129908\n",
      "第24000次Loss： 64.00598898530956\n",
      "第24100次Loss： 27.744605693789694\n",
      "第24200次Loss： 21.687192072615012\n",
      "第24300次Loss： 17.87887614034956\n",
      "第24400次Loss： 26.29700843467312\n",
      "第24500次Loss： 55.795185116903234\n",
      "第24600次Loss： 8.81012745596013\n",
      "第24700次Loss： 33.18622377458187\n",
      "第24800次Loss： 46.86731080762246\n",
      "第24900次Loss： 17.0357752064955\n",
      "第25000次Loss： 32.40052053659341\n",
      "第25100次Loss： 15.867450946976486\n",
      "第25200次Loss： 57.49280820085597\n",
      "第25300次Loss： 42.48244780607815\n",
      "第25400次Loss： 42.81580947204781\n",
      "第25500次Loss： 40.14874319767865\n",
      "第25600次Loss： 43.78658652861957\n",
      "第25700次Loss： 29.76325955582855\n",
      "第25800次Loss： 33.0999580223763\n",
      "第25900次Loss： 15.335108679291853\n",
      "第26000次Loss： 14.167133188502872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第26100次Loss： 45.76078007794684\n",
      "第26200次Loss： 58.34839994837908\n",
      "第26300次Loss： 21.506726260067435\n",
      "第26400次Loss： 54.07269278248768\n",
      "第26500次Loss： 64.79348514732746\n",
      "第26600次Loss： 31.24467176997166\n",
      "第26700次Loss： 28.99735493696986\n",
      "第26800次Loss： 15.090860736811791\n",
      "第26900次Loss： 18.955443598776277\n",
      "第27000次Loss： 46.604627197760394\n",
      "第27100次Loss： 74.04215747426346\n",
      "第27200次Loss： 16.528009164338727\n",
      "第27300次Loss： 37.40287076718219\n",
      "第27400次Loss： 28.687096984555538\n",
      "第27500次Loss： 25.31863539061315\n",
      "第27600次Loss： 25.433139019323217\n",
      "第27700次Loss： 58.18750453632286\n",
      "第27800次Loss： 52.72510480264256\n",
      "第27900次Loss： 34.97072457294588\n",
      "第28000次Loss： 19.768590056555684\n",
      "第28100次Loss： 49.316501889217314\n",
      "第28200次Loss： 19.938605519354358\n",
      "第28300次Loss： 47.329380833810376\n",
      "第28400次Loss： 35.925456641234135\n",
      "第28500次Loss： 41.25806287458408\n",
      "第28600次Loss： 22.70244184409851\n",
      "第28700次Loss： 51.34436700496453\n",
      "第28800次Loss： 13.447721707813134\n",
      "第28900次Loss： 15.888351590413192\n",
      "第29000次Loss： 38.1087715434318\n",
      "第29100次Loss： 15.622427745277184\n",
      "第29200次Loss： 59.19106324790394\n",
      "第29300次Loss： 35.238048559084135\n",
      "第29400次Loss： 15.384518785548035\n",
      "第29500次Loss： 29.213304192279036\n",
      "第29600次Loss： 22.354100035263976\n",
      "第29700次Loss： 7.666191727422749\n",
      "第29800次Loss： 16.180386064725198\n",
      "第29900次Loss： 8.337954197707038\n",
      "第30000次Loss： 50.95863250214079\n",
      "第30100次Loss： 19.03501285964813\n",
      "第30200次Loss： 11.648883387211637\n",
      "第30300次Loss： 23.990087463715746\n",
      "第30400次Loss： 8.444681003290391\n",
      "第30500次Loss： 15.756716661731407\n",
      "第30600次Loss： 8.340763285319522\n",
      "第30700次Loss： 24.34966520853381\n",
      "第30800次Loss： 47.97684155169742\n",
      "第30900次Loss： 18.39976351823427\n",
      "第31000次Loss： 7.876267750469973\n",
      "第31100次Loss： 34.78401931029851\n",
      "第31200次Loss： 4.753769864182704\n",
      "第31300次Loss： 29.50945554963105\n",
      "第31400次Loss： 55.45109280287618\n",
      "第31500次Loss： 6.506627484445701\n",
      "第31600次Loss： 22.23193479281098\n",
      "第31700次Loss： 3.5402184751668377\n",
      "第31800次Loss： 16.39805863344065\n",
      "第31900次Loss： 12.255294708843941\n",
      "第32000次Loss： 43.07766780176777\n",
      "第32100次Loss： 33.81538438358174\n",
      "第32200次Loss： 16.301021087578352\n",
      "第32300次Loss： 22.16454694926774\n",
      "第32400次Loss： 19.055381827851487\n",
      "第32500次Loss： 8.105491077772324\n",
      "第32600次Loss： 13.839255105212791\n",
      "第32700次Loss： 8.1100355533591\n",
      "第32800次Loss： 38.04089289513547\n",
      "第32900次Loss： 25.46745215057468\n",
      "第33000次Loss： 3.3615586856341846\n",
      "第33100次Loss： 14.262430258958283\n",
      "第33200次Loss： 11.166405689504185\n",
      "第33300次Loss： 22.45211873732504\n",
      "第33400次Loss： 5.784745474137363\n",
      "第33500次Loss： 7.91546226603405\n",
      "第33600次Loss： 6.651728207584158\n",
      "第33700次Loss： 5.106397289132224\n",
      "第33800次Loss： 12.721215482173772\n",
      "第33900次Loss： 9.783570041276672\n",
      "第34000次Loss： 31.80591940507615\n",
      "第34100次Loss： 2.4256223078387595\n",
      "第34200次Loss： 9.932577606462157\n",
      "第34300次Loss： 16.4185107960977\n",
      "第34400次Loss： 5.044809629395486\n",
      "第34500次Loss： 1.9383858841498034\n",
      "第34600次Loss： 19.377579865446776\n",
      "第34700次Loss： 3.3843921812099294\n",
      "第34800次Loss： 11.364569995484281\n",
      "第34900次Loss： 6.913766277624862\n",
      "第35000次Loss： 17.204232065209723\n",
      "第35100次Loss： 6.760721888856333\n",
      "第35200次Loss： 17.703464387074245\n",
      "第35300次Loss： 8.938536731021557\n",
      "第35400次Loss： 16.10011412657238\n",
      "第35500次Loss： 26.409334744098732\n",
      "第35600次Loss： 3.6751227970322002\n",
      "第35700次Loss： 6.514947589745728\n",
      "第35800次Loss： 11.179392417189302\n",
      "第35900次Loss： 13.894269695873849\n",
      "第36000次Loss： 28.771114211210516\n",
      "第36100次Loss： 7.204376645774896\n",
      "第36200次Loss： 24.907406489747796\n",
      "第36300次Loss： 3.3351351495923867\n",
      "第36400次Loss： 20.72770143406374\n",
      "第36500次Loss： 5.328594015175642\n",
      "第36600次Loss： 3.0295154277230822\n",
      "第36700次Loss： 8.775880912052362\n",
      "第36800次Loss： 3.9382970093213787\n",
      "第36900次Loss： 5.795554668355486\n",
      "第37000次Loss： 23.042081213663927\n",
      "第37100次Loss： 2.980118399646738\n",
      "第37200次Loss： 14.72786536745095\n",
      "第37300次Loss： 7.972881110843039\n",
      "第37400次Loss： 22.8159067021244\n",
      "第37500次Loss： 14.49871861400682\n",
      "第37600次Loss： 21.292521946770716\n",
      "第37700次Loss： 33.93236654636873\n",
      "第37800次Loss： 4.9586923974216175\n",
      "第37900次Loss： 2.147134961786424\n",
      "第38000次Loss： 6.88501620396395\n",
      "第38100次Loss： 2.196389407811004\n",
      "第38200次Loss： 2.086917486525955\n",
      "第38300次Loss： 20.69246000123375\n",
      "第38400次Loss： 7.536564430989278\n",
      "第38500次Loss： 3.628132607827773\n",
      "第38600次Loss： 1.934256716072605\n",
      "第38700次Loss： 20.824940358562007\n",
      "第38800次Loss： 30.8632093027704\n",
      "第38900次Loss： 7.385524871417849\n",
      "第39000次Loss： 23.160422061600602\n",
      "第39100次Loss： 10.519678793651554\n",
      "第39200次Loss： 3.4955663806817165\n",
      "第39300次Loss： 31.74381088565302\n",
      "第39400次Loss： 5.1331577123271215\n",
      "第39500次Loss： 9.903337941150827\n",
      "第39600次Loss： 26.538143978331963\n",
      "第39700次Loss： 7.62519216424578\n",
      "第39800次Loss： 3.12879244635383\n",
      "第39900次Loss： 11.643025598555804\n",
      "第40000次Loss： 6.020683827201989\n",
      "第40100次Loss： 11.432863356621194\n",
      "第40200次Loss： 20.462093374344366\n",
      "第40300次Loss： 2.9599338718984454\n",
      "第40400次Loss： 7.487137856193441\n",
      "第40500次Loss： 22.779175415357326\n",
      "第40600次Loss： 9.65514850047067\n",
      "第40700次Loss： 8.119998227618147\n",
      "第40800次Loss： 5.176834181332979\n",
      "第40900次Loss： 9.844783529744578\n",
      "第41000次Loss： 10.953166892321782\n",
      "第41100次Loss： 13.836583685725511\n",
      "第41200次Loss： 4.842189412183465\n",
      "第41300次Loss： 12.905464813605965\n",
      "第41400次Loss： 1.324199247342626\n",
      "第41500次Loss： 21.676235586933927\n",
      "第41600次Loss： 1.6011260034438577\n",
      "第41700次Loss： 2.6672478494870284\n",
      "第41800次Loss： 1.7668829652023845\n",
      "第41900次Loss： 2.691295823875508\n",
      "第42000次Loss： 4.619637298645015\n",
      "第42100次Loss： 15.372507664472508\n",
      "第42200次Loss： 2.400541392139865\n",
      "第42300次Loss： 1.0640274901033364\n",
      "第42400次Loss： 22.615070899199306\n",
      "第42500次Loss： 1.903858241485361\n",
      "第42600次Loss： 31.070262965017015\n",
      "第42700次Loss： 2.5465369352805745\n",
      "第42800次Loss： 5.841585854608548\n",
      "第42900次Loss： 1.9285079664980271\n",
      "第43000次Loss： 1.9686800736155905\n",
      "第43100次Loss： 5.891539982887758\n",
      "第43200次Loss： 1.1344509843081636\n",
      "第43300次Loss： 1.6813754363930375\n",
      "第43400次Loss： 6.389591011302673\n",
      "第43500次Loss： 5.217240931088405\n",
      "第43600次Loss： 4.301873311401525\n",
      "第43700次Loss： 3.4420372362436544\n",
      "第43800次Loss： 2.9286534365224877\n",
      "第43900次Loss： 9.556018235936763\n",
      "第44000次Loss： 18.671911362652263\n",
      "第44100次Loss： 1.8886168233924976\n",
      "第44200次Loss： 1.9898907740111027\n",
      "第44300次Loss： 2.6596440851868692\n",
      "第44400次Loss： 2.894736337904334\n",
      "第44500次Loss： 14.064493729249676\n",
      "第44600次Loss： 12.427912992043108\n",
      "第44700次Loss： 11.513132083948797\n",
      "第44800次Loss： 2.1975613217161607\n",
      "第44900次Loss： 1.2943270816306005\n",
      "第45000次Loss： 1.662358861830003\n",
      "第45100次Loss： 2.6236319994648793\n",
      "第45200次Loss： 2.7578273276244265\n",
      "第45300次Loss： 5.765751270758937\n",
      "第45400次Loss： 14.984508998510105\n",
      "第45500次Loss： 1.6837308228202823\n",
      "第45600次Loss： 20.94064988344172\n",
      "第45700次Loss： 0.8475760280527265\n",
      "第45800次Loss： 2.911459214484786\n",
      "第45900次Loss： 1.5468809390318845\n",
      "第46000次Loss： 1.5132115532006984\n",
      "第46100次Loss： 3.2050411534541996\n",
      "第46200次Loss： 1.6500479740626743\n",
      "第46300次Loss： 12.7622990244895\n",
      "第46400次Loss： 1.1077855111932395\n",
      "第46500次Loss： 1.8032249649835634\n",
      "第46600次Loss： 0.9958779316326553\n",
      "第46700次Loss： 1.927194709478298\n",
      "第46800次Loss： 3.9575532897488346\n",
      "第46900次Loss： 14.988570698077709\n",
      "第47000次Loss： 2.940694994645426\n",
      "第47100次Loss： 11.135201981886084\n",
      "第47200次Loss： 2.6035194331497147\n",
      "第47300次Loss： 7.672973971949204\n",
      "第47400次Loss： 1.4452985437558155\n",
      "第47500次Loss： 1.5414867932599514\n",
      "第47600次Loss： 1.1215243020731305\n",
      "第47700次Loss： 11.492288509367858\n",
      "第47800次Loss： 1.4232318775954333\n",
      "第47900次Loss： 4.110922020645237\n",
      "第48000次Loss： 0.9237448769078954\n",
      "第48100次Loss： 1.5982018608611\n",
      "第48200次Loss： 1.135490587470522\n",
      "第48300次Loss： 1.4035604900688838\n",
      "第48400次Loss： 2.4581831988647105\n",
      "第48500次Loss： 7.969044520860145\n",
      "第48600次Loss： 0.8195473120337978\n",
      "第48700次Loss： 1.1379527870824455\n",
      "第48800次Loss： 1.1016821394438723\n",
      "第48900次Loss： 1.682176948891071\n",
      "第49000次Loss： 1.4385760743458267\n",
      "第49100次Loss： 1.1605721434314282\n",
      "第49200次Loss： 3.453659489646977\n",
      "第49300次Loss： 11.304541691454007\n",
      "第49400次Loss： 2.771555252852715\n",
      "第49500次Loss： 1.1254599138762766\n",
      "第49600次Loss： 1.0608091295036346\n",
      "第49700次Loss： 0.9881804206094407\n",
      "第49800次Loss： 3.301020032975545\n",
      "第49900次Loss： 3.760837206156996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp/ipykernel_4540/1955289168.py:162: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  fig.show()\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    ptb_data = DataPreproess(window_size=2,data_format = 'id',negative_sampling_size=4,negative_sampling_params =0.75,module= 'skip_gram')\n",
    "    ptb_data.get_ptb_data()\n",
    "    print(ptb_data.train_data.shape,'\\ndown!')\n",
    "    \n",
    "    sg_model = SkipGram(ptb_data.corpus,ptb_data.word_to_id,ptb_data.id_to_word,ptb_data.co_matrix,weight_width=300,window_size=2,batch_size=1000)\n",
    "    sg_model.train(ptb_data.train_data,iter=50000,lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e5f230f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickle dump down!\n"
     ]
    }
   ],
   "source": [
    "#将训练好的模型写入pickle文件\n",
    "import pickle\n",
    "\n",
    "with open('chapter4_trained_ptb_skip_gram_model.pickle','wb') as f:\n",
    "    pickle.dump(sg_model,f)\n",
    "    print('pickle dump down!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c02458a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickle load down!\n"
     ]
    }
   ],
   "source": [
    "with open('chapter4_trained_ptb_skip_gram_model.pickle','rb') as f:\n",
    "    model = pickle.load(f)\n",
    "    print('pickle load down!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "71f4f3b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.51852357, -1.63232146, -0.20128639, ...,  0.29107702,\n",
       "        -1.12156357,  0.96195488],\n",
       "       [ 3.25388854, -1.22920727, -0.16872985, ...,  2.56879846,\n",
       "        -0.14908005,  0.10458817],\n",
       "       [ 0.84250966,  1.59138189, -0.13245826, ...,  0.80776642,\n",
       "         1.19345538,  0.05179749],\n",
       "       ...,\n",
       "       [ 0.78327062, -1.48215117, -0.30895907, ...,  1.27973431,\n",
       "         0.13287863,  0.33468735],\n",
       "       [-0.47650139,  0.35102506, -0.50774876, ..., -0.95888461,\n",
       "        -0.94946736, -1.16222081],\n",
       "       [-0.64629925, -0.15392495, -0.17539903, ...,  0.87610187,\n",
       "        -0.28083982, -0.73327278]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_model.layers['embedding1'].params['W']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e8891ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.51852357, -1.63232146, -0.20128639, ...,  0.29107702,\n",
       "        -1.12156357,  0.96195488],\n",
       "       [ 3.25388854, -1.22920727, -0.16872985, ...,  2.56879846,\n",
       "        -0.14908005,  0.10458817],\n",
       "       [ 0.84250966,  1.59138189, -0.13245826, ...,  0.80776642,\n",
       "         1.19345538,  0.05179749],\n",
       "       ...,\n",
       "       [ 0.78327062, -1.48215117, -0.30895907, ...,  1.27973431,\n",
       "         0.13287863,  0.33468735],\n",
       "       [-0.47650139,  0.35102506, -0.50774876, ..., -0.95888461,\n",
       "        -0.94946736, -1.16222081],\n",
       "       [-0.64629925, -0.15392495, -0.17539903, ...,  0.87610187,\n",
       "        -0.28083982, -0.73327278]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers['embedding1'].params['W']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc889cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #测试FeatEmbedding()类\n",
    "# if __name__ == '__main__':\n",
    "#     W =np.random.randn(5,5)\n",
    "#     fe1 = Embedding(W)\n",
    "#     X = [0,3,1]\n",
    "#     dout = np.random.rand(3,5)\n",
    "#     print('fe1.forward(X):\\n',fe1.forward(X),'\\n')\n",
    "#     fe1.backward(dout)\n",
    "#     print('fe1.params:\\n',fe1.params,'\\n')\n",
    "#     print('dout:\\n',dout,'\\n')\n",
    "#     print('fe1.grads:\\n',fe1.grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df4a127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #测试sigmoid类\n",
    "# if __name__ == '__main__':\n",
    "#     s= Sigmoid()\n",
    "#     X = np.random.randn(3,5)\n",
    "#     dout = np.random.randn(3,5)\n",
    "#     print('X:\\n',X,'\\n')\n",
    "#     print('dout:\\n',dout,'\\n')\n",
    "#     print('s.forward(X):\\n',s.forward(X),'\\n')\n",
    "#     print('s.backward(dout):\\n',s.backward(dout),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e6dd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #交叉熵损失测试\n",
    "# if __name__ == '__main__':\n",
    "#     tl = CrossEntropyError()\n",
    "#     a=[0,1]\n",
    "#     X = 0.5*np.random.choice(a,size=(10))+0.1\n",
    "#     y = np.random.choice(a,size=(10))\n",
    "#     print('X:\\n',X,'\\n')\n",
    "#     print('y:\\n',y,'\\n')\n",
    "#     print('tl.forward(X,y):\\n',tl.forward(X,y),'\\n')\n",
    "#     print('tl.backward():\\n',tl.backward(),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abce7ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #测试EmbeddingDot()类\n",
    "# if __name__ == '__main__':\n",
    "#     W = np.random.randn(3,5)\n",
    "#     X = np.random.randn(2,3)\n",
    "#     y = [3,1]\n",
    "#     dout = np.random.randn(2)\n",
    "    \n",
    "#     print('W\\n',W,'\\n')\n",
    "#     print('X\\n',X,'\\n')\n",
    "#     print('y\\n',y,'\\n')\n",
    "#     print('dout\\n',dout,'\\n')\n",
    "    \n",
    "#     ed = EmbeddingDot(W)\n",
    "#     print('ed.forward(X,y)\\n',ed.forward(X,y),'\\n')\n",
    "#     print('ed.backward(dout)\\n',ed.backward(dout),'\\n')\n",
    "#     print('dw:\\n',ed.grads['W'],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b629f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #negative_sampling()负采样功能测试\n",
    "# if __name__ == '__main__':\n",
    "#     from chapter2 import s2c,c2m\n",
    "#     input_str = 'you say goodbye and i say hello.'\n",
    "#     corpus = s2c(input_str)\n",
    "#     word_to_id = corpus['word_to_id']\n",
    "#     id_to_word = corpus['id_to_word']\n",
    "#     co_matrix = c2m(corpus)\n",
    "#     corpus = corpus['corpus']\n",
    "#     postive_target = [1,2,3,4,5]\n",
    "#     sampling_result = negative_sampling(corpus,co_matrix,word_to_id,id_to_word,postive_target,negative_sampling_size=2)\n",
    "#     print(sampling_result.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247fea46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Affine:\n",
    "#     '''\n",
    "#     一个带有偏置的全链接层，偏置默认是零\n",
    "#     '''\n",
    "#     def __init__(self,W,b=0):\n",
    "        \n",
    "#         self.params = {}\n",
    "#         self.params['W'] = W\n",
    "#         self.params['b'] = b\n",
    "#         self.grads = {}\n",
    "#         self.grads['W'] = np.zeros_like(self.params['W'])\n",
    "#         self.grads['b'] = np.zeros_like(self.params['b'])\n",
    "        \n",
    "#     def forward(self,X):\n",
    "        \n",
    "#         self.params['X'] = X\n",
    "        \n",
    "#         return np.dot(X,self.params['W'])+self.params['b']\n",
    "    \n",
    "#     def backward(self,dout):\n",
    "        \n",
    "#         self.grads['W'] = np.dot(dout,self.params['X'].T)\n",
    "#         self.grads['b'] = np.sum(dout,axis=0)\n",
    "#         self.grads['X'] = np.dot(dout,self.params['W'].T)\n",
    "        \n",
    "#         return self.grads['X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58ef65e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DataAverage():\n",
    "    \n",
    "#     def __init__(self):\n",
    "\n",
    "#         self.params ={}\n",
    "#         self.gards = {}\n",
    "    \n",
    "#     def forward(self,*arrays):\n",
    "            \n",
    "#         result = np.zeros_like(arrays[0])\n",
    "        \n",
    "#         for array in arrays:\n",
    "            \n",
    "#             result += array\n",
    "        \n",
    "#         result = result/len(arrays)\n",
    "        \n",
    "#         return result\n",
    "    \n",
    "#     def backward(self,dout,window_size):\n",
    "        \n",
    "#         return (1/window_size)*dout"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
