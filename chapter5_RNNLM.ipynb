{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a437266",
   "metadata": {},
   "source": [
    "# SimpleRNNLM\n",
    "\n",
    "### Embedding\n",
    "X(batch_size,T),We(len(co_matrix),hyperparam1)    \n",
    "            ↓    \n",
    "y(batch_size,T,hyperparam1)\n",
    "### TimeRNN\n",
    "X(batch_size,T,hyperparam1),Wx(hyperparam1,hyperparam2),Wh(hyperparam2,hyperparam2),Br(1,hyperparam2)    \n",
    "            ↓     \n",
    "y(batch_size,T,hyperparam2)\n",
    "### TimeAffine\n",
    "X(batch_size,T,hyperparam2),Wa(hyperparam2,hyperparam3),Ba(1,hyperparam3)    \n",
    "            ↓     \n",
    "y(batch_size,T,hyperparam3)\n",
    "### Softmax\n",
    "x(batch_size,T,hyperparam3)     \n",
    "            ↓\n",
    "y(batch_size,T,hyperparam3)\n",
    "### TimePerplexityLoss\n",
    "x(batch_size,T,hyperparam3)     \n",
    "            ↓     \n",
    "y(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "14f40b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d7913a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRnnlm():\n",
    "    \n",
    "    def __init__(self,init_W,init_b,time_T,batch_size,lr=0.01):\n",
    "        \n",
    "        We,Wx,Wh,Wa = init_W\n",
    "        Br,Ba = init_b\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.layers = OrderedDict()\n",
    "        \n",
    "        self.layers['embedding'] = Embedding(We)\n",
    "        self.layers['TimeRNN'] = TimeRNN(Wx,Wh,Br,time_T)\n",
    "        self.layers['TimeAffine'] = Affine(Wa,Ba)\n",
    "        self.layers['TimeSoftmax'] = Softmax()\n",
    "        self.layers['TimePerPlexityLoss'] = PerPlexityLoss()\n",
    "        \n",
    "    def forward(self,X,T_id):\n",
    "        \n",
    "        for layer in self.layers.values()[:-1]:\n",
    "            X = layer.forward(X)\n",
    "        \n",
    "        total_loss = self.layers['TimePerPlexityLoss'].forward(X,T_id)\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def backward(self,dout=1):\n",
    "        \n",
    "        for layer in reversed(self.layers.values()):\n",
    "            dout = layer.backward(dout)      \n",
    "        \n",
    "        return\n",
    "    \n",
    "    def renew_params(self):\n",
    "        \n",
    "        layer_list = [self.layers['embedding'],\n",
    "              self.layers['TimeRNN'],\n",
    "              self.layers['TimeAffine']]\n",
    "        \n",
    "        for layer in layer_list:\n",
    "            for key in layer.params.keys():\n",
    "                layer.params[key] -= self.lr*layer.grads[key]\n",
    "                \n",
    "        return\n",
    "    \n",
    "    def train(self,input_corpus,iter_num=100):\n",
    "        \n",
    "        loss_list = []\n",
    "        iter_index_list = []\n",
    "        \n",
    "        N = self.batch_size\n",
    "        T = len(input_corpus)\n",
    "        \n",
    "        #生成多个batch_size所需要的数据,batch_data之间最小的间距为1\n",
    "        batch_dist = max(int(T/N),1)\n",
    "        X = np.empty((N,T))\n",
    "        T_id = np.empty((N,T,1))\n",
    "        \n",
    "        for i in range(N):\n",
    "            X[i] = input_corpus[i*batch_dist:]+input_corpus[:i*batch_dist]\n",
    "            T_id[i] = input_corpus[i*batch_dist+1:]+input_corpus[:i*batch_dist+1].reshape((T,1))\n",
    "        \n",
    "        #根据循环次数进行训练\n",
    "        for n in range(iter_num):\n",
    "            \n",
    "            #进行一次前向传播和反向传播\n",
    "            loss = self.forward(X,T_id)\n",
    "            self.backward(dout=1)\n",
    "            \n",
    "            #根据反向传播导数更新参数\n",
    "            renew_params()\n",
    "            \n",
    "            if n%10 == 0:\n",
    "                print(f'第{n}次的损失为：',loss)\n",
    "                loss_list.append(loss)\n",
    "                iter_index_list.append(n)              \n",
    "        \n",
    "        #画图观察训练过程中loss的变化\n",
    "        fig = plt.figure(figsize=(10,8))\n",
    "        plt.plot(iter_index_list,loss_list)\n",
    "        fig.show()\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dda84dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding():\n",
    "    \n",
    "    def __init__(self,W):\n",
    "        \n",
    "        self.params = {}\n",
    "        self.params['W'] = W\n",
    "        \n",
    "        self.grads = {}\n",
    "    \n",
    "    def forward(self,X):\n",
    "        \n",
    "        '''\n",
    "        X(N,len_corpus),W(max_id_in_corpus,WW)\n",
    "        这里的X是单词id形式，不是one hot形式.\n",
    "        N是batch_size.\n",
    "        T是corpus的长度，即需要训练的这段话总共有多少字,也是TimeRNN包含RNN单元的数量值\n",
    "        WW是分布式向量的维度\n",
    "        '''\n",
    "        N,T = X.shape\n",
    "        MD,WW = self.params['W'].shape\n",
    "        \n",
    "        y = np.empty((N,T,WW))\n",
    "        \n",
    "        for i in range(N):\n",
    "            y[i] = W[X[i]]\n",
    "                \n",
    "        self.params['X'] = X\n",
    "                \n",
    "        return y\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        \n",
    "        #因为backward中导数存在+=操作，因此每次反向传播开始前，需要先把导数清零\n",
    "        self.grads['W'] = np.zeros_like(W)\n",
    "        \n",
    "        for i in range(N):\n",
    "            self.grads['W'][X[i]] += dout[i]\n",
    "\n",
    "        return self.grads['W']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0f25832f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN():\n",
    "    \n",
    "    def __init__(self,Wx,Wh,Br):\n",
    "        \n",
    "        self.params = {}\n",
    "        self.params['Wx'] = Wx\n",
    "        self.params['Wh'] = Wh\n",
    "        self.params['Br'] = Br\n",
    "        \n",
    "        self.grads = {}        \n",
    "    \n",
    "    def forward(self,X,h_prev):\n",
    "        \n",
    "        Wx,Wh,Br = self.params['Wx'],self.params['Wh'],self.params['Br']\n",
    "        \n",
    "        h_next = np.tan(np.dot(X,Wx) + np.dot(h_prev,Wh) + Br)\n",
    "        \n",
    "        self.cache = []\n",
    "        \n",
    "        self.cache.append(h_next)\n",
    "        self.cache.append(X)\n",
    "        self.cache.append(h_prev)\n",
    "        \n",
    "        return h_next\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        \n",
    "        '''\n",
    "        此处的dout是dh_next和d_out之和,\n",
    "        dh_next和d_out分别为这个RNN单元输出的两个值h_next和h_out的导数\n",
    "        '''\n",
    "        \n",
    "        #每次反向传播开始前先把导数清理\n",
    "        self.grads['Wx'] = np.zeros_like(Wx)\n",
    "        self.grads['Wh'] = np.zeros_like(Wh)\n",
    "        self.grads['Br'] = np.zeros_like(Br)        \n",
    "        \n",
    "        Wx,Wh,Br = self.params['Wx'],self.params['Wh'],self.params['Br']\n",
    "        h_next,X,h_prev = self.cache\n",
    "        \n",
    "        dtan = dout * (1 + h_next**2)\n",
    "        dx = np.dot(dtan,Wx.T)\n",
    "        dwx = np.dot(dtan,X.T)\n",
    "        dh_prev = np.dot(dtan,Wh.T)\n",
    "        dwh = np.dot(dtan,h_prev.T)\n",
    "        dbr = np.sum(dtan,axis=0)\n",
    "        \n",
    "        self.grads['Wx'] = dwx\n",
    "        self.grads['Wh'] = dwh\n",
    "        self.grads['Br'] = dbr    \n",
    "        \n",
    "        return dx,dh_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "19b6eb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeRNN():\n",
    "    \n",
    "    def __init__(self,Wx,Wh,Br,T,statful=False,truncate_size=10):\n",
    "        \n",
    "        '''\n",
    "        T是Time RNN中包含的RNN单元的个数，也是整个时间序列的长度\n",
    "        statful是控制是否要在反向传播的时候按一定距离对反向传播的导数进行阶段的开关\n",
    "        truncate_size进行截断反向传播操作的RNN间距\n",
    "        '''\n",
    "        \n",
    "        #初始化TimeRNN的参数\n",
    "        self.params = {}\n",
    "        self.params['Wx'] = Wx\n",
    "        self.params['Wh'] = Wh\n",
    "        self.params['Br'] = Br\n",
    "        \n",
    "        #初始化TimeRNN的导数列表\n",
    "        self.grads = {}\n",
    "        \n",
    "        #初始化前向传播的记忆\n",
    "        self.h = 0\n",
    "        \n",
    "        #初始化反向传播的记忆\n",
    "        self.dh_prev = 0\n",
    "        \n",
    "        #初始化Truncate状态\n",
    "        self.statful = statful\n",
    "        self.truncate_size = truncate_size\n",
    "        \n",
    "        #初始化具体的各个RNN单元\n",
    "        self.layers = []\n",
    "        for i in range(T):\n",
    "            layer = RNN(Wx,Wh,Br)\n",
    "            self.layers.append(layer)\n",
    "            \n",
    "    def fowrward(self,Xs):\n",
    "        \n",
    "        #初始化前向传播最终输出的array\n",
    "        Wx,Wh,Br = self.params['Wx'],self.params['Wh'],self.params['Br']\n",
    "        N,T,D = Xs.shape\n",
    "        D,H = Wx.shape\n",
    "        Hs = np.empty((N,T,H))\n",
    "        \n",
    "        #对每个时刻T分别计算其输出h,并将生成的每个h放到hs集合中合适的位置\n",
    "        for T,layer in enumerate(self.layers):\n",
    "            \n",
    "            X = Xs[:,T,:]\n",
    "            h_prev = self.h\n",
    "            \n",
    "            self.h = layer.forward(X,h_prev)\n",
    "            \n",
    "            Hs[:,T,:] = self.h\n",
    "        \n",
    "        return Hs\n",
    "    \n",
    "    def backward(self,dhs):\n",
    "        \n",
    "        #因为backward的权重更新中有+=操作，需要在每次backward之前将grads初始化为0\n",
    "        Wx,Wh,Br = self.params['Wx'],self.params['Wh'],self.params['Br']\n",
    "        self.grads['Wx'] = np.zeros_like(Wx)\n",
    "        self.grads['Wh'] = np.zeros_like(Wh)\n",
    "        self.grads['Br'] = np.zeros_like(Br)\n",
    "         \n",
    "        len_of_layers = len(self.layers)\n",
    "        \n",
    "        #反向传播需要从最后一个RNN开始，这里用了reversed将存放RNN的列表进行了翻转\n",
    "        #为了让时刻T也进行翻转，用enumerate先产生了reversed_T,再用这个推导出跟RNN匹配的编号T\n",
    "        for reversed_T,layer in enumerate(reversed(self.layers)):\n",
    "            \n",
    "            T = len_of_layers - reversed_T -1\n",
    "\n",
    "            dh_out = dhs[:,T,:]\n",
    "\n",
    "            if (T+1)%self.truncate_size == 0 and self.statful == True:self.dh_prev = 0\n",
    "\n",
    "            dout = dh_out + self.dh_prev\n",
    "\n",
    "            dx,self.dh_prev = layer.backward(dout)\n",
    "            \n",
    "            #因为每个RNN的权重和偏置一样。\n",
    "            #与其每算出来一个RNN的导数更新一次权重和偏置\n",
    "            #还不如把所有的导数都叠加起来，最后更新一次偏置和偏置\n",
    "            self.grads['Wx'] += layer.grads['Wx']\n",
    "            self.grads['Wh'] += layer.grads['Wh']\n",
    "            self.grads['Br'] += layer.grads['Br']\n",
    "        \n",
    "        return dxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8f0ea9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeAffine():\n",
    "    \n",
    "    def __init__(self,Wa,Ba):\n",
    "        \n",
    "        self.params = {}\n",
    "        self.params['Wa'] = Wa\n",
    "        self.params['Ba'] = Ba\n",
    "        \n",
    "        self.grads = {}\n",
    "        self.grads['Wa'] = np.zeros_like(Wa)\n",
    "        self.grads['Ba'] = np.zeros_like(Ba)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        \n",
    "        Wa,Ba = self.params['Wa'],self.params['Ba']\n",
    "        N,T,H = X.shape\n",
    "        H,D = Wa.shape\n",
    "\n",
    "        #全链接层的权重只有两个维度，而输入的X包含N,T,H三个维度，需要设置两个中间变量X_tmp和y_tmp来进行转换\n",
    "        X_tmp = X.reshape((N*T,H))\n",
    "        self.params['X_tmp'] = X_tmp\n",
    "\n",
    "        y_tmp = np.dot(X_tmp,Wa) + Ba\n",
    "        \n",
    "        y = y_tmp.reshape((N,T,D))\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        \n",
    "        Wa,Ba,X_tmp = self.params['Wa'],self.params['Ba'],self.params['X_tmp']\n",
    "        N,T,D = dout.shape\n",
    "        H,D = Wa.shape\n",
    "        \n",
    "        #反向传播与前向传播同理，而输入的dout是三个维度的，无法直接与W.T和X.T进行矩阵点乘，所以同样也需要进行中间转换\n",
    "        dout_tmp = dout.reshape((N*T,D))\n",
    "        \n",
    "        dx_tmp = np.dot(dout_tmp,Wa.T)\n",
    "        dx = dx_tmp.reshape((N,T,H))\n",
    "        \n",
    "        self.grads['Wa'] = np.dot(X_tmp.T,dout_tmp)\n",
    "        self.grads['Ba'] = np.sum(dout_tmp,axis=0)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b7bd76db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSoftmax():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.params = {}\n",
    "        self.grads = {}\n",
    "        \n",
    "    def __forward(self,X):\n",
    "        \n",
    "        self.params['X'] = X\n",
    "        N,T,D = X.shape\n",
    "        \n",
    "        X_tmp = X.reshape((N*T,D))\n",
    "        y_tmp = np.exp(X_tmp)/np.sum(np.exp(X_tmp),axis=1,keepdims=True).repeat(D,axis=1)\n",
    "        y = y_tmp.reshape((N,T,D))\n",
    "        \n",
    "        #存储y,在反向传播中会用到\n",
    "        self.params['y'] = y\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        \n",
    "        y = self.params['y']\n",
    "        \n",
    "        #softmax的导数为y(1-y)，其中y为每个元素正向传播的结果\n",
    "        dx = dout*(y*(1-y))\n",
    "        \n",
    "        self.grads['X'] = dx \n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "870a0817",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimePerPlexityLoss():\n",
    "    \n",
    "    def __init__(self,eps = 1e-8):\n",
    "        \n",
    "        #因为采用困惑度作为loss衡量标准，为了防止出现0为分母的情况，所以需要加上一个微小数eps\n",
    "        self.params ={}\n",
    "        self.grads = {}\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self,X,T_id):\n",
    "        \n",
    "        #input数据T_id需要用id的方式表示,T_id.shape = (N,T,1)\n",
    "        #只需要把每个长度为D的向量的序号为id的那个值拿出来，计算它的倒数，即为这个单词的损失\n",
    "        #最终损失需要把N*T个单词的损失全加起来之后除以N*T,表示每个单词的平均困惑度。\n",
    "        #如果完全拟合的情况下，即最终损失最小的值，为1\n",
    "        \n",
    "        loss_t = 0\n",
    "        \n",
    "        N,T,D = X.shape\n",
    "        self.params['X'] = X\n",
    "        self.params['T_id'] = T_id\n",
    "        \n",
    "        for i in range(N):\n",
    "            for j in range(T):\n",
    "                loss = 1/(X[i,j,T_id[i,j,0]] + self.eps)\n",
    "                loss_t += loss\n",
    "        \n",
    "        loss_t = loss_t/(N*T)\n",
    "        \n",
    "        return loss_t\n",
    "    \n",
    "    def backward(self,dout=1):\n",
    "        \n",
    "        X,T_id = self.params['X'],self.params['T_id']\n",
    "        N,T,D = X.shape\n",
    "        \n",
    "        dx = np.zeros_like(X)\n",
    "        for i in range(N):\n",
    "            for j in range(T):\n",
    "                dx[i,j,T_id[i,j,0]] += dout*(-1/(X[i,j,T_id[i,j,0]]**2))\n",
    "        \n",
    "        self.grads['X'] =dx\n",
    "        \n",
    "        return dx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
