{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f0dae87",
   "metadata": {},
   "source": [
    "# LSTM Model(with Dropout, weight tying)\n",
    "## params:\n",
    "###### corpus,word_to_id,id_to_word = DataPreprocess(file_addr).creat_corpus_w2d_d2w()\n",
    "\n",
    "###### N = batch size   \n",
    "#每个batch_size的长度都是T，但是每个batch的第一个单词的位置会发生变化        \n",
    "    \n",
    "###### D = len(word_to_id.items())\n",
    "    \n",
    "###### T = len(corpus)-window_size  \n",
    "#整个时序的长度,与窗口大小有关  \n",
    "    \n",
    "###### Ws = hyperParam1           \n",
    "#window_size，需要向后预测的大小，一般为1 \n",
    "    \n",
    "###### Ts = hyperParam2          \n",
    "#truncated_size，截断反向传播的间距     \n",
    "    \n",
    "###### WW_emb = hyperParam3      \n",
    "#embedding层权重的维度,width of weight of embedding\n",
    "\n",
    "###### Dr = hyperParam4    \n",
    "#drop rate的值，取0-1之间的值\n",
    "\n",
    "###### WW_lstm = Wlf/Wlg/Wli/Wlo = hyperParam5\n",
    "###### WB_lstm = Blf/Blg/Bli/Blo = hyperParam5    \n",
    "#LSTM中各种门的权重的维度,Wlf表示width of weight of lstm f gate，Blf表示 width of bias of lstm f gate\n",
    "\n",
    "## layers:\n",
    "### Time Embedding ( weight tying with Time Affine)\n",
    "###### X(N,T)，We(D,WW_emb)→Y1(N,T,WW_emb)   \n",
    "#X:[[3,2,3,4,7……],……],每个元素是一个单词的id    \n",
    "### Time Dropout\n",
    "###### Y1(N,T,WW_emb)，Dr(1)→Y2(N,T,WW_emb)，mask1(N,T,WW_emb)\n",
    "#dropout丢弃的数据是在哪个维度上？有多种可能，第一种是在batch_size维度，每次丢掉一部分的数据。第二种是在权重的宽度维度上，每次忽略一部分特征的权重，第三种是数据+权重，既丢弃部分数据也丢弃部分权重。还有一些可能是把时间维度也作为丢弃的一部分。    \n",
    "书中的dropout是在所有的维度上随机丢弃\n",
    "### Time LSTM1\n",
    "###### Y2(N,T,WW_emb),    \n",
    "###### Wl1x(WW_emb,WW_lstm1+WW_lstm1+WW_lstm1+WW_lstm1),\n",
    "###### Wl1h(WW_lstm1,WW_lstm1+WW_lstm1+WW_lstm1+WW_lstm1),    \n",
    "###### Bl1(1,WB_lstm1+WB_lstm1+WB_lstm1+WB_lstm1)    \n",
    "    ↓    \n",
    "###### Y3(N,T,WW_lstm1)\n",
    "### Time Dropout\n",
    "###### Y3(N,T,WW_lstm1)，Dr(1)→Y4(N,T,WW_lstm1)，mask2(N,T,WW_lstm1)\n",
    "### Time LSTM2\n",
    "#当存在权重绑定时，需要在输出Affine的前一层，此处为Time LSTM2，对数据维度进行转换，将权重的维度设置成绑定数据的权重维度，才能在下面的Affine实现权重绑定\n",
    "###### Y4(N,T,WW_lstm1),    \n",
    "###### Wl2x(WW_lstm1,WW_emb+WW_emb+WW_emb+WW_emb), \n",
    "###### Wl2h(WW_emb,WW_emb+WW_emb+WW_emb+WW_emb),    \n",
    "###### Bl2(1,WW_emb+WW_emb+WW_emb+WW_emb)\n",
    "    ↓    \n",
    "###### Y5(N,T,WW_emb)\n",
    "#### Time Dropout\n",
    "###### Y5(N,T,WW_emb)，Dr(1)→Y6(N,T,WW_emb)，mask3(N,T,WW_emb)\n",
    "### Time Affine (weight tying with Time Embedding)\n",
    "###### Y6(N,T,WW_emb)，We.T(WW_emb,D)，Ba(1,D)→Y7(N,T,D)\n",
    "### Time Softmax with PerplexityLoss\n",
    "###### Y7(N,T,D)→Y8(1)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10756999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6c1b4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetterRNN():\n",
    "\n",
    "    def __init__(self,init_W,init_b,time_T,batch_size,lr=0.01):\n",
    "        \n",
    "        We,Wx1,Wh1,Wx2,Wh2 = init_W\n",
    "        Bl1,Bl2,Ba = init_b\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "            \n",
    "        self.layers = OrderedDict()\n",
    "        \n",
    "        self.layers['embedding'] = Embedding(We)\n",
    "        self.layers['dropout1'] = Dropout()\n",
    "        self.layers['TimeLSTM1'] = TimeLSTM(Wx1,Wh1,Bl1)\n",
    "        self.layers['dropout2'] = Dropout()\n",
    "        self.layers['TimeLSTM2'] = TimeLSTM(Wx2,Wh2,Bl2)\n",
    "        self.layers['dropout3'] = Dropout()\n",
    "        self.layers['TimeAffine'] = Affine(We.T,Ba)\n",
    "        self.layers['TimeSoftmax'] = Softmax()\n",
    "        self.layers['TimePerPlexityLoss'] = PerPlexityLoss()\n",
    "        \n",
    "    def forward(self,X,T_id):\n",
    "        \n",
    "        for layer in self.layers.values()[:-1]:\n",
    "            X = layer.forward(X)\n",
    "        \n",
    "        total_loss = self.layers['TimePerPlexityLoss'].forward(X,T_id)\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def backward(self,dout=1):\n",
    "        \n",
    "        for layer in reversed(self.layers.values()):\n",
    "            dout = layer.backward(dout)      \n",
    "        \n",
    "        return\n",
    "    \n",
    "    def renew_params(self):\n",
    "        \n",
    "        layer_list = [self.layers['embedding'],\n",
    "              self.layers['TimeLSTM1'],\n",
    "              self.layers['TimeLSTM2'],\n",
    "              self.layers['TimeLSTM3'],\n",
    "              self.layers['TimeAffine']]\n",
    "        \n",
    "        for layer in layer_list:\n",
    "            for key in layer.params.keys():\n",
    "                layer.params[key] -= self.lr*layer.grads[key]\n",
    "                \n",
    "        return\n",
    "    \n",
    "    def train(self,input_corpus,iter_num=100):\n",
    "        \n",
    "        loss_list = []\n",
    "        iter_index_list = []\n",
    "        \n",
    "        N = self.batch_size\n",
    "        T = len(input_corpus)\n",
    "        \n",
    "        #生成多个batch_size所需要的数据,batch_data之间最小的间距为1\n",
    "        batch_dist = max(int(T/N),1)\n",
    "        X = np.empty((N,T))\n",
    "        T_id = np.empty((N,T,1))\n",
    "        \n",
    "        for i in range(N):\n",
    "            X[i] = input_corpus[i*batch_dist:]+input_corpus[:i*batch_dist]\n",
    "            T_id[i] = input_corpus[i*batch_dist+1:]+input_corpus[:i*batch_dist+1].reshape((T,1))\n",
    "        \n",
    "        #根据循环次数进行训练\n",
    "        for n in range(iter_num):\n",
    "            \n",
    "            #进行一次前向传播和反向传播\n",
    "            loss = self.forward(X,T_id)\n",
    "            self.backward(dout=1)\n",
    "            \n",
    "            #根据反向传播导数更新参数\n",
    "            renew_params()\n",
    "            \n",
    "            if n%10 == 0:\n",
    "                print(f'第{n}次的损失为：',loss)\n",
    "                loss_list.append(loss)\n",
    "                iter_index_list.append(n)              \n",
    "        \n",
    "        #画图观察训练过程中loss的变化\n",
    "        fig = plt.figure(figsize=(10,8))\n",
    "        plt.plot(iter_index_list,loss_list)\n",
    "        fig.show()\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbf5d88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocess():\n",
    "    \n",
    "    def __init__(self,file_addr,language_type='e'):\n",
    "        self.file_addr = file_addr\n",
    "        self.language_type = language_type\n",
    "\n",
    "    def creat_corpus_w2d_d2w(self):\n",
    "        '''\n",
    "        input=string\n",
    "        output=corpus&word_to_id&id_to_word\n",
    "        '''\n",
    "        if self.language_type == 'e':\n",
    "            \n",
    "            with open(self.file_addr,'r') as f:\n",
    "                input_str = f.readlines()\n",
    "                \n",
    "            input_str = input_str.lower()\n",
    "            input_str = input_str.replace('.',' .')\n",
    "\n",
    "            words = input_str.split(' ')\n",
    "\n",
    "            word_to_id = {}\n",
    "            id_to_word = {}\n",
    "\n",
    "            for word in words:\n",
    "                if word not in word_to_id:\n",
    "                    new_id = len(word_to_id)\n",
    "                    word_to_id[word] = new_id\n",
    "                    id_to_word[new_id] = word\n",
    "\n",
    "            corpus = [word_to_id[word] for word in words]\n",
    "            corpus = np.array(corpus)\n",
    "\n",
    "            return corpus,word_to_id,id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb9f76ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding():\n",
    "    \n",
    "    def __init__(self,W):\n",
    "        \n",
    "        self.params = {}\n",
    "        self.params['W'] = W\n",
    "        \n",
    "        self.grads = {}\n",
    "    \n",
    "    def forward(self,X):\n",
    "        \n",
    "        '''\n",
    "        X(N,len_corpus),W(max_id_in_corpus,WW)\n",
    "        这里的X是单词id形式，不是one hot形式.\n",
    "        N是batch_size.\n",
    "        T是corpus的长度，即需要训练的这段话总共有多少字,也是TimeRNN包含RNN单元的数量值\n",
    "        WW是分布式向量的维度\n",
    "        '''\n",
    "        N,T = X.shape\n",
    "        MD,WW = self.params['W'].shape\n",
    "        \n",
    "        y = np.empty((N,T,WW))\n",
    "        \n",
    "        for i in range(N):\n",
    "            y[i] = W[X[i]]\n",
    "                \n",
    "        self.params['X'] = X\n",
    "                \n",
    "        return y\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        \n",
    "        #因为backward中导数存在+=操作，因此每次反向传播开始前，需要先把导数清零\n",
    "        self.grads['W'] = np.zeros_like(W)\n",
    "        \n",
    "        for i in range(N):\n",
    "            self.grads['W'][X[i]] += dout[i]\n",
    "\n",
    "        return self.grads['W']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a20ed1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lstm():\n",
    "    \n",
    "    def __init__(self,Wx,Wh,Bl):\n",
    "        \n",
    "        self.params = {}\n",
    "        self.params['Wx'] = Wx\n",
    "        self.params['Wh'] = Wh\n",
    "        self.params['Bl'] = Bl\n",
    "                \n",
    "        self.grads = {}\n",
    "                \n",
    "    def sigmoid(self,X):\n",
    "        \n",
    "        y = 1/(1+np.exp(-1*X))\n",
    "        \n",
    "        return y \n",
    "    \n",
    "    def back_sigmoid(self,y,dout):\n",
    "        \n",
    "        dx = dout*y(1-y)\n",
    "        \n",
    "        return dx \n",
    "    \n",
    "    def tanh(self,X):\n",
    "        \n",
    "        y = (np.exp(X) - np.exp(-1*X))/(np.exp(X) + np.exp(-1*X))\n",
    "        \n",
    "        return y\n",
    "\n",
    "    def back_tanh(self,y,dout):\n",
    "        \n",
    "        dx = dout*(1-y**2)\n",
    "        \n",
    "        return dx\n",
    "        \n",
    "    def forward(self,X,H_prev,C_prev):\n",
    "        \n",
    "        self.cache = []\n",
    "                \n",
    "        #lstm有4个门，Wx是4个门拼合在一起的权重组合，单个的门的权重的维度是整体的1/4 \n",
    "        four_width_of_weight = self.params['Wx'].shape[1]\n",
    "        width_of_weight = int(four_width_of_weight/4) \n",
    "        d = width_of_weight\n",
    "        \n",
    "        #计算各个门的输出\n",
    "        y_tmp = np.dot(X,Wx) + np.dot(H_prev,Wh) + Bl\n",
    "        \n",
    "        yf = y_tmp[:,:d]\n",
    "        yf = self.sigmoid(yf)\n",
    "        \n",
    "        yg = y_tmp[:,d:2*d]\n",
    "        yg = self.tanh(yg)\n",
    "        \n",
    "        yi = y_tmp[:,2*d:3*d]\n",
    "        yi = self.sigmoid(yi)\n",
    "        \n",
    "        yo = y_tmp[:,3*d:]\n",
    "        yo = self.sigmoid(yo)\n",
    "        \n",
    "        #输出记忆内容C和隐藏状态H\n",
    "        C_next = C_prev*yf + yg*yi\n",
    "        H_next = self.tanh(C_next)*yo\n",
    "        \n",
    "        #将中间结果添加到缓存，方便反向传播调用\n",
    "        self.cache.append(yf)\n",
    "        self.cache.append(yg)\n",
    "        self.cache.append(yi)\n",
    "        self.cache.append(yo)\n",
    "        self.cache.append(C_prev)\n",
    "        self.cache.append(C_next)\n",
    "        self.cache.append(H_prev)\n",
    "        self.cache.append(X)\n",
    "        \n",
    "        return H_next,C_next\n",
    "        \n",
    "    def backward(self,dnext_C,dnext_H,dout_H):\n",
    "        \n",
    "        #每次反向传播前初始化权重和偏置的导数,读取正向传播过程中的中间结果\n",
    "        Wx,Wh,Bl = self.params['Wx'],self.params['Wh'],self.params['Bl']\n",
    "        self.grads['Wx'] = np.zeros_like(Wx)\n",
    "        self.grads['Wh'] = np.zeros_like(Wh)\n",
    "        self.grads['Bl'] = np.zeros_like(Bl)              \n",
    "        \n",
    "        yf,yg,yi,yo,C_prev,C_next,H_prev,X = self.cache\n",
    "        \n",
    "        #C的导数比较简单，只与yf相关   \n",
    "        d_prev_C = dnext_C*self.yf\n",
    "        \n",
    "        d_H = dnext_H + dout_H\n",
    "        #求其他导数是时候先求四个中间结果y_f,y_g,y_i,y_o的导数\n",
    "        #d_f，d_g，d_i导数包含两个部分，一是从C_next传回来,一部分是从H_next传到C_next再传回来\n",
    "        #d_o导数包含一个部分，从H_next传回来\n",
    "        d_f = dnext_C*C_prev + d_H*yo*C_prev\n",
    "        d_g = dnext_C*yi + d_H*yo*yi\n",
    "        d_i = dnext_C*yg + d_H*yo*yg\n",
    "        d_o = dout*self.tanh(C_next)\n",
    "        \n",
    "        #再求激活函数的导数\n",
    "        d_f = self.back_sigmoid(yf,d_f)\n",
    "        d_g = self.back_tanh(yg,d_g)\n",
    "        d_i = self.back_sigmoid(yi,d_i)\n",
    "        d_o = self.back_sigmoid(yo,d_o)\n",
    "        \n",
    "        #np.hstack()的作用是将array在水平方向按顺序进行合并\n",
    "        d_y_tmp = np.hstack((d_f,d_g,d_i,d_o))\n",
    "        \n",
    "        d_Wx = np.dot(X.T,d_y_tmp)\n",
    "        d_X = np.dot(d_y_tmp,Wx.T)\n",
    "        \n",
    "        d_Wh = np.dot(H_prev.T,d_y_tmp)\n",
    "        d_prev_H = np.dot(d_y_tmp,Wh.T)\n",
    "        \n",
    "        d_Bl = np.sum(d_y_tmp,axis=0)\n",
    "        \n",
    "        #将最终导数存入属性或返回\n",
    "        self.grads['Wx'] = d_Wx\n",
    "        self.grads['Wh'] = d_Wh\n",
    "        self.grads['Bl'] = d_Bl \n",
    "        \n",
    "        return d_X,d_prev_C,d_prev_H\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6174c5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TimeLSTM():\n",
    "    \n",
    "    def __init__(self,Wx,Wh,Bl,T,Ts,statful=False):\n",
    "        \n",
    "        '''\n",
    "        T = time\n",
    "        Ts = truncated_size 反向传播截断的距离\n",
    "        statful=False 不进行truncate,即不进行反向传播截断\n",
    "        '''\n",
    "        self.params = {}\n",
    "        self.params['Wx'] = Wx\n",
    "        self.params['Wh'] = Wh\n",
    "        self.params['Bl'] = Bl\n",
    "        \n",
    "        self.Ts = Ts\n",
    "        self.statful = statful\n",
    "            \n",
    "        self.grads = {}\n",
    "        self.grads['Wx'] = np.zeros_like(Wx)\n",
    "        self.grads['Wh'] = np.zeros_like(Wh)\n",
    "        self.grads['Bl'] = np.zeros_like(Bl)\n",
    "        \n",
    "        self.layers = []\n",
    "        for i in range(T):\n",
    "            layer = Lstm(Wx,Wh,Bl)\n",
    "            self.layers.append(layer)\n",
    "        \n",
    "    def forward(self,Xs):\n",
    "        \n",
    "        #如果输入数据的时间序列长度与初始化的time_lstm时间序列长度不一致，则报错\n",
    "        Wx =self.params['Wx']\n",
    "        N,T,D = Xs.shape\n",
    "        _,H = Wx.shape\n",
    "        \n",
    "        self.h = 0 \n",
    "        self.c = 0\n",
    "        \n",
    "        self.cache = []\n",
    "        \n",
    "        Hs = np.empty((N,T,H))\n",
    "        \n",
    "        if T != len(self.layers):\n",
    "            print('长度匹配错误，输入数据的时间序列长度与生成的TimeLstm时间序列长度不一致，请检查')\n",
    "            return\n",
    "\n",
    "        for t,layer in enumerate(self.layers):\n",
    "            Xt= Xs[:,t,:]\n",
    "            H_prev = self.h\n",
    "            C_prev = self.c\n",
    "            self.h,self.c, = layer.forward(Xt,H_prev,C_prev)\n",
    "            Hs[:,t,:] = self.h\n",
    "        \n",
    "        self.cache.append(Xs)\n",
    "        \n",
    "        return Hs\n",
    "        \n",
    "    def backward(self,d_Hs):\n",
    "        \n",
    "        Xs = self.cache[0]\n",
    "        N,T,D = Xs.shape\n",
    "        \n",
    "        self.d_h = 0 \n",
    "        self.d_c = 0\n",
    "        \n",
    "        d_Xs = np.empty_like(Xs)\n",
    "        \n",
    "        #每次反向传播前初始化权重和偏置的导数,读取正向传播过程中的中间结果\n",
    "        self.grads['Wx'] = np.empty_like(Wx)\n",
    "        self.grads['Wh'] = np.empty_like(Wh)\n",
    "        self.grads['Bl'] = np.empty_like(Bl)\n",
    "        \n",
    "        grads=[0,0,0]\n",
    "        \n",
    "        for t in reversed(range(T)):\n",
    "            \n",
    "            #当使用truncate时，即将statful设置为True时，在固定位置，截断反向传播的H,C的导数\n",
    "            if self.statful and (T-t+1)%self.Ts == 0:\n",
    "                self.d_h = 0\n",
    "                self.d_c = 0            \n",
    "            dnext_C = self.d_c\n",
    "            dnext_H = self.d_h\n",
    "            \n",
    "            dout_H = d_Hs[:,t,:]\n",
    "            \n",
    "            d_Xt,self.d_c,self.d_h = self.layer[t].backward(dnext_C,dnext_H,dout_H)\n",
    "            \n",
    "            d_Xs[:,t,:] = d_Xt\n",
    "            \n",
    "            grads[0] += self.layer[t].grads['Wx']\n",
    "            grads[1] += self.layer[t].grads['Wh']\n",
    "            grads[2] += self.layer[t].grads['Bl']\n",
    "            \n",
    "        self.grads['Wx'] = grads[0]\n",
    "        self.grads['Wh'] = grads[1]\n",
    "        self.grads['Bl'] = grads[2]\n",
    "        \n",
    "        return d_Xs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dff2e3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout():\n",
    "    \n",
    "    def __init__(self,drop_rate=0.5,train_flg = True):\n",
    "        \n",
    "        self.dr = drop_rate\n",
    "        self.cache = []\n",
    "        self.train_flg = train_flg  #只在训练的时候用dropout防止过拟合，当预测的时候不用dropout\n",
    "        \n",
    "    def forward(self,X):\n",
    "        \n",
    "        if self.train_flg:\n",
    "            \n",
    "            #np.random.rand()根据输入形状返回一个[0,1)之间的【均匀】分布，randn()返回的是[0,1)之间的【正态】分布\n",
    "            self.mask = np.random.rand(*X.shape)<self.dr\n",
    "\n",
    "            #这里除以1-self.dr的原因是，为了在drop掉一部分数据之后，整个数据仍然能保持期望值不变。期望值如果发生了改变，整个数据相当于发生了平移，对最终的预测结果有影响\n",
    "            self.mask = self.mask/(1-self.dr)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            self.mask = np.ones_like(X)\n",
    "            \n",
    "        return X*self.mask \n",
    "    \n",
    "    def backward(self,dout):\n",
    "        \n",
    "        dx = dout*self.mask\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0110fbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeAffine():\n",
    "    \n",
    "    def __init__(self,Wa,Ba):\n",
    "        \n",
    "        self.params = {}\n",
    "        self.params['Wa'] = Wa\n",
    "        self.params['Ba'] = Ba\n",
    "        \n",
    "        self.grads = {}\n",
    "        self.grads['Wa'] = np.zeros_like(Wa)\n",
    "        self.grads['Ba'] = np.zeros_like(Ba)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        \n",
    "        Wa,Ba = self.params['Wa'],self.params['Ba']\n",
    "        N,T,H = X.shape\n",
    "        H,D = Wa.shape\n",
    "\n",
    "        #全链接层的权重只有两个维度，而输入的X包含N,T,H三个维度，需要设置两个中间变量X_tmp和y_tmp来进行转换\n",
    "        X_tmp = X.reshape((N*T,H))\n",
    "        self.params['X_tmp'] = X_tmp\n",
    "\n",
    "        y_tmp = np.dot(X_tmp,Wa) + Ba\n",
    "        \n",
    "        y = y_tmp.reshape((N,T,D))\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        \n",
    "        Wa,Ba,X_tmp = self.params['Wa'],self.params['Ba'],self.params['X_tmp']\n",
    "        N,T,D = dout.shape\n",
    "        H,D = Wa.shape\n",
    "        \n",
    "        #反向传播与前向传播同理，而输入的dout是三个维度的，无法直接与W.T和X.T进行矩阵点乘，所以同样也需要进行中间转换\n",
    "        dout_tmp = dout.reshape((N*T,D))\n",
    "        \n",
    "        dx_tmp = np.dot(dout_tmp,Wa.T)\n",
    "        dx = dx_tmp.reshape((N,T,H))\n",
    "        \n",
    "        self.grads['Wa'] = np.dot(X_tmp.T,dout_tmp)\n",
    "        self.grads['Ba'] = np.sum(dout_tmp,axis=0)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b80dc5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSoftmax():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.params = {}\n",
    "        self.grads = {}\n",
    "        \n",
    "    def __forward(self,X):\n",
    "        \n",
    "        self.params['X'] = X\n",
    "        N,T,D = X.shape\n",
    "        \n",
    "        X_tmp = X.reshape((N*T,D))\n",
    "        y_tmp = np.exp(X_tmp)/np.sum(np.exp(X_tmp),axis=1,keepdims=True).repeat(D,axis=1)\n",
    "        y = y_tmp.reshape((N,T,D))\n",
    "        \n",
    "        #存储y,在反向传播中会用到\n",
    "        self.params['y'] = y\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        \n",
    "        y = self.params['y']\n",
    "        \n",
    "        #softmax的导数为y(1-y)，其中y为每个元素正向传播的结果\n",
    "        dx = dout*(y*(1-y))\n",
    "        \n",
    "        self.grads['X'] = dx \n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6cbdfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimePerPlexityLoss():\n",
    "    \n",
    "    def __init__(self,eps = 1e-8):\n",
    "        \n",
    "        #因为采用困惑度作为loss衡量标准，为了防止出现0为分母的情况，所以需要加上一个微小数eps\n",
    "        self.params ={}\n",
    "        self.grads = {}\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self,X,T_id):\n",
    "        \n",
    "        #input数据T_id需要用id的方式表示,T_id.shape = (N,T,1)\n",
    "        #只需要把每个长度为D的向量的序号为id的那个值拿出来，计算它的倒数，即为这个单词的损失\n",
    "        #最终损失需要把N*T个单词的损失全加起来之后除以N*T,表示每个单词的平均困惑度。\n",
    "        #如果完全拟合的情况下，即最终损失最小的值，为1\n",
    "        \n",
    "        loss_t = 0\n",
    "        \n",
    "        N,T,D = X.shape\n",
    "        self.params['X'] = X\n",
    "        self.params['T_id'] = T_id\n",
    "        \n",
    "        for i in range(N):\n",
    "            for j in range(T):\n",
    "                loss = 1/(X[i,j,T_id[i,j,0]] + self.eps)\n",
    "                loss_t += loss\n",
    "        \n",
    "        loss_t = loss_t/(N*T)\n",
    "        \n",
    "        return loss_t\n",
    "    \n",
    "    def backward(self,dout=1):\n",
    "        \n",
    "        X,T_id = self.params['X'],self.params['T_id']\n",
    "        N,T,D = X.shape\n",
    "        \n",
    "        dx = np.zeros_like(X)\n",
    "        for i in range(N):\n",
    "            for j in range(T):\n",
    "                dx[i,j,T_id[i,j,0]] += dout*(-1/(X[i,j,T_id[i,j,0]]**2))\n",
    "        \n",
    "        self.grads['X'] =dx\n",
    "        \n",
    "        return dx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
