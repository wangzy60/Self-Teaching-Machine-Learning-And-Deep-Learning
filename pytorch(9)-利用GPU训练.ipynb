{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cb6db05",
   "metadata": {},
   "source": [
    "### 都知道在torch中用GPU训练速度更快，但是需要把什么放在GPU上，怎么放，怎么判断放上去没有，这是这个文件想要说明的问题\n",
    "\n",
    "#### 首先，需要把什么放在GPU上？\n",
    "回答：需要把数据，网络模型，以及部分情况下的损失模型放到GUP上。数据、网络模型后面细说，部分情况下的损失函数指的是什么呢？具体是这样：GPU运算起来更快指的是一些矩阵的运算，而运算的内容就是各种矩阵，其中，数据中有输入矩阵，网络模型中有权重和偏置矩阵，所以都需要放到GPU上，而损失模型大部分时候都含有矩阵，除非某些自定的特殊的损失函数，所以，综上，数据、网络模型都需要放到GPU上，而损失函数需要分情况，当损失函数含有参数或需要跟其他数据进行矩阵运算时，需要将损失函数也放到GPU上，不然不需要。\n",
    "#### 其次，怎么放？\n",
    "放之前，需要先使用torch.cuda.is_available()方法来判断设备上是否有GPU设备，如果有，一般来说有两种不同的方法：\n",
    "一、在创建参数，网络模型和损失模型的时候指定设备，在这之前，我们要先创建cuda设备。\n",
    "二、先在cpu上创建参数、网络模型和损失函数模型，再将这些内容转移到cuda上，可以使用两个不同的函数分别实现，.to(device)和.cuda()。\n",
    "#### 第三，怎么检查上面说的这些在不在GPU上？\n",
    "回答：对于数据来说，比较简单，所有的数据都有两种属性能表明数据在哪，分别是.device和.is_cuda。对于网络模型和损失函数模型，需要通过他们的参数所在的位置来判断在哪，通过next(model.parameters())来获取参数，然后获取参数的.device或.is_cuda属性来判断。\n",
    "#### 最后，如果电脑上有多个GPU，希望让不同的GPU设备上进行指定数据的运算怎么办？\n",
    "回答：使用.to(device= )函数，可以将数据、网络模型、损失函数放到指定的GPU设备上。\n",
    "#### 接下来进行一些详细说明\n",
    "#### 1、 在cuda设备上直接创建数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fa7163b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f6a4521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No cuda,return cpu device\n"
     ]
    }
   ],
   "source": [
    "#先检测设备上是否有GPU设备\n",
    "has_cuda = torch.cuda.is_available()\n",
    "\n",
    "#根据是否有GPU，创建设备\n",
    "#torch.device()传入的设备名称字符串要求：Expected one of \n",
    "#cpu, cuda, ipu, xpu, mkldnn, opengl, opencl, ideep, hip, ve, ort, mps, xla, lazy, vulkan, meta, hpu\n",
    "if has_cuda:\n",
    "    my_device = torch.device('cuda')\n",
    "    print('found cuda,return cuda device')\n",
    "else:\n",
    "    my_device = torch.device('cpu')\n",
    "    print('No cuda,return cpu device')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee15332b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#在device上创建数据\n",
    "train_data = torch.Tensor([1,2,3],device=my_device)\n",
    "\n",
    "#判断创建的数据是否在cuda上\n",
    "print(train_data.is_cuda)\n",
    "#或者是\n",
    "print(train_data.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d3b2d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#在device上创建torch.nn库下已经实现的网络模型\n",
    "my_model1 = torch.nn.Conv2d(16,32,2,device=my_device)\n",
    "\n",
    "#检查模型是否在cuda设备上：不能直接检查模型的位置，但是可以通过检查模型参数的位置来判断\n",
    "next(my_model1.parameters()).device\n",
    "next(my_model1.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "862928a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#在device上创建自定义的网络模型，同样需要对网络层指定device=关键字参数\n",
    "class My_Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn = torch.nn.Sequential(torch.nn.Conv2d(16,32,2,device=my_device))\n",
    "    def forward(self,x):\n",
    "        return self.cnn(x)\n",
    "\n",
    "#检查模型是否在cuda上\n",
    "my_model2 = My_Model()\n",
    "next(my_model2.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e31cb66e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSELoss()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#在device上创建损失函数：torch.nn下的已有的损失函数没有device参数，但是所有的损失函数都是继承自torch.nn.Module类\n",
    "#而torch.nn.Module基类有一个cuda()函数，功能是将模型上的所有参数和缓存都移动到cuad上\n",
    "#所以损失函数只能在创建后再转移到cuda上\n",
    "my_loss = torch.nn.MSELoss()\n",
    "my_loss.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d516368b",
   "metadata": {},
   "source": [
    "#### 2、先创建数据，模型，然后统一转移到GPU上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff939316",
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用默认设备创建数据、网络模型、损失模型\n",
    "data2 = torch.Tensor([1,2,3])\n",
    "model2 = torch.nn.Conv2d(16,32,2)\n",
    "loss2 = torch.nn.MSELoss()\n",
    "\n",
    "cuda_list = []\n",
    "cuda_list.append(data2)\n",
    "cuda_list.append(model2)\n",
    "cuda_list.append(loss2)\n",
    "\n",
    "#使用.cuda()函数，将需要的内容转移到GPU上\n",
    "for content in cuda_list:\n",
    "    content.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17e79e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将网络模型在gpu上训练\n",
    "model = Model()\n",
    "model = model.cuda()\n",
    "\n",
    "# 损失函数在gpu上训练\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn = loss_fn.cuda()\n",
    "\n",
    "# 数据在gpu上训练\n",
    "for data in dataloader:                        \n",
    "\timgs, targets = data\n",
    "\timgs = imgs.cuda()\n",
    "\ttargets = targets.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f1a742",
   "metadata": {},
   "source": [
    "#### 3、一些特殊的例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db0e803",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''loss函数模型需要放到GPU上的例子'''\n",
    "\n",
    "output = torch.randn(10, 10, requires_grad=True, device='cuda')\n",
    "target = torch.randint(0, 10, (10,), device='cuda')\n",
    "\n",
    "#这里为损失函数增加了权重，参与了矩阵运算\n",
    "weight = torch.empty(10).uniform_(0, 1)\n",
    "criterion = nn.CrossEntropyLoss(weight=weight)\n",
    "\n",
    "loss = criterion(output, target) # error\n",
    "> RuntimeError: Expected object of device type cuda but got device type cpu for argument \n",
    "#3 'weight' in call to _thnn_nll_loss_forward\n",
    "\n",
    "criterion.cuda()\n",
    "loss = criterion(output, target) # works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de32c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''一个使用cuda进行完整训练的例子，来自CSDN'''\n",
    "\n",
    "# 以 CIFAR10 数据集为例，展示一下完整的模型训练套路，完成对数据集的分类问题\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "\n",
    "# 准备数据集\n",
    "train_data = torchvision.datasets.CIFAR10(root=\"dataset\", train=True, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "test_data = torchvision.datasets.CIFAR10(root=\"dataset\", train=False, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "\n",
    "# 获得数据集的长度 len(), 即length\n",
    "train_data_size = len(train_data)\n",
    "test_data_size = len(test_data)\n",
    "\n",
    "# 格式化字符串, format() 中的数据会替换 {}\n",
    "print(\"训练数据集及的长度为: {}\".format(train_data_size))\n",
    "print(\"测试数据集及的长度为: {}\".format(test_data_size))\n",
    "\n",
    "# 利用DataLoader 来加载数据\n",
    "train_dataloader = DataLoader(train_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "# 创建网络模型\n",
    "class Model(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 5, 1, 2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 32, 5, 1, 2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 5, 1, 2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*4*4, 64),\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = self.model(input)\n",
    "        return input\n",
    "\n",
    "model = Model()\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()                        # 在 GPU 上进行训练\n",
    "\n",
    "# 创建损失函数\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "if torch.cuda.is_available():\n",
    "    loss_fn = loss_fn.cuda()                    # 在 GPU 上进行训练\n",
    "\n",
    "# 优化器\n",
    "learning_rate = 1e-2        # 1e-2 = 1 * (10)^(-2) = 1 / 100 = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# 设置训练网络的一些参数\n",
    "total_train_step = 0                        # 记录训练的次数\n",
    "total_test_step = 0                         # 记录测试的次数\n",
    "epoch = 10                                  # 训练的轮数\n",
    "\n",
    "# 添加tensorboard\n",
    "writer = SummaryWriter(\"logs_train\")\n",
    "start_time = time.time()                    # 开始训练的时间\n",
    "for i in range(epoch):\n",
    "    print(\"------第 {} 轮训练开始------\".format(i+1))\n",
    "\n",
    "    # 训练步骤开始\n",
    "    for data in train_dataloader:\n",
    "        imgs, targets = data\n",
    "        if torch.cuda.is_available():\n",
    "            imgs = imgs.cuda()\n",
    "        targets = targets.cuda()            # 在gpu上训练\n",
    "        outputs = model(imgs)               # 将训练的数据放入\n",
    "        loss = loss_fn(outputs, targets)    # 得到损失值\n",
    "\n",
    "        optimizer.zero_grad()               # 优化过程中首先要使用优化器进行梯度清零\n",
    "        loss.backward()                     # 调用得到的损失，利用反向传播，得到每一个参数节点的梯度\n",
    "        optimizer.step()                    # 对参数进行优化\n",
    "        total_train_step += 1               # 上面就是进行了一次训练，训练次数 +1\n",
    "\n",
    "        # 只有训练步骤是100 倍数的时候才打印数据，可以减少一些没有用的数据，方便我们找到其他数据\n",
    "        if total_train_step % 100 == 0:\n",
    "            end_time = time.time()          # 训练结束时间\n",
    "            print(\"训练时间: {}\".format(end_time - start_time))\n",
    "            print(\"训练次数: {}, Loss: {}\".format(total_train_step, loss))\n",
    "            writer.add_scalar(\"train_loss\", loss.item(), total_train_step)\n",
    "\n",
    "\n",
    "    # 如何知道模型有没有训练好，即有咩有达到自己想要的需求\n",
    "    # 我们可以在每次训练完一轮后，进行一次测试，在测试数据集上跑一遍，以测试数据集上的损失或正确率评估我们的模型有没有训练好\n",
    "\n",
    "    # 顾名思义，下面的代码没有梯度，即我们不会利用进行调优\n",
    "    total_test_loss = 0\n",
    "    total_accuracy = 0                                      # 准确率\n",
    "    with torch.no_grad():\n",
    "        for data in test_dataloader:                        # 测试数据集中取数据\n",
    "            imgs, targets = data\n",
    "            if torch.cuda.is_available():\n",
    "                imgs = imgs.cuda()                          # 在 GPU 上进行训练\n",
    "                targets = targets.cuda()\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, targets)                # 这里的 loss 只是一部分数据(data) 在网络模型上的损失\n",
    "            total_test_loss = total_test_loss + loss        # 整个测试集的loss\n",
    "            accuracy = (outputs.argmax(1) == targets).sum() # 分类正确个数\n",
    "            total_accuracy += accuracy                      # 相加\n",
    "\n",
    "    print(\"整体测试集上的loss: {}\".format(total_test_loss))\n",
    "    print(\"整体测试集上的正确率: {}\".format(total_accuracy / test_data_size))\n",
    "    writer.add_scalar(\"test_loss\", total_test_loss)\n",
    "    writer.add_scalar(\"test_accuracy\", total_accuracy / test_data_size, total_test_step)\n",
    "    total_test_loss += 1                                    # 测试完了之后要 +1\n",
    "\n",
    "    torch.save(model, \"model_{}.pth\".format(i))\n",
    "    print(\"模型已保存\")\n",
    "\n",
    "writer.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
