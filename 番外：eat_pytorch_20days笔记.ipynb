{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5709d92c",
   "metadata": {},
   "source": [
    "### 一、完整流程总结"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d125dae",
   "metadata": {},
   "source": [
    "#### 1、准备数据\n",
    "#### 2、数据可视化\n",
    "#### 3、数据和模型管道搭建\n",
    "先求数据集有多长\n",
    "\n",
    "然后生成索引列表\n",
    "\n",
    "索引列表要打散\n",
    "\n",
    "再来创建batch索引生成器\n",
    "\n",
    "index_select要用LongTensor\n",
    "\n",
    "这里就该转换了\n",
    "\n",
    "indexes记得要用min\n",
    "\n",
    "避免最后一个batch不够数\n",
    "\n",
    "返回要用yield\n",
    "\n",
    "index_select还要选维度\n",
    "\n",
    "要选最大维度才可以\n",
    "#### 4、定义模型\n",
    "参数定义\n",
    "\n",
    "正向传播\n",
    "\n",
    "损失函数\n",
    "\n",
    "准确率预测\n",
    "#### 5、定义单步训练\n",
    "正向传播求估值\n",
    "\n",
    "估值标签求损失\n",
    "\n",
    "反向传播求梯度\n",
    "\n",
    "不记梯度改参数\n",
    "\n",
    "梯度归零下一轮\n",
    "#### 6、训练模型\n",
    "先定要for多少epoch\n",
    "\n",
    "再for每个mini_batch\n",
    "\n",
    "循环里面主要放单步\n",
    "\n",
    "记得返回计算损失值\n",
    "\n",
    "每当完成一个epoch\n",
    "\n",
    "打印信息看看怎么样\n",
    "#### 7、结果可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9bea824d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchkeras import KerasModel \n",
    "from torchkeras import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d24996",
   "metadata": {},
   "source": [
    "### 二、tensor数据生成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3554fdb",
   "metadata": {},
   "source": [
    "#### 1、固定正态分布随机数\n",
    "#### torch.randn()\n",
    "\n",
    "均值为0，标准差为1的随机数，参数依次为\n",
    "shape,数据类型，是否需要导数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1e33707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.8693, -0.1101, -0.0944, -0.5898, -1.2877],\n",
      "         [-0.2041,  0.4701,  2.1172,  0.1126, -1.3039],\n",
      "         [ 0.1538,  1.4080, -0.3981,  0.2262,  0.4479]],\n",
      "\n",
      "        [[-0.3301, -0.0203, -1.0164,  0.6360, -0.0439],\n",
      "         [ 0.0301,  0.1619,  2.0155,  1.0248, -0.6261],\n",
      "         [ 0.5696,  0.0786,  1.3952, -1.3562,  0.1008]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn((2,3,5),dtype=torch.float32,requires_grad= True)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e8141d",
   "metadata": {},
   "source": [
    "#### 2、指定正态分布随机数\n",
    "#### torch.normal()，传入参数包含两种形式。\n",
    "\n",
    "①分别给定平均值和方差的tensor。mean和std的元素数量必须相同，如果满足数量相同，但是形状不相同，最终输出的元素形状以mean，也就是传入的第一个tensor为准。\n",
    "\n",
    "②分别给定两个float以及一个元祖。分别表示mean和std的数值，以及输出的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ac5011f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp/ipykernel_10872/154419691.py:3: UserWarning: std and mean have the same number of elements, but are not broadcastable. This was previously a supported mode of operation, but is now deprecated and the support will be removed in version 1.6 release. Note that the current implementation reshapes std to the shape of mean, which may be incur data copies. Please ensure that std and mean are broadcastable to avoid these issues. (Triggered internally at  ..\\aten\\src\\ATen/native/DistributionTemplates.h:191.)\n",
      "  torch.normal(torch.zeros(3,5),torch.ones(5,3))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1581, -0.3717,  0.2744,  0.2189,  1.2002],\n",
       "        [ 0.4153, -2.2840,  0.6194, -0.7958, -0.5260],\n",
       "        [-0.3098,  1.7093,  0.6096,  0.0840, -0.1748]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#①给定平均值和方差的tensor，两者元素数量相同，但是形状不同，能运行，但是会提醒\n",
    "\n",
    "torch.normal(torch.zeros(3,5),torch.ones(5,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e81c2012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.7387,  3.8171,  4.5361,  0.7913,  3.3621],\n",
       "        [ 4.3505,  4.0955, -2.5083, -0.6528,  2.6160],\n",
       "        [ 2.4690, -5.2908,  3.6296,  2.0837,  3.5731]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#②满足平均值1，方差3，形状（3,5）的随机数组\n",
    "\n",
    "torch.normal(1,3,(3,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480fc933",
   "metadata": {},
   "source": [
    "#### 3、numpy对象和tensor对象相互转换  \n",
    "#### tensor.numpy()， torch.from_numpy(ndarray)\n",
    "\n",
    "①tensor转numpy，共享内存：\n",
    "\n",
    "tensor.numpy()。无参数,tensor对象有grads时使用\n",
    "\n",
    "tensor.detach().numpy()。无参数，tensor对象没有grads时使用\n",
    "\n",
    "②numpy转tensor，共享内存：\n",
    "\n",
    "torch.from_numpy(ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5da41e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "①此处X包含grads，如果直接用.numpy()进行转换，\n",
    "torch库会提示，需要对tensor先使用detach()，之后才能转换成numpy对象\n",
    "'''\n",
    "#X.detach()会返回一个新的tensor，所以更新X也不会导致Z变化\n",
    "Z = X.detach().numpy()\n",
    "print(type(X))\n",
    "print(type(Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b4c11821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: tensor([1., 2., 3.])\n",
      "Z: [1. 2. 3.] \n",
      "\n",
      "A: tensor([5., 2., 3.])\n",
      "Z: [5. 2. 3.]\n"
     ]
    }
   ],
   "source": [
    "#tensor转numpy,不共享内存\n",
    "A= torch.tensor([1.,2.,3.])\n",
    "print('A:',A)\n",
    "Z= A.numpy()\n",
    "print('Z:',Z,'\\n')\n",
    "\n",
    "A[0]=5\n",
    "print('A:',A)\n",
    "print('Z:',Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ac9e7de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y: tensor([5., 2., 3.])\n",
      "Y: tensor([10.,  2.,  3.]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "②numpy转tensor，共享内存,改变原来的numpy数据时，tensor会一起更新\n",
    "'''\n",
    "#将numpy数据Z转换为tensor数据\n",
    "Y = torch.from_numpy(Z)\n",
    "print('Y:',Y)\n",
    "\n",
    "#修改Z的值\n",
    "Z[0]=10 \n",
    "#Y的值将一起被调整\n",
    "print('Y:',Y,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f91e70",
   "metadata": {},
   "source": [
    "### 三、tensor数据操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda2a94b",
   "metadata": {},
   "source": [
    "#### 1、增加和删除 \n",
    "#### tensor数据的合并和切分  torch.cat(), torch.stack(), torch.split()\n",
    "\n",
    "①torch.cat()。对形状相同的tensor进行合并，且不增加维度。第一个参数是待合并tensor的集合，第二个参数是合并的维度，默认是0维，维度不能超过原有维度的范围。\n",
    "\n",
    "②torch.stack()。像栈的模式一样，对于形状相同的tensor进行合并，且增加维度。第一个参数是tensor的集合，第二个参数是合并的维度，默认是0维，维度最多可以在原有范围基础上增加1。\n",
    "\n",
    "③torch.split()。将tensor进行分隔，不改变形状。第一个参数是tensor，第二个参数是整数或者整数列表，代表需要划分的个数，第三个参数是进行划分操作的维度，默认是0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b5cd879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([3, 5]) \n",
      "\n",
      "tensor([[ 0.2569,  1.3447, -1.0964, -0.7743,  0.4888],\n",
      "        [-0.3436, -0.7069,  0.2946, -0.0270, -0.3178],\n",
      "        [ 0.2489, -0.2260, -0.0092, -0.4739,  0.2413],\n",
      "        [ 0.2569,  1.3447, -1.0964, -0.7743,  0.4888],\n",
      "        [-0.3436, -0.7069,  0.2946, -0.0270, -0.3178],\n",
      "        [ 0.2489, -0.2260, -0.0092, -0.4739,  0.2413]], grad_fn=<CatBackward0>)\n",
      "torch.Size([6, 5]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "①torch.cat()。对形状相同的tensor进行合并，且不增加维度。\n",
    "第一个参数是待合并tensor的集合，\n",
    "第二个参数是合并的维度，默认是0维。\n",
    "'''\n",
    "print('X shape:',X.shape,'\\n')\n",
    "#对2个X在0维进行cat合并\n",
    "print(torch.cat((X,X),0))\n",
    "print(torch.cat((X,X),0).shape,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "234487da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2569,  1.3447, -1.0964, -0.7743,  0.4888,  0.2569,  1.3447, -1.0964,\n",
      "         -0.7743,  0.4888],\n",
      "        [-0.3436, -0.7069,  0.2946, -0.0270, -0.3178, -0.3436, -0.7069,  0.2946,\n",
      "         -0.0270, -0.3178],\n",
      "        [ 0.2489, -0.2260, -0.0092, -0.4739,  0.2413,  0.2489, -0.2260, -0.0092,\n",
      "         -0.4739,  0.2413]], grad_fn=<CatBackward0>)\n",
      "torch.Size([3, 10]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#对2个X在1维进行cat合并\n",
    "print(torch.cat((X,X),1))\n",
    "print(torch.cat((X,X),1).shape,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b3b3a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([3, 5]) \n",
      "\n",
      "tensor([[[ 0.2569,  1.3447, -1.0964, -0.7743,  0.4888],\n",
      "         [-0.3436, -0.7069,  0.2946, -0.0270, -0.3178],\n",
      "         [ 0.2489, -0.2260, -0.0092, -0.4739,  0.2413]],\n",
      "\n",
      "        [[ 0.2569,  1.3447, -1.0964, -0.7743,  0.4888],\n",
      "         [-0.3436, -0.7069,  0.2946, -0.0270, -0.3178],\n",
      "         [ 0.2489, -0.2260, -0.0092, -0.4739,  0.2413]]],\n",
      "       grad_fn=<StackBackward0>)\n",
      "torch.Size([2, 3, 5]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "②torch.stack()。\n",
    "像栈的模式一样，对于形状相同的tensor进行合并，且增加维度。\n",
    "第一个参数是tensor的集合，第二个参数是合并的维度，默认是0维\n",
    "'''\n",
    "\n",
    "print('X shape:',X.shape,'\\n')\n",
    "#对2个X在0维进行stack合并\n",
    "print(torch.stack((X,X),0))\n",
    "print(torch.stack((X,X),0).shape,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37ea82c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2569,  1.3447, -1.0964, -0.7743,  0.4888],\n",
      "         [ 0.2569,  1.3447, -1.0964, -0.7743,  0.4888]],\n",
      "\n",
      "        [[-0.3436, -0.7069,  0.2946, -0.0270, -0.3178],\n",
      "         [-0.3436, -0.7069,  0.2946, -0.0270, -0.3178]],\n",
      "\n",
      "        [[ 0.2489, -0.2260, -0.0092, -0.4739,  0.2413],\n",
      "         [ 0.2489, -0.2260, -0.0092, -0.4739,  0.2413]]],\n",
      "       grad_fn=<StackBackward0>)\n",
      "torch.Size([3, 2, 5]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#对2个X在2维进行stack合并\n",
    "print(torch.stack((X,X),1))\n",
    "print(torch.stack((X,X),1).shape,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "89661d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([2, 3, 5]) \n",
      "\n",
      "X splited0 shape: torch.Size([2, 2, 5])\n",
      "X splited1 shape: torch.Size([2, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "③torch.split()。\n",
    "将tensor进行分隔，不改变形状。\n",
    "第一个参数是tensor，\n",
    "第二个参数是整数或者整数列表，代表需要划分的个数，\n",
    "第三个参数是进行划分操作的维度，默认是0\n",
    "'''\n",
    "\n",
    "#将X从第1个维度按照2个元素为一组，返回含多个tensor的列表，如果最后一组不够2，则按照其实际数量形成一个tensor\n",
    "print('X shape:',X.shape,'\\n')\n",
    "print('X splited0 shape:',torch.split(X,2,1)[0].shape)\n",
    "#对划分的区间使用列表形式，[2,1]分别表示希望第0个tensor在维度1上有2个数据，以及第1个tensor在维度1上有1个数据\n",
    "print('X splited1 shape:',torch.split(X,[2,1],1)[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ea5779",
   "metadata": {},
   "source": [
    "#### 2、修改\n",
    "#### tensor数据的修改  torch.where(), torch.index_fill(), torch.masked_fill()\n",
    "\n",
    "\n",
    "①torch.where()。对满足条件和不满足条件的元素分别赋值。第一个参数为判断语句，将根据判断语句对数据集内每个元素进行运算。对于符合判断语句的参数，赋给第二个参数(tensor或者scalar)的值，对于不符合判断语句的参数，赋给第三个参数(tensor或者scalar)的值\n",
    "\n",
    "②torch.index_fill()。对一个维度的目标索引进行直接赋值。对目标数据集(第一个参数)中的目标维度(第二个参数)中的目标索引tensor(第三个参数)，赋予给定的值(第四个参数，可以为标量或tensor)\n",
    "\n",
    "③torch.masked_fill()。对所有符合条件的进行赋值。将目标数据集(第一个参数)中，满足判断语句(第二个参数)的所有元素，用一个给定的值(第三个参数)代替。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2b00b0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id X: 2875858602080\n",
      "tensor([[[2, 1, 1, 1, 1],\n",
      "         [2, 2, 2, 1, 1],\n",
      "         [1, 2, 1, 1, 2]],\n",
      "\n",
      "        [[1, 1, 2, 2, 2],\n",
      "         [1, 2, 1, 1, 2],\n",
      "         [2, 1, 1, 1, 2]]])\n",
      "id Y: 2875877576928 \n",
      "\n",
      "tensor([[[2, 1, 1, 1, 1],\n",
      "         [2, 2, 2, 1, 1],\n",
      "         [1, 2, 1, 1, 2]],\n",
      "\n",
      "        [[1, 1, 2, 2, 2],\n",
      "         [1, 2, 1, 1, 2],\n",
      "         [2, 1, 1, 1, 2]]])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "①torch.where()。\n",
    "第一个参数为判断语句，将根据判断语句对数据集内每个元素进行运算。\n",
    "对于符合判断语句的参数，赋给第二个参数(tensor或者scalar)的值，\n",
    "对于不符合判断语句的参数，赋给第三个参数(tensor或者scalar)的值\n",
    "'''\n",
    "#第二个和第三个参数可以是标量\n",
    "print('id X:',id(X))\n",
    "Y = torch.where(X>0,1,2)\n",
    "print(Y)\n",
    "print('id Y:',id(Y),'\\n')\n",
    "\n",
    "#第二个和第三个参数也可以是tensor\n",
    "print(torch.where(X>0,torch.tensor(1),torch.tensor(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b00fe2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0000,  0.0000,  0.0000,  0.3553,  2.0771],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.2399,  0.6537],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.3208, -1.5857]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000, -1.2062, -2.2592],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.5264, -0.7635],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.9923, -0.0999]]],\n",
      "       grad_fn=<IndexFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "②torch.index_fill()。\n",
    "对目标数据集(第一个参数)中的目标维度(第二个参数)中的目标索引tensor(第三个参数)，\n",
    "赋予给定的值(第四个参数,可以为标量或tensor)\n",
    "'''\n",
    "\n",
    "#对目标数据集的第3个维度索引为0、1、2的数据，赋值0\n",
    "print(torch.index_fill(X,2,torch.tensor([0,1,2]),0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "cab7662b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0000, 1.5211, 0.5001, 0.3553, 2.0771],\n",
      "         [0.0000, 0.0000, 0.0000, 0.2399, 0.6537],\n",
      "         [0.1136, 0.0000, 0.5320, 0.3208, 0.0000]],\n",
      "\n",
      "        [[0.2852, 1.4625, 0.0000, 0.0000, 0.0000],\n",
      "         [0.7998, 0.0000, 1.0541, 1.5264, 0.0000],\n",
      "         [0.0000, 0.0989, 0.1931, 0.9923, 0.0000]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "③torch.masked_fill()。\n",
    "对所有符合条件的进行赋值。\n",
    "将目标数据集(第一个参数)中，满足判断语句(第二个参数)的所有元素，\n",
    "用一个给定的值(第三个参数)代替。\n",
    "'''\n",
    "#将X中所有小于0的值变成0\n",
    "print(X.masked_fill(X<0,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7339bee1",
   "metadata": {},
   "source": [
    "#### 3、查询\n",
    "#### 规则查询和提取 [ ], [,], [:::,], [...,]\n",
    "和数组的索引操作基本一致，准确来说遵循如下原则\n",
    "\n",
    "①如有多个方括号，第一个方括号表示对第一个维度的操作，第二个方括号表示对第二个维度的操作，依次类推\n",
    "\n",
    "②方括号内如果有逗号，表示对多行进行操作，如果每个逗号范围内只有整数，则表示选择对应维度的索引。\n",
    "\n",
    "③方括号内如果有逗号，且逗号范围内有':'，表示该维度存在切片操作，切片操作完整形式为(开始编号):(结束编号+1):(间隔)，且三个圆括号内都为int，都可选。\n",
    "\n",
    "④省略号可以表示多个冒号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d7dd1090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.7035, -1.3238, -0.2862,  0.3141, -0.4803],\n",
      "         [-0.5608, -0.8724,  1.1259,  0.5028, -1.4117],\n",
      "         [-0.7568, -0.2669,  0.7090,  0.3945,  0.3372]],\n",
      "\n",
      "        [[-0.1660,  0.5910,  0.8902, -0.0372,  0.2757],\n",
      "         [-0.4745,  0.1998, -0.9100,  0.5116, -1.3115],\n",
      "         [-0.0752, -0.5022,  1.0219,  0.5539,  0.0771]]], requires_grad=True)\n",
      "torch.Size([2, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn((2,3,5),dtype=torch.float32,requires_grad= True)\n",
    "print(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "12a005a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7035, -1.3238, -0.2862,  0.3141, -0.4803],\n",
       "        [-0.5608, -0.8724,  1.1259,  0.5028, -1.4117],\n",
       "        [-0.7568, -0.2669,  0.7090,  0.3945,  0.3372]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "①如有多个方括号，第一个方括号表示对第一个维度的操作，\n",
    "第二个方括号表示对第二个维度的操作，依次类推\n",
    "'''\n",
    "\n",
    "#选取第一个维度\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ea25ad5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.7035, -1.3238, -0.2862,  0.3141, -0.4803],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([-0.7035, -1.3238, -0.2862,  0.3141, -0.4803],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#选取第二个维度\n",
    "print(X[0][0])\n",
    "\n",
    "#或者是:\n",
    "'''\n",
    "②方括号内如果有逗号，表示对多行进行操作，\n",
    "如果每个逗号范围内只有整数，则表示选择对应维度的索引。\n",
    "'''\n",
    "print(X[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f12162ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.7035, grad_fn=<SelectBackward0>)\n",
      "tensor(-0.7035, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#选取第三个维度\n",
    "print(X[0][0][0])\n",
    "#或者是\n",
    "print(X[0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d6c45717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1660,  0.8902],\n",
       "         [-0.4745, -0.9100],\n",
       "         [-0.0752,  1.0219]]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "③方括号内如果有逗号，且逗号范围内有':'，\n",
    "表示该维度存在切片操作，\n",
    "切片操作完整形式为(开始编号):(结束编号+1):(间隔)，\n",
    "三个圆括号内都为int，且都可选。\n",
    "'''\n",
    "#获取第一个维度从索引1开始的所有，第二个维度的所有，第三个维度从0到3（不包含4）间隔为2的所有数据\n",
    "X[1::,::,0:4:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "92a50dc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1660,  0.8902],\n",
       "         [-0.4745, -0.9100],\n",
       "         [-0.0752,  1.0219]]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "④省略号可以代替多个冒号\n",
    "'''\n",
    "X[1::,...,0:4:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974c3481",
   "metadata": {},
   "source": [
    "#### 不规则查询和提取  torch.index_select(), torch.take(), torch.masked_select()\n",
    "\n",
    "\n",
    "①torch.index_select()。自动保持维度不变。第一个参数为待选取的tensor数据集，第二个参数为需要选取的维度编号，第三个参数为第二个参数代表维度下需要选取的索引，如果需要对多个维度进行选取，可以将该方法嵌套使用\n",
    "\n",
    "②torch.take()。维度会发生变化。第一个参数为待选取的tensor数据集，第二个参数为待选取的数据在数据集中的序号组成的tensor对象，可以包含多个序号，这里的序号指的是将目标数据集看做一维之后，目标数据在一维中的编号。\n",
    "\n",
    "③torch.masked_select()。维度会发生变化。第一个参数为待选取的tensor数据集，第二个参数为一个数据集的条件判断语句，如：tensor>0，其中tensor为待选取的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b4a6af83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0651,  1.5211,  0.5001,  0.3553,  2.0771],\n",
      "         [-0.3993, -0.1759, -0.3652,  0.2399,  0.6537],\n",
      "         [ 0.1136, -0.6993,  0.5320,  0.3208, -1.5857]],\n",
      "\n",
      "        [[ 0.2852,  1.4625, -1.5551, -1.2062, -2.2592],\n",
      "         [ 0.7998, -0.5394,  1.0541,  1.5264, -0.7635],\n",
      "         [-0.6598,  0.0989,  0.1931,  0.9923, -0.0999]]], requires_grad=True)\n",
      "torch.Size([2, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn((2,3,5),dtype=torch.float32,requires_grad= True)\n",
    "print(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c64fb531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.3993, -0.1759, -0.3652,  0.2399,  0.6537]],\n",
      "\n",
      "        [[ 0.7998, -0.5394,  1.0541,  1.5264, -0.7635]]],\n",
      "       grad_fn=<IndexSelectBackward0>)\n",
      "torch.Size([2, 1, 5]) \n",
      "\n",
      "\n",
      "tensor([[[-0.3993, -0.3652,  0.6537]],\n",
      "\n",
      "        [[ 0.7998,  1.0541, -0.7635]]], grad_fn=<IndexSelectBackward0>)\n",
      "torch.Size([2, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "①torch.index_select()。\n",
    "第一个参数为待选取的tensor数据集，\n",
    "第二个参数为需要选取的维度编号，\n",
    "第三个参数为第二个参数代表维度下需要选取的索引，\n",
    "如果需要对多个维度进行选取，可以将该方法嵌套使用\n",
    "'''\n",
    "#选择第1个维度的第1个数据\n",
    "print(torch.index_select(X, 1, torch.tensor(1)))\n",
    "print(X.index_select(1, torch.tensor(1)).shape,'\\n\\n')\n",
    "\n",
    "#对第1个维度的第1个数据和第2个维度的第2个数据进行选取\n",
    "#因为维度保持不变，所以嵌套使用时，选取维度时还是要参考X数据的维度\n",
    "print(torch.index_select(torch.index_select(X, 1, torch.tensor(1)),2,torch.tensor([0,2,4])))\n",
    "print(torch.index_select(torch.index_select(X, 1, torch.tensor(1)),2,torch.tensor([0,2,4])).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "67f487bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0651,  0.3553, -0.3993], grad_fn=<TakeBackward0>)\n",
      "tensor([-0.0651,  0.3553, -0.3993], grad_fn=<TakeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "②torch.take()。\n",
    "第一个参数为待选取的tensor数据集，\n",
    "第二个参数为待选取的数据在数据集中的序号组成的tensor对象，\n",
    "可以包含多个序号，\n",
    "这里的序号指的是将目标数据集看做一维之后，目标数据在一维中的编号。\n",
    "'''\n",
    "\n",
    "#抽取X数据集中的第0、3、5个数据（按一维计算）\n",
    "print(X.take(torch.tensor([0,3,5])))\n",
    "print(torch.take(X,torch.tensor([0,3,5])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7367cd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.5211, 0.5001, 0.3553, 2.0771, 0.2399, 0.6537, 0.1136, 0.5320, 0.3208,\n",
      "        0.2852, 1.4625, 0.7998, 1.0541, 1.5264, 0.0989, 0.1931, 0.9923],\n",
      "       grad_fn=<MaskedSelectBackward0>)\n",
      "tensor([-0.0651, -0.3993, -0.1759, -0.3652, -0.6993, -1.5857, -1.5551, -1.2062,\n",
      "        -2.2592, -0.5394, -0.7635, -0.6598, -0.0999],\n",
      "       grad_fn=<MaskedSelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "③torch.masked_select()。\n",
    "第一个参数为待选取的tensor数据集，\n",
    "第二个参数为一个数据集的条件判断语句，\n",
    "如：tensor>0，其中tensor为待选取的数据集\n",
    "'''\n",
    "\n",
    "#选取X中所有大于0的元素\n",
    "print(X.masked_select(X>0))\n",
    "\n",
    "#选取X中所有小于0的元素\n",
    "print(torch.masked_select(X,X<0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70d1a69",
   "metadata": {},
   "source": [
    "#### 4、形状调整\n",
    "#### tensor数据形状调整  torch.reshape()&torch.squeeze()/torch.unsqueeze(), torch.transpose()\n",
    "\n",
    "①torch.reshape()&torch.squeeze()/torch.unsqueeze()。reshape根据传入的维度形状对数据集进行变形,传递的维度形状中可以有一个数值为-1，为-1的维度将由库自动计算得出。增加元素数量为0的维度需要用到unsqueeze，减少元素数量为0的维度需要用到squeeze\n",
    "\n",
    "②torch.transpose()。和numpy的transpose能依次指定每个轴的顺序不太一样，此处的transpose只能指定前两个轴进行交换，且不对原对象进行修改，会返回一个新的对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8cad3211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: torch.Size([2, 3, 5])\n",
      "Y.shape: torch.Size([30, 1])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "①torch.reshape()。根据传入的维度形状对数据集进行变形\n",
    "\n",
    "'''\n",
    "Y = torch.reshape(X,(30,-1))\n",
    "print('X.shape:',X.shape)\n",
    "print('Y.shape:',Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "628f6636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: torch.Size([2, 3, 5])\n",
      "Y.shape: torch.Size([5, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "②torch.transpose()。只能指定对哪两个维度进行交换。\n",
    "且不影响原变量，而是返回一个新的对象\n",
    "\n",
    "'''\n",
    "#对第2个维度和第0个维度进行交换\n",
    "Y = torch.transpose(X,2,0)\n",
    "print('X.shape:',X.shape)\n",
    "print('Y.shape:',Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0a159315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z.shape: (2, 3, 5)\n",
      "W.shape: (5, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "#numpy中的transpose可以指定所有轴的次序\n",
    "Z = X.detach().numpy()\n",
    "print('Z.shape:',Z.shape)\n",
    "W = Z.transpose(2,0,1)\n",
    "print('W.shape:',W.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35a5075",
   "metadata": {},
   "source": [
    "### 四、tensor数据运算\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f80586",
   "metadata": {},
   "source": [
    "#### 1、标量运算\n",
    "\n",
    "加减乘除余数等\n",
    "\n",
    "torch.max()   \n",
    "求每个位置的最大值\n",
    "\n",
    "torch.min()   \n",
    "求每个位置的最小值\n",
    "\n",
    "torch.round()   \n",
    "求整数:每个位置四舍五入\n",
    "\n",
    "torch.floor()   \n",
    "求整数：每个位置靠近的两个整数中小的那一个\n",
    "\n",
    "torch.ceil()   \n",
    "求整数：每个位置靠近的两个整数中大的那一个\n",
    "\n",
    "torch.trunc()   \n",
    "求整数：每个位置直接把小数部分去掉\n",
    "\n",
    "torch.fmod()   \n",
    "求余数：余数有正有负\n",
    "\n",
    "torch.remainder()   \n",
    "求余数：余数一定为正\n",
    "\n",
    "torch.clamp()   \n",
    "限定最大和最小值，超出的改成最大或最小值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "d086f61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tensor([[0.2635, 0.6008],\n",
      "        [0.0630, 1.0336]])\n",
      "b: tensor([[ 0.0929,  1.0667],\n",
      "        [ 2.2848, -0.2672]]) \n",
      "\n",
      "torch.max(a,b):\n",
      " tensor([[0.2635, 1.0667],\n",
      "        [2.2848, 1.0336]]) \n",
      "\n",
      "torch.min(a,b):\n",
      " tensor([[ 0.0929,  0.6008],\n",
      "        [ 0.0630, -0.2672]]) \n",
      "\n",
      "torch.round(a):\n",
      " tensor([[0., 1.],\n",
      "        [0., 1.]]) \n",
      "\n",
      "torch.floor(a):\n",
      " tensor([[0., 0.],\n",
      "        [0., 1.]]) \n",
      "\n",
      "torch.ceil(a):\n",
      " tensor([[1., 1.],\n",
      "        [1., 2.]]) \n",
      "\n",
      "torch.trunc(a):\n",
      " tensor([[0., 0.],\n",
      "        [0., 1.]]) \n",
      "\n",
      "torch.fmod(a,1):\n",
      " tensor([[0.2635, 0.6008],\n",
      "        [0.0630, 0.0336]]) \n",
      "\n",
      "torch.remainder(a,1):\n",
      " tensor([[0.2635, 0.6008],\n",
      "        [0.0630, 0.0336]]) \n",
      "\n",
      "torch.clamp(a,min=-1,max=1): tensor([[0.2635, 0.6008],\n",
      "        [0.0630, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2,2)\n",
    "b = torch.randn(2,2)\n",
    "print('a:',a)\n",
    "print('b:',b,'\\n')\n",
    "print('torch.max(a,b):\\n',torch.max(a,b),'\\n') #返回两个或多个数据集在每个位置上的最大值\n",
    "print('torch.min(a,b):\\n',torch.min(a,b),'\\n') #返回两个或多个数据集在每个位置上的最小值\n",
    "print('torch.round(a):\\n',torch.round(a),'\\n') #返回一个数据集所有元素四舍五入之后的整数值\n",
    "print('torch.floor(a):\\n',torch.floor(a),'\\n') #返回一个数据集中所有元素的靠近每个元素的两个整数中较小的那个整数\n",
    "print('torch.ceil(a):\\n',torch.ceil(a),'\\n') #返回一个数据集中所有元素的靠近每个元素的两个整数中较大的那个整数\n",
    "print('torch.trunc(a):\\n',torch.trunc(a),'\\n') #返回一个数据集中所有元素的整数部分\n",
    "print('torch.fmod(a,1):\\n',torch.fmod(a,1),'\\n') #除法取余数部分，余数可以为正数也可以为负数\n",
    "print('torch.remainder(a,1):\\n',torch.remainder(a,1),'\\n') #除法取余数部分，所有的余数都要求是正数，如果元素是负数，需要先加n倍分母至最小的正数，然后取余数\n",
    "print('torch.clamp(a,min=-1,max=1):',torch.clamp(a,min=-1,max=1)) #将数据集中超过指定最大值或最小值范围的元素改成给定的最大或最小值\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75864eab",
   "metadata": {},
   "source": [
    "#### 2、多维向量运算\n",
    "\n",
    "torch.max()   \n",
    "求一个或多个维度上的最大值\n",
    "\n",
    "torch.min()   \n",
    "求一个或多个维度上的最小值\n",
    "\n",
    "torch.sum()   \n",
    "求一个维度或多个维度上的和\n",
    "\n",
    "torch.mean()   \n",
    "求一个维度或多个维度上的平均值\n",
    "\n",
    "torch.prod()   \n",
    "求一个或多个维度上的累加值\n",
    "\n",
    "torch.std()   \n",
    "求一个或多个维度上的标准差\n",
    "\n",
    "torch.var()   \n",
    "求一个或多个维度上的方差\n",
    "\n",
    "torch.median()  \n",
    "求一个或多个维度上的中位数\n",
    "\n",
    "torch.cumsum() \n",
    "\n",
    "torch.cumprod() \n",
    "\n",
    "torch.cummax()\n",
    "\n",
    "torch.cummin()\n",
    "\n",
    "torch.topk()  \n",
    "返回指定维度下最大的k个值，第一个参数是tensor,第二个参数是k的值，第三个参数是维度的值\n",
    "\n",
    "torch.sort()  \n",
    "对指定维度进行排序，可以通过关键字说明是否为倒序\n",
    "\n",
    "@/torch.matmul()/torch.mm() \n",
    "矩阵乘积，注意矩阵只能是二维的\n",
    "\n",
    "tensor.t()/tensor.T  \n",
    "矩阵转置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7700b5c",
   "metadata": {},
   "source": [
    "#### 3、爱因斯坦运算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e148abf0",
   "metadata": {},
   "source": [
    "顾名思义，einsum这个函数的思想起源于家喻户晓的小爱同学：爱因斯坦~。\n",
    "\n",
    "很久很久以前，小爱同学在捣鼓广义相对论。广义相对论表述各种物理量用的都是张量。\n",
    "\n",
    "比如描述时空有一个四维时空度规张量，描述电磁场有一个电磁张量，描述运动的有能量动量张量。\n",
    "\n",
    "在理论物理学家中，小爱同学的数学基础不算特别好，在捣鼓这些张量的时候，他遇到了一个比较头疼的问题：公式太长太复杂了。\n",
    "\n",
    "有没有什么办法让这些张量运算公式稍微显得对人类友好一些呢，能不能减少一些那种扭曲的$\\sum$求和符号呢？\n",
    "\n",
    "小爱发现，求和导致维度收缩，因此求和符号操作的指标总是只出现在公式的一边。\n",
    "\n",
    "例如在我们熟悉的矩阵乘法中\n",
    "\n",
    "$$C_{ij} = \\sum_{k} A_{ik} B_{kj}$$\n",
    "\n",
    "k这个下标被求和了，求和导致了这个维度的消失，所以它只出现在右边而不出现在左边。\n",
    "\n",
    "这种只出现在张量公式的一边的下标被称之为哑指标，反之为自由指标。\n",
    "\n",
    "小爱同学脑瓜子一转，反正这种只出现在一边的哑指标一定是被求和求掉的，干脆把对应的$\\sum$求和符号省略得了。\n",
    "\n",
    "这就是爱因斯坦求和约定：\n",
    "\n",
    " <font color=\"red\">**只出现在公式一边的指标叫做哑指标，针对哑指标的$\\sum$求和符号可以省略。** </font> \n",
    "\n",
    "公式立刻清爽了很多。\n",
    "\n",
    "$$C_{ij} =  A_{ik} B_{kj}$$\n",
    "\n",
    "这个公式表达的含义如下：\n",
    "\n",
    "C这个张量的第i行第j列由$A$这个张量的第i行第k列和$B$这个张量的第k行第j列相乘，这样得到的是一个三维张量$D$, 其元素为$D_{ikj}$，然后对$D$在维度k上求和得到。\n",
    "\n",
    "公式展现形式中除了省去了求和符号，还省去了乘法符号(代数通识)。\n",
    "\n",
    "\n",
    "借鉴爱因斯坦求和约定表达张量运算的清爽整洁，numpy、tensorflow和 torch等库中都引入了 einsum这个函数。\n",
    "\n",
    "上述矩阵乘法可以被einsum这个函数表述成\n",
    "\n",
    "\n",
    "```python\n",
    "C = torch.einsum(\"ik,kj->ij\",A,B)\n",
    "```\n",
    "\n",
    "这个函数的规则原理非常简洁，3句话说明白。\n",
    "\n",
    "<font color=\"red\">\n",
    "   \n",
    "* 1，用元素计算公式来表达张量运算。\n",
    "\n",
    "* 2，只出现在元素计算公式箭头左边的指标叫做哑指标。\n",
    "\n",
    "* 3，省略元素计算公式中对哑指标的求和符号。\n",
    "    \n",
    "</font> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "42f733fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[19., 22.],\n",
      "        [43., 50.]])\n",
      "tensor([[19., 22.],\n",
      "        [43., 50.]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.tensor([[1,2],[3,4.0]])\n",
    "B = torch.tensor([[5,6],[7,8.0]])\n",
    "\n",
    "C1 = A@B\n",
    "print(C1)\n",
    "\n",
    "C2 = torch.einsum(\"ik,kj->ij\",[A,B])\n",
    "print(C2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "db60fd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: torch.Size([3, 4, 5])\n",
      "after: torch.Size([3, 5, 4])\n"
     ]
    }
   ],
   "source": [
    "#例1，张量转置\n",
    "A = torch.randn(3,4,5)\n",
    "\n",
    "#B = torch.permute(A,[0,2,1])\n",
    "B = torch.einsum(\"ijk->ikj\",A) \n",
    "\n",
    "print(\"before:\",A.shape)\n",
    "print(\"after:\",B.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "86ef3f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: torch.Size([4, 5])\n",
      "after: torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "#例3，求和降维\n",
    "A = torch.randn(4,5)\n",
    "#B = torch.sum(A,1)\n",
    "B = torch.einsum(\"ij->i\",A)\n",
    "print(\"before:\",A.shape)\n",
    "print(\"after:\",B.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "68f0cd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: torch.Size([5, 5]) torch.Size([5, 5])\n",
      "after: torch.Size([5, 5])\n"
     ]
    }
   ],
   "source": [
    "#例4，哈达玛积\n",
    "A = torch.randn(5,5)\n",
    "B = torch.randn(5,5)\n",
    "#C=A*B\n",
    "C = torch.einsum(\"ij,ij->ij\",A,B)\n",
    "print(\"before:\",A.shape, B.shape)\n",
    "print(\"after:\",C.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b514fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.Parameter(tensor)\n",
    "torch.nn.ParameterDict(key1:torch.nn.Parameter(tensor),key2:torch.nn.Parameter(tensor))\n",
    "torch.nn.ParameterList(torch.nn.Parameter(tensor),torch.nn.Parameter(tensor))\n",
    "\n",
    "torch.nn.Module()\n",
    "torch.nn.ModuleDict(torch.nn.Module/网络层/sequrntial,……) #返回一个模型，传入的参数是各种模型层或网络层\n",
    "torch.nn.ModuleList(torch.nn.Module/网络层/sequrntial,……)\n",
    "\n",
    "torch.nn.Module().attr1 = torch.nn.Embedding()  #创建一个网络层,将其赋值给一个.Module类对象的属性\n",
    "\n",
    "torch.nn.Module().attr2 =torch.nn.Sequential()  #创建一个放置网络层的容器，与网络层平级，将其作为一个模型对象的属性值\n",
    "\n",
    "torch.nn.Sequential.add_module()  #向容器中添加模型\n",
    "torch.nn.Conv1d()  #各种不同的神经网络模型，设置参数个数，然后实例化同时也可以是一个单独的网络层\n",
    "torch.nn.Conv2d()\n",
    "torch.nn.Conv3d()\n",
    "torch.nn.MaxPool1d()  #…………\n",
    "torch.nn.ReLU()\n",
    "torch.nn.Flatten()\n",
    "torch.nn.Linear()  #…………\n",
    "torch.nn.Sequential()\n",
    "\n",
    "torch.nn.Module.children()  #返回模型中所有容器包含的层的信息\n",
    "torch.nn.Module.named_children()  #返回模型中所有容器的名字和其包含的层的信息\n",
    "\n",
    "torch.nn.Module.parameters() #返回模型所有的attrs，即torch.nn.Module().attr1……\n",
    "\n",
    "summary() #torchkears.summary()显示模型详细信息\n",
    "\n",
    "\n",
    "#所有的网络层都是以模型的属性而存在，有下面三类形式\n",
    "\n",
    "#1、每个网络层作为模型的一个属性\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self,.....):\n",
    "        super().__init__()\n",
    "        self.layer1 = torch.nn.Module.Conv1d(....)\n",
    "        self.layer2 = torch.nn.Module.Flatten(...)\n",
    "        ......\n",
    "    def forward(self,x):  #x就是输入数据\n",
    "        x = self.layer1(x)\n",
    "        y = self.layer2(x)\n",
    "        return y  #返回最终的loss数据\n",
    "    \n",
    "    \n",
    "#2、将网络层放在一个容器中，容器内的层会自动的进行forward，适用于简单的网络定义\n",
    "#①直接实例化一个容器，然后添加网络层\n",
    "net = torch.nn.Sequential()\n",
    "net.add_module('str of model name',torch.nn.Module.Conv1d())\n",
    "net.add_module('str of model name',torch.nn.Module.Conv2d())\n",
    "net.add_module('str of model name',torch.nn.Module.Conv3d())\n",
    "#②用长参数直接指定网络层内容和顺序，但是不能给网络层命名\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Module.Conv1d()\n",
    "    torch.nn.Module.Conv2d()\n",
    "    torch.nn.Module.Conv3d()\n",
    ") \n",
    "#③用OrderedDict来组成有序的字典模型，解决方法2不能命名的问题\n",
    "net = torch.nn.Sequential(OrderedDict(\n",
    "    [('str of model name',torch.nn.Module.Conv1d())\n",
    "     ('str of model name',torch.nn.Module.Conv2d())\n",
    "     ('str of model name',torch.nn.Module.Conv3d())\n",
    "    ])\n",
    ")\n",
    "\n",
    "\n",
    "#3、将多个网络层防在多个容器里，将每个容器作为模型的一个属性\n",
    "#①用sequential作为容器，因为它可以自动对容器内的网络层进行前向传播，所以最方便\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self,.....):\n",
    "        super().__init__()\n",
    "        self.conv  = torch.nn.Sequential()\n",
    "        self.conv.add_module('str of model name',torch.nn.Module.Conv1d())\n",
    "        self.conv.add_module('str of model name',torch.nn.Module.Conv2d())\n",
    "        self.conv.add_module('str of model name',torch.nn.Module.Conv3d())\n",
    "        sellf.dense = torch.nn.Sequential()\n",
    "        self.dense.add_module('str of model name',torch.nn.Module.Conv1d())\n",
    "        self.dense.add_module('str of model name',torch.nn.Module.Conv2d())\n",
    "        self.dense.add_module('str of model name',torch.nn.Module.Conv3d())\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        y = self.dense(x)\n",
    "        return y\n",
    "#②用ModuleList作为容器，需要写循环对每个网络层进行前向传播\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self,....):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.ModuleList(\n",
    "        [torch.nn.Module.Conv1d(),\n",
    "         torch.nn.Module.Conv2d(),\n",
    "         torch.nn.Module.Conv3d()\n",
    "        ])\n",
    "    def forward(self,x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "#③使用ModuleDict作为容器。\n",
    "#  需要将所有的key按需要的顺序组成一个列表，然后对所有key进行循环以前向传播\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self,...):\n",
    "        super().__init__()\n",
    "        slef.layers =torch.nn.ModuleDict(\n",
    "        {'modelName1':torch.nn.Module.Conv1d(),\n",
    "         'modelName2':torch.nn.Module.Conv2d(),\n",
    "         'modelName3':torch.nn.Module.Conv3d()\n",
    "        })\n",
    "    def forward(self,x):\n",
    "        li= ['modelName1','modelName2','modelName3']\n",
    "        for modelName in li:\n",
    "            x=self.layers[modelName](x)\n",
    "        return x\n",
    "\n",
    "#\n",
    "KerasModel.fit() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "20f7bd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "cpu\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cpu\n",
      "cpu\n",
      "if on cuda: True\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "CUDA的使用\n",
    "需要数据和模型都在cuda上\n",
    "'''\n",
    "\n",
    "#①数据如何放在cuda上\n",
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available()) #判断机器上是否有cuda\n",
    "print(torch.cuda.device_count()) #判断有几个cuda\n",
    "\n",
    "cpu_device = torch.device('cpu')     #创建一个cpu设备\n",
    "cuda_device = torch.device('cuda:0') #创建一个cuda设备\n",
    "\n",
    "cpu_tensor = torch.randn(20,10)                       #创建一个cpu上的tensor\n",
    "cuda_tensor = torch.randn((20,10),device=cuda_device) #创建一个cuda上的tensor\n",
    "\n",
    "print(cpu_tensor.device)   #查看创建的cpu上的tensor是否在cpu上\n",
    "print(cuda_tensor.device)  #查看创建的cuda上的tensor是否在cuda上\n",
    "\n",
    "cuda_tensor2 = cpu_tensor.to(cuda_device) #将cpu上的tensor转移到cuda上，两种方式都可以\n",
    "cuda_tensor3 = cpu_tensor.cuda()          #将cpu上的tensor转移到cuda上，两种方式都可以\n",
    "\n",
    "print(cuda_tensor2.device)  #查看创建的cuda上的tensor2是否在cuda上\n",
    "print(cuda_tensor3.device)  #查看创建的cuda上的tensor3是否在cuda上\n",
    "\n",
    "cpu_tensor2 = cuda_tensor.to(cpu_device) #将cuda上的tensor转移到cpu上，两种方式都可以\n",
    "cpu_tensor3 = cuda_tensor.cpu()          #将cuda上的tensor转移到cpu上，两种方式都可以\n",
    "\n",
    "print(cpu_tensor2.device)  #查看创建的cpu上的tensor2是否在cpu上\n",
    "print(cpu_tensor3.device)  #查看创建的cpu上的tensor3是否在cpu上\n",
    "\n",
    "#②模型如何放在cuda上\n",
    "net = torch.nn.Module()                        #创建一个模型\n",
    "net.w = torch.nn.Parameter(torch.zeros(3,10))  #通过观察模型的参数是否在cuda上的形式来判断模型是否转移成功，所以这里添加了一个参数，方便后面进行验证\n",
    "net = net.to(cuda_device)                      #将模型移动到cuda上\n",
    "print('if on cuda:',next(net.parameters()).is_cuda)  #验证模型的参数是否在cuda上"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a760dc64",
   "metadata": {},
   "source": [
    "#### 从大数据集中提取一个小批次的随机数据，以供下一步模型进行训练\n",
    "又称为random_mini_batch_size。这个批次数据的数量称为batch_size,批次数据数量并不大，所以称为mini_batch_size。另外，这个批次的数据已经是打散的了，打散的目的是为了解决原始数据集中，相邻数据样本往往都是同类型的样本的情况，这种情况往往会导致模型在一段时间内因为接受同样类型的数据，不能进行优化的问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6301bdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "#创建一个数据分批次生成器\n",
    "#参数有三个，分别是数据集，标签，以及批次数据的大小\n",
    "def data_iter(features, labels, batch_size= 'defalut'):\n",
    "    \n",
    "    #计算数据集的长度\n",
    "    num_examples = len(features) \n",
    "    \n",
    "    #生成一个包含所有数据索引号的列表\n",
    "    indices = list(range(num_examples)) \n",
    "    \n",
    "    #利用numpy的.random.shuffle功能将所有的索引号打乱，注意，此处生成的是一个numpy对象\n",
    "    np.random.shuffle(indices) \n",
    "    \n",
    "    #创建batch_size规模的生成器\n",
    "    for i in range(0, num_examples, batch_size): \n",
    "        \n",
    "        #首先取一个batch_size的所有序号，注意取的时候要跟数组的最大序号比较，不能超出最大序号，所以用了min()\n",
    "        #然后将所有的numpy数据转换成torch.int64类型，.LongTensor()的作用是将numpy数据转换成tensor数据\n",
    "        #需要使用.LongTensor()的原因是，后面的torch.index_select()函数其中一个参数要求必须是LongTensor类型的\n",
    "        indexes = torch.LongTensor(indices[i : min(i + batch_size, num_examples)])\n",
    "        \n",
    "        #使用torch.index_select()函数，根据数据的索引生成数据\n",
    "        #torch.index_select(参数1 params：待选取的数据,\n",
    "        #                     参数2 int:在待选取数据的哪个维度上进行选取,\n",
    "        #                     参数3 torch.LongTensor:需要被选中的数据的索引，参数类型要求为tensor的整数型)\n",
    "        #                     参数4 out=tensorName :可选参数，可以将生成的结果通过明确‘out=’的形式返回写入到一个tensor\n",
    "        yield torch.index_select(features, 0, indexes), torch.index_select(labels, 0, indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983effe5",
   "metadata": {},
   "source": [
    "#### 利用torch.utils.data下的TensorDataset和DataLoader生成batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d3a310",
   "metadata": {},
   "source": [
    "TensorDataset：   \n",
    "可以传入多个tensor(如训练数据和label可以作为两个tensor一起传入)，返回一个对象，这个对象将按照传入数据的第0个维度为目标，根据索引输出数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4148f9",
   "metadata": {},
   "source": [
    "DataLoader：   \n",
    "能够控制batch的大小，batch中元素的采样方法，以及将batch结果整理成模型所需输入形式的方法，并且能够使用多进程读取数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d935",
   "metadata": {},
   "source": [
    "```python\n",
    "DataLoader(\n",
    "    dataset,  #数据集\n",
    "    batch_size=1,   #批次大小\n",
    "    shuffle=False,  #是否乱序\n",
    "    sampler=None,   #样本采样函数，一般无需设置。\n",
    "    batch_sampler=None,   #批次采样函数，一般无需设置。\n",
    "    num_workers=0,  #使用多进程读取数据，设置的进程数。\n",
    "    collate_fn=None, #整理一个批次数据的函数。\n",
    "    pin_memory=False,#是否设置为锁业内存。默认为False，锁业内存不会使用虚拟内存(硬盘)，从锁业内存拷贝到GPU上速度会更快。\n",
    "    drop_last=False, #是否丢弃最后一个样本数量不足batch_size批次数据。\n",
    "    timeout=0,     #加载一个数据批次的最长等待时间，一般无需设置。\n",
    "    worker_init_fn=None, #每个worker中dataset的初始化函数，常用于 IterableDataset。一般不使用。\n",
    "    multiprocessing_context=None,\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f4199389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3588,  0.5223,  0.3082,  1.4179],\n",
      "        [-0.5506,  2.1220,  0.4137, -0.1276],\n",
      "        [ 0.0302,  0.0172,  2.2157,  1.5716],\n",
      "        [-0.3546,  0.8855,  0.2870, -1.0324],\n",
      "        [ 0.0562, -0.4361, -0.5903, -2.6593],\n",
      "        [ 0.2717, -0.0872, -1.3222,  1.0993],\n",
      "        [-0.4902,  0.1126,  0.2516, -1.2350],\n",
      "        [ 0.1216, -0.0113,  0.3717, -0.1140],\n",
      "        [-0.1222, -0.2311, -1.7783,  0.1156],\n",
      "        [ 0.3714,  0.4311, -0.1355, -0.8100]])\n",
      "tensor([-1.,  2.,  0., -0., -1.,  2.,  0.,  0.,  0.,  1.]) \n",
      "\n",
      "batch0\n",
      "train: tensor([[ 0.1216, -0.0113,  0.3717, -0.1140],\n",
      "        [ 0.0302,  0.0172,  2.2157,  1.5716],\n",
      "        [-0.4902,  0.1126,  0.2516, -1.2350]])\n",
      "label: tensor([0., 0., 0.]) \n",
      "\n",
      "batch1\n",
      "train: tensor([[-0.3588,  0.5223,  0.3082,  1.4179],\n",
      "        [-0.3546,  0.8855,  0.2870, -1.0324],\n",
      "        [ 0.0562, -0.4361, -0.5903, -2.6593]])\n",
      "label: tensor([-1., -0., -1.]) \n",
      "\n",
      "batch2\n",
      "train: tensor([[-0.5506,  2.1220,  0.4137, -0.1276],\n",
      "        [ 0.3714,  0.4311, -0.1355, -0.8100],\n",
      "        [-0.1222, -0.2311, -1.7783,  0.1156]])\n",
      "label: tensor([2., 1., 0.]) \n",
      "\n",
      "batch3\n",
      "train: tensor([[ 0.2717, -0.0872, -1.3222,  1.0993]])\n",
      "label: tensor([2.]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "a=torch.randn(10,4)\n",
    "b=torch.randn(10).round()\n",
    "print(a)\n",
    "print(b,'\\n')\n",
    "\n",
    "ds = TensorDataset(a,b)\n",
    "dl=DataLoader(ds,batch_size=3,shuffle=True,drop_last=False)\n",
    "#print(list(dl)) DataLoader返回的是一个类list对象，需要通过for循环进行读取\n",
    "\n",
    "for i,(batch,label) in enumerate(dl):\n",
    "    print(f'batch{i}')\n",
    "    print('train:',batch,)\n",
    "    print('label:',label,'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
